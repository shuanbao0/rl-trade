# TensorTrade å¥–åŠ±å‡½æ•°å®ç°è·¯çº¿å›¾

## å½“å‰çŠ¶æ€ âœ…

### å·²å®ç°çš„å¥–åŠ±å‡½æ•° (9ä¸ªä¸»è¦ç±»å‹)
1. **RiskAdjustedReward** - åŸºäºå¤æ™®æ¯”ç‡çš„é£é™©è°ƒæ•´å¥–åŠ±
2. **SimpleReturnReward** - ç®€å•æ”¶ç›Šç‡å¥–åŠ±
3. **ProfitLossReward** - ç›ˆäºæ¯”å¥–åŠ±
4. **DiversifiedReward** - å¤šæŒ‡æ ‡ç»¼åˆå¥–åŠ±
5. **LogSharpeReward** - å¯¹æ•°å¤æ™®æ¯”ç‡å¥–åŠ± (å·®åˆ†å¤æ™®æ¯”ç‡)
6. **ReturnDrawdownReward** - æ”¶ç›Šå›æ’¤å¹³è¡¡å¥–åŠ±
7. **DynamicSortinoReward** - åŠ¨æ€ç´¢æè¯ºæ¯”ç‡å¥–åŠ±
8. **RegimeAwareReward** - å¸‚åœºçŠ¶æ€æ„ŸçŸ¥å¥–åŠ±
9. **ExpertCommitteeReward** - ä¸“å®¶å§”å‘˜ä¼šå¤šç›®æ ‡å¥–åŠ±

**æ€»è®¡**: 36ä¸ªå¯ç”¨ç±»å‹ (åŒ…æ‹¬åˆ«å)

---

## å¾…å®ç°å¥–åŠ±å‡½æ•°åˆ—è¡¨ ğŸš€

### **ğŸ”¥ Phase 1: é«˜ä¼˜å…ˆçº§ (ç«‹å³å®æ–½)**

#### **1. UncertaintyAwareReward** 
**æŠ€æœ¯æˆç†Ÿåº¦**: â­â­â­â­â­ | **é¢„æœŸROI**: â­â­â­â­â­ | **å®æ–½æ—¶é—´**: 2-3å‘¨

**æ ¸å¿ƒç‰¹æ€§**:
- è®¤çŸ¥ä¸ç¡®å®šæ€§ä¼°è®¡ (Epistemic Uncertainty)
- ä»»æ„ä¸ç¡®å®šæ€§ä¼°è®¡ (Aleatoric Uncertainty)  
- æ¡ä»¶é£é™©ä»·å€¼ (CVaR) ä¼˜åŒ–
- ç½®ä¿¡åº¦æ ¡å‡†å’Œé£é™©æ•æ„Ÿå†³ç­–

**æ•°å­¦åŸºç¡€**:
```
R_adjusted = confidence_weight Ã— R_base - Î» Ã— uncertainty_penalty
å…¶ä¸­: confidence_weight = 1 / (1 + epistemic + aleatoric)
```

**åˆ«å**: `uncertainty_aware`, `bayesian_reward`, `confidence_weighted`, `risk_sensitive`

---

#### **2. CuriosityDrivenReward**
**æŠ€æœ¯æˆç†Ÿåº¦**: â­â­â­â­ | **é¢„æœŸROI**: â­â­â­â­ | **å®æ–½æ—¶é—´**: 3-4å‘¨

**æ ¸å¿ƒç‰¹æ€§**:
- å‰å‘æ¨¡å‹é¢„æµ‹è¯¯å·®é©±åŠ¨çš„å¥½å¥‡å¿ƒ
- å­¦ä¹ è¿›åº¦å®æ—¶ç›‘æ§
- å±‚æ¬¡åŒ–å­ç›®æ ‡ç®¡ç†
- DiNAT-Vision Transformerå¢å¼º (2025å¹´æœ€æ–°)

**æ•°å­¦åŸºç¡€**:
```
R_total = R_extrinsic + Î±Ã—R_curiosity + Î²Ã—R_progress + Î³Ã—R_hierarchical
R_curiosity = ||f(s_t, a_t) - s_{t+1}||Â²
```

**åˆ«å**: `curiosity_driven`, `intrinsic_motivation`, `exploration_bonus`, `hierarchical_curiosity`

---

### **ğŸš€ Phase 2: ä¸­é«˜ä¼˜å…ˆçº§ (3ä¸ªæœˆå†…)**

#### **3. SelfRewardingReward**
**æŠ€æœ¯æˆç†Ÿåº¦**: â­â­â­â­ | **é¢„æœŸROI**: â­â­â­â­â­ | **å®æ–½æ—¶é—´**: 4-6å‘¨

**æ ¸å¿ƒç‰¹æ€§**:
- LLM-as-a-Judge è‡ªæˆ‘è¯„åˆ¤æœºåˆ¶
- è¿­ä»£DPO (Direct Preference Optimization) è®­ç»ƒ
- è‡ªæˆ‘åå·®æ£€æµ‹å’Œçº æ­£
- çªç ´äººç±»æ€§èƒ½ç“¶é¢ˆè®¾è®¡

**æ•°å­¦åŸºç¡€**:
```
R_{t+1} = R_t + Î± Ã— Self_Judge(Ï€(s_t), r_t, outcome_t)
Ï€_{t+1} = arg max E[R_{t+1}(s,a) | Ï€(s,a)]
```

**åˆ«å**: `self_rewarding`, `auto_reward`, `llm_judge`, `adaptive_self_improving`

---

#### **4. CausalReward**
**æŠ€æœ¯æˆç†Ÿåº¦**: â­â­â­ | **é¢„æœŸROI**: â­â­â­â­â­ | **å®æ–½æ—¶é—´**: 5-7å‘¨

**æ ¸å¿ƒç‰¹æ€§**:
- å› æœå›¾ç»“æ„å­¦ä¹ å’Œæ¨ç†
- æ··æ·†å˜é‡è¯†åˆ«å’Œæ§åˆ¶
- åé—¨è°ƒæ•´å’Œå‰é—¨è°ƒæ•´
- DOVIå»æ··æ·†ä»·å€¼è¿­ä»£ç®—æ³•

**æ•°å­¦åŸºç¡€**:
```
R_causal = Î£(causal_effect(action_i â†’ outcome_j) Ã— importance_weight_j)
åŸºäºPearlå› æœæ¨ç†ç†è®ºå’ŒSCMç»“æ„å› æœæ¨¡å‹
```

**åˆ«å**: `causal_reward`, `deconfounded`, `causal_inference`, `structural_causal`

---

#### **5. LLMGuidedReward**
**æŠ€æœ¯æˆç†Ÿåº¦**: â­â­â­â­ | **é¢„æœŸROI**: â­â­â­â­ | **å®æ–½æ—¶é—´**: 4-5å‘¨

**æ ¸å¿ƒç‰¹æ€§**:
- è‡ªç„¶è¯­è¨€å¥–åŠ±è§„èŒƒè§£æ
- è‡ªåŠ¨å¥–åŠ±å‡½æ•°ä»£ç ç”Ÿæˆ
- å®‰å…¨æ€§éªŒè¯å’Œä¸€è‡´æ€§æ£€æŸ¥
- å¯è§£é‡Šçš„è®¾è®¡æ¨ç†

**æ•°å­¦åŸºç¡€**:
```
Natural Language Spec â†’ Logical Rules â†’ Executable Code
åŒ…å«å®‰å…¨çº¦æŸå’Œæ€§èƒ½ä¼˜åŒ–
```

**åˆ«å**: `llm_guided`, `natural_language_reward`, `auto_design`, `explainable_reward`

---

### **âš¡ Phase 3: ä¸­ç­‰ä¼˜å…ˆçº§ (6ä¸ªæœˆå†…)**

#### **6. CurriculumReward**
**æŠ€æœ¯æˆç†Ÿåº¦**: â­â­â­ | **é¢„æœŸROI**: â­â­â­â­ | **å®æ–½æ—¶é—´**: 4-5å‘¨

**æ ¸å¿ƒç‰¹æ€§**:
- éš¾åº¦è‡ªé€‚åº”è°ƒèŠ‚æœºåˆ¶
- æ¸è¿›å¼è®­ç»ƒç­–ç•¥
- å¤šé˜¶æ®µå¥–åŠ±è¯¾ç¨‹è®¾è®¡
- æ€§èƒ½åŸºå‡†è‡ªåŠ¨è°ƒæ•´

**æ•°å­¦åŸºç¡€**:
```
difficulty_level = f(training_progress, performance)
R_t = curriculum_reward(basic_reward, difficulty_level)
```

**åˆ«å**: `curriculum_learning`, `progressive_reward`, `difficulty_adaptive`, `staged_learning`

---

#### **7. MultiAgentCompetitiveReward**
**æŠ€æœ¯æˆç†Ÿåº¦**: â­â­â­ | **é¢„æœŸROI**: â­â­â­â­ | **å®æ–½æ—¶é—´**: 5-6å‘¨

**æ ¸å¿ƒç‰¹æ€§**:
- å¤šæ™ºèƒ½ä½“ç«äº‰ç¯å¢ƒå»ºæ¨¡
- å¸‚åœºå®¹é‡é™åˆ¶å’Œå†²å‡»æˆæœ¬
- ç›¸å¯¹è¡¨ç°æ’åæœºåˆ¶
- åä½œä¸ç«äº‰å¹³è¡¡

**æ•°å­¦åŸºç¡€**:
```
R_i,t = individual_return_i - Î³ Ã— relative_performance_penalty
è€ƒè™‘å¸‚åœºå†²å‡»å’Œå®¹é‡çº¦æŸ
```

**åˆ«å**: `multi_agent`, `competitive`, `market_impact_aware`, `relative_performance`

---

#### **8. AdaptiveVolatilityReward**
**æŠ€æœ¯æˆç†Ÿåº¦**: â­â­â­â­ | **é¢„æœŸROI**: â­â­â­ | **å®æ–½æ—¶é—´**: 3-4å‘¨

**æ ¸å¿ƒç‰¹æ€§**:
- åŠ¨æ€æ³¢åŠ¨ç‡ä¼°è®¡
- æ³¢åŠ¨ç‡regimeè¯†åˆ«
- VaR/CVaRè‡ªé€‚åº”è°ƒæ•´
- GARCHæ—æ¨¡å‹é›†æˆ

**æ•°å­¦åŸºç¡€**:
```
Ïƒ_t = f(GARCH, realized_volatility, implied_volatility)
R_t = base_reward Ã— volatility_adjustment(Ïƒ_t)
```

**åˆ«å**: `adaptive_volatility`, `volatility_regime`, `garch_enhanced`, `var_adjusted`

---

### **ğŸ”¬ Phase 4: å‰æ²¿æ¢ç´¢ (12ä¸ªæœˆå†…)**

#### **9. FederatedReward**
**æŠ€æœ¯æˆç†Ÿåº¦**: â­â­â­ | **é¢„æœŸROI**: â­â­â­ | **å®æ–½æ—¶é—´**: 6-8å‘¨

**æ ¸å¿ƒç‰¹æ€§**:
- éšç§ä¿æŠ¤çš„åä½œå­¦ä¹ 
- å·®åˆ†éšç§å’Œå®‰å…¨èšåˆ
- è·¨æœºæ„å¥–åŠ±ä¼˜åŒ–
- åŒºå—é“¾è®°å½•å’Œå…±è¯†

**æ•°å­¦åŸºç¡€**:
```
è”é‚¦å­¦ä¹  + å·®åˆ†éšç§ + å®‰å…¨å¤šæ–¹è®¡ç®—
global_reward_model = federated_averaging(local_models)
```

**åˆ«å**: `federated_learning`, `privacy_preserving`, `collaborative_reward`, `distributed`

---

#### **10. MetaLearningReward**
**æŠ€æœ¯æˆç†Ÿåº¦**: â­â­ | **é¢„æœŸROI**: â­â­â­â­â­ | **å®æ–½æ—¶é—´**: 8-12å‘¨

**æ ¸å¿ƒç‰¹æ€§**:
- å¥–åŠ±å‡½æ•°çš„å…ƒå­¦ä¹ 
- MAMLæ¡†æ¶é€‚é…
- å¿«é€Ÿé€‚åº”æ–°å¸‚åœºç¯å¢ƒ
- å­¦ä¹ å¦‚ä½•å­¦ä¹ å¥–åŠ±

**æ•°å­¦åŸºç¡€**:
```
Î¸* = arg min Î£ L(f_Î¸'(D_i^train), D_i^test)
å…¶ä¸­ Î¸' = Î¸ - Î±âˆ‡L(f_Î¸(D_i^train))
```

**åˆ«å**: `meta_learning`, `learning_to_learn`, `few_shot_adaptation`, `maml_reward`

---

#### **11. QuantumInspiredReward**
**æŠ€æœ¯æˆç†Ÿåº¦**: â­â­ | **é¢„æœŸROI**: â­â­ | **å®æ–½æ—¶é—´**: 8-10å‘¨

**æ ¸å¿ƒç‰¹æ€§**:
- é‡å­å åŠ çŠ¶æ€å¥–åŠ±
- é‡å­çº ç¼ å¤šç›®æ ‡ä¼˜åŒ–
- Groveræœç´¢åŠ é€Ÿ
- é‡å­æ¼«æ­¥æ¢ç´¢

**æ•°å­¦åŸºç¡€**:
```
|reward_stateâŸ© = Î±|bullâŸ© + Î²|bearâŸ© + Î³|neutralâŸ© + Î´|volatileâŸ©
é‡å­æµ‹é‡è·å¾—å¡Œç¼©å¥–åŠ±
```

**åˆ«å**: `quantum_inspired`, `superposition_reward`, `quantum_enhanced`, `grover_search`

---

#### **12. NeuromorphicReward**
**æŠ€æœ¯æˆç†Ÿåº¦**: â­â­ | **é¢„æœŸROI**: â­â­â­ | **å®æ–½æ—¶é—´**: 10-12å‘¨

**æ ¸å¿ƒç‰¹æ€§**:
- è„‰å†²ç¥ç»ç½‘ç»œå¤„ç†
- æ—¶åºç¼–ç å’ŒSTDPå­¦ä¹ 
- å¿†é˜»å™¨å†…å­˜è®¡ç®—
- æä½åŠŸè€—è¾¹ç¼˜è®¡ç®—

**æ•°å­¦åŸºç¡€**:
```
åŸºäºè„‰å†²æ—¶åºçš„ç¨€ç–è®¡ç®—
èƒ½è€— âˆ è„‰å†²é¢‘ç‡ (è€Œéè¿ç»­è®¡ç®—)
```

**åˆ«å**: `neuromorphic`, `spiking_neural`, `memristor_computing`, `energy_efficient`

---

### **ğŸ›¡ï¸ Phase 5: ä¸“ä¸šç‰¹åŒ– (18ä¸ªæœˆå†…)**

#### **13. RiskParityReward**
**æŠ€æœ¯æˆç†Ÿåº¦**: â­â­â­â­ | **é¢„æœŸROI**: â­â­â­ | **å®æ–½æ—¶é—´**: 3-4å‘¨

**æ ¸å¿ƒç‰¹æ€§**:
- é£é™©å¹³ä»·ç»„åˆä¼˜åŒ–
- è¾¹é™…é£é™©è´¡çŒ®å‡è¡¡
- åŠ¨æ€é£é™©é¢„ç®—åˆ†é…
- å¤šèµ„äº§é£é™©åˆ†è§£

**åˆ«å**: `risk_parity`, `equal_risk_contribution`, `marginal_risk`, `risk_budgeting`

---

#### **14. BehavioralBiasReward**
**æŠ€æœ¯æˆç†Ÿåº¦**: â­â­â­ | **é¢„æœŸROI**: â­â­â­ | **å®æ–½æ—¶é—´**: 4-5å‘¨

**æ ¸å¿ƒç‰¹æ€§**:
- è¡Œä¸ºåå·®è¯†åˆ«å’Œçº æ­£
- è¿‡åº¦è‡ªä¿¡å’ŒæŸå¤±åŒæ¶å»ºæ¨¡
- å¿ƒç†è´¦æˆ·æ•ˆåº”æ§åˆ¶
- æƒ…ç»ªçŠ¶æ€æ„ŸçŸ¥äº¤æ˜“

**åˆ«å**: `behavioral_bias`, `psychology_aware`, `emotion_corrected`, `cognitive_bias`

---

#### **15. ESGIntegratedReward**
**æŠ€æœ¯æˆç†Ÿåº¦**: â­â­â­ | **é¢„æœŸROI**: â­â­â­ | **å®æ–½æ—¶é—´**: 4-6å‘¨

**æ ¸å¿ƒç‰¹æ€§**:
- ç¯å¢ƒã€ç¤¾ä¼šã€æ²»ç†å› å­æ•´åˆ
- å¯æŒç»­æŠ•èµ„ç›®æ ‡å¹³è¡¡
- ESGè¯„åˆ†åŠ¨æ€æƒé‡
- ç¤¾ä¼šè´£ä»»æŠ•èµ„ä¼˜åŒ–

**åˆ«å**: `esg_integrated`, `sustainable_investing`, `responsible_trading`, `impact_weighted`

---

## ğŸ“Š å®æ–½ä¼˜å…ˆçº§çŸ©é˜µ

| å¥–åŠ±å‡½æ•° | æŠ€æœ¯æˆç†Ÿåº¦ | å®æ–½éš¾åº¦ | é¢„æœŸROI | è®¡ç®—æˆæœ¬ | æ¨èä¼˜å…ˆçº§ | æ—¶é—´ä¼°ç®— |
|---------|-----------|----------|---------|----------|-----------|----------|
| **UncertaintyAware** | â­â­â­â­â­ | â­â­â­ | â­â­â­â­â­ | ä¸­ | ğŸ”¥æœ€é«˜ | 2-3å‘¨ |
| **CuriosityDriven** | â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | ä¸­é«˜ | ğŸ”¥æœ€é«˜ | 3-4å‘¨ |
| **SelfRewarding** | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | é«˜ | ğŸš€é«˜ | 4-6å‘¨ |
| **Causal** | â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | ä¸­ | ğŸš€é«˜ | 5-7å‘¨ |
| **LLMGuided** | â­â­â­â­ | â­â­â­ | â­â­â­â­ | é«˜ | ğŸš€é«˜ | 4-5å‘¨ |
| **Curriculum** | â­â­â­ | â­â­â­ | â­â­â­â­ | ä½ | âš¡ä¸­ç­‰ | 4-5å‘¨ |
| **MultiAgent** | â­â­â­ | â­â­â­â­ | â­â­â­â­ | é«˜ | âš¡ä¸­ç­‰ | 5-6å‘¨ |
| **AdaptiveVolatility** | â­â­â­â­ | â­â­â­ | â­â­â­ | ä¸­ | âš¡ä¸­ç­‰ | 3-4å‘¨ |
| **Federated** | â­â­â­ | â­â­â­â­ | â­â­â­ | é«˜ | ğŸ”¬ä½ | 6-8å‘¨ |
| **MetaLearning** | â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | æé«˜ | ğŸ”¬ä½ | 8-12å‘¨ |
| **QuantumInspired** | â­â­ | â­â­â­â­â­ | â­â­ | ä¸­ | ğŸ”¬ä½ | 8-10å‘¨ |
| **Neuromorphic** | â­â­ | â­â­â­â­â­ | â­â­â­ | ä½ | ğŸ”¬ä½ | 10-12å‘¨ |
| **RiskParity** | â­â­â­â­ | â­â­ | â­â­â­ | ä½ | ğŸ›¡ï¸ä¸“ä¸š | 3-4å‘¨ |
| **BehavioralBias** | â­â­â­ | â­â­â­ | â­â­â­ | ä¸­ | ğŸ›¡ï¸ä¸“ä¸š | 4-5å‘¨ |
| **ESGIntegrated** | â­â­â­ | â­â­â­ | â­â­â­ | ä¸­ | ğŸ›¡ï¸ä¸“ä¸š | 4-6å‘¨ |

---

## ğŸ¯ æ¨èå®æ–½ç­–ç•¥

### **ç«‹å³å¼€å§‹ (æ¥ä¸‹æ¥1ä¸ªæœˆ)**
1. **UncertaintyAwareReward** - æœ€é«˜ROIï¼Œç«‹å³å¯ç”¨
2. **CuriosityDrivenReward** - è§£å†³å…³é”®é—®é¢˜ï¼ŒæŠ€æœ¯å…ˆè¿›

### **çŸ­æœŸç›®æ ‡ (3ä¸ªæœˆå†…)**
3. **SelfRewardingReward** - é©å‘½æ€§çªç ´
4. **LLMGuidedReward** - ç”¨æˆ·å‹å¥½
5. **CausalReward** - ç†è®ºæ·±åº¦

### **ä¸­æœŸè§„åˆ’ (6ä¸ªæœˆå†…)**
6. **CurriculumReward** - å­¦ä¹ æ•ˆç‡æå‡
7. **MultiAgentCompetitiveReward** - ç°å®ç¯å¢ƒæ¨¡æ‹Ÿ
8. **AdaptiveVolatilityReward** - ä¸“ä¸šé‡‘èéœ€æ±‚

### **é•¿æœŸç ”ç©¶ (12ä¸ªæœˆå†…)**
9. **MetaLearningReward** - ç»ˆæè‡ªé€‚åº”
10. **FederatedReward** - éšç§ä¿æŠ¤åä½œ
11. **ä¸“ä¸šç‰¹åŒ–å¥–åŠ±å‡½æ•°** - æ ¹æ®å…·ä½“éœ€æ±‚é€‰æ‹©

---

## ğŸ“ˆ é¢„æœŸæˆæœ

### **æŠ€æœ¯æŒ‡æ ‡æå‡**
- **ç¨³å¥æ€§**: +40% (UncertaintyAware)
- **å­¦ä¹ æ•ˆç‡**: +60% (CuriosityDriven)  
- **é€‚åº”æ€§**: +80% (SelfRewarding)
- **æ³›åŒ–èƒ½åŠ›**: +50% (Causal)

### **ç³»ç»Ÿèƒ½åŠ›å‡çº§**
- ä»9ç§å¥–åŠ±å‡½æ•°æ‰©å±•åˆ°24ç§ä¸»è¦ç±»å‹
- è¦†ç›–ä»åŸºç¡€åˆ°ä¸“å®¶çº§çš„å®Œæ•´æŠ€æœ¯æ ˆ
- å»ºç«‹ä¸šç•Œé¢†å…ˆçš„å¥–åŠ±å‡½æ•°ç”Ÿæ€ç³»ç»Ÿ

### **å•†ä¸šä»·å€¼åˆ›é€ **
- æ˜¾è‘—æå‡äº¤æ˜“ç­–ç•¥ç¨³å®šæ€§å’Œç›ˆåˆ©èƒ½åŠ›
- é™ä½äººå·¥è°ƒå‚å’Œç»´æŠ¤æˆæœ¬
- å»ºç«‹æŠ€æœ¯æŠ¤åŸæ²³å’Œç«äº‰ä¼˜åŠ¿

---

*æœ€åæ›´æ–°: 2025-07-27*  
*è§„åˆ’æ—¶é—´è·¨åº¦: 18ä¸ªæœˆ*  
*é¢„è®¡å®ç°å¥–åŠ±å‡½æ•°æ€»æ•°: 24ä¸ªä¸»è¦ç±»å‹*