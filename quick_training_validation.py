#!/usr/bin/env python3
"""
Quick Training Validation for Experiment 006
å®éªŒ006å¿«é€Ÿè®­ç»ƒéªŒè¯

Purpose: éªŒè¯DirectPnLRewardå¥–åŠ±å‡½æ•°åœ¨å®é™…è®­ç»ƒä¸­çš„è¡¨ç°
ä½¿ç”¨è¾ƒçŸ­çš„è®­ç»ƒæ—¶é—´æ¥å¿«é€ŸéªŒè¯ç³»ç»Ÿæ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import sys
import os
sys.path.append('.')

import numpy as np
import pandas as pd
from datetime import datetime
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.callbacks import EvalCallback

from src.environment.rewards import create_reward_function
from src.environment.trading_environment import TradingEnvironment  
from src.data.data_manager import DataManager
from src.utils.config import Config
from src.utils.logger import setup_logger

class QuickTrainingValidator:
    """å¿«é€Ÿè®­ç»ƒéªŒè¯å™¨"""
    
    def __init__(self):
        self.logger = setup_logger("QuickTrainingValidator")
        self.config = Config()
        self.results = {}
        
    def run_quick_validation(self) -> dict:
        """è¿è¡Œå¿«é€Ÿè®­ç»ƒéªŒè¯"""
        
        self.logger.info("=== å¼€å§‹å®éªŒ006å¿«é€Ÿè®­ç»ƒéªŒè¯ ===")
        
        try:
            # 1. å‡†å¤‡æ•°æ®
            self.logger.info("1. å‡†å¤‡EURUSDæ•°æ®...")
            dm = DataManager()
            raw_data = dm.get_stock_data('EURUSD=X', period='6mo')
            
            if len(raw_data) < 50:
                raise ValueError(f"æ•°æ®é‡ä¸è¶³: {len(raw_data)}")
                
            self.logger.info(f"æ•°æ®è·å–æˆåŠŸ: {len(raw_data)}æ¡è®°å½•")
            
            # 2. ç‰¹å¾å·¥ç¨‹ (ç®€åŒ–ç‰ˆ)
            self.logger.info("2. åˆ›å»ºç‰¹å¾æ•°æ®...")
            features = raw_data[['open', 'high', 'low', 'close', 'volume']].copy()
            features = features.dropna()
            
            # æ·»åŠ ç®€å•çš„æŠ€æœ¯æŒ‡æ ‡
            features['sma_5'] = features['close'].rolling(5).mean()
            features['rsi'] = self._calculate_rsi(features['close'], 14)
            features['volatility'] = features['close'].rolling(10).std()
            
            # ç§»é™¤NaNå€¼
            features = features.dropna()
            self.logger.info(f"ç‰¹å¾æ•°æ®å½¢çŠ¶: {features.shape}")
            
            # 3. æ•°æ®åˆ†å‰²
            train_size = int(len(features) * 0.8)
            train_data = features[:train_size]
            test_data = features[train_size:]
            
            self.logger.info(f"è®­ç»ƒæ•°æ®: {len(train_data)}æ¡, æµ‹è¯•æ•°æ®: {len(test_data)}æ¡")
            
            # 4. åˆ›å»ºDirectPnLå¥–åŠ±å‡½æ•°
            self.logger.info("3. åˆ›å»ºDirectPnLå¥–åŠ±å‡½æ•°...")
            reward_fn = create_reward_function('direct_pnl_reward', initial_balance=10000)
            
            # 5. åˆ›å»ºè®­ç»ƒç¯å¢ƒ
            self.logger.info("4. åˆ›å»ºè®­ç»ƒç¯å¢ƒ...")
            train_env = TradingEnvironment(
                df=train_data,
                config=self.config,
                initial_balance=10000,
                window_size=5,
                reward_function=reward_fn,
                max_episode_steps=50  # é™åˆ¶episodeé•¿åº¦
            )
            
            # åˆ›å»ºéªŒè¯ç¯å¢ƒ
            test_env = TradingEnvironment(
                df=test_data,
                config=self.config,
                initial_balance=10000,
                window_size=5,
                reward_function=reward_fn,
                max_episode_steps=len(test_data)-10
            )
            
            # åŒ…è£…ç¯å¢ƒ
            train_env_vec = DummyVecEnv([lambda: train_env])
            test_env_vec = DummyVecEnv([lambda: test_env])
            
            self.logger.info("5. å¼€å§‹PPOè®­ç»ƒ...")
            
            # 6. åˆ›å»ºPPOæ¨¡å‹ - ä½¿ç”¨è¾ƒç®€å•çš„é…ç½®
            model = PPO(
                "MlpPolicy",
                train_env_vec,
                verbose=1,
                learning_rate=0.001,
                n_steps=512,
                batch_size=64,
                n_epochs=5,
                gamma=0.99,
                device='cpu',  # å¼ºåˆ¶ä½¿ç”¨CPUé¿å…CUDAé—®é¢˜
                tensorboard_log="./logs/tensorboard/"
            )
            
            # 7. è®­ç»ƒæ¨¡å‹ (çŸ­æ—¶é—´)
            self.logger.info("å¼€å§‹è®­ç»ƒ - 20Kæ­¥å¿«é€ŸéªŒè¯...")
            
            start_time = datetime.now()
            model.learn(
                total_timesteps=20_000,  # çŸ­æ—¶é—´è®­ç»ƒ
                callback=None,
                tb_log_name="quick_validation"
            )
            training_time = datetime.now() - start_time
            
            self.logger.info(f"è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {training_time}")
            
            # 8. è¯„ä¼°æ¨¡å‹
            self.logger.info("6. è¯„ä¼°è®­ç»ƒç»“æœ...")
            eval_results = self._evaluate_model(model, test_env_vec, n_episodes=5)
            
            # 9. éªŒè¯å¥–åŠ±-å›æŠ¥ç›¸å…³æ€§
            correlation_results = self._validate_reward_correlation(reward_fn)
            
            # 10. æ±‡æ€»ç»“æœ
            self.results = {
                "training_success": True,
                "training_time": str(training_time),
                "training_steps": 20_000,
                "data_size": len(features),
                "train_episodes": len(train_data) - 10,
                "evaluation_results": eval_results,
                "reward_correlation": correlation_results,
                "validation_timestamp": datetime.now().isoformat()
            }
            
            self.logger.info("=== å¿«é€Ÿè®­ç»ƒéªŒè¯å®Œæˆ ===")
            self._print_results()
            
            return self.results
            
        except Exception as e:
            self.logger.error(f"å¿«é€Ÿè®­ç»ƒéªŒè¯å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {"training_success": False, "error": str(e)}
    
    def _calculate_rsi(self, prices, window=14):
        """è®¡ç®—RSIæŒ‡æ ‡"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi
    
    def _evaluate_model(self, model, test_env, n_episodes=5):
        """è¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹"""
        
        self.logger.info(f"åœ¨æµ‹è¯•ç¯å¢ƒä¸­è¯„ä¼°æ¨¡å‹ ({n_episodes}è½®)...")
        
        episode_returns = []
        episode_lengths = []
        episode_rewards = []
        
        for episode in range(n_episodes):
            obs = test_env.reset()
            episode_reward = 0
            episode_length = 0
            
            done = False
            while not done:
                action, _ = model.predict(obs, deterministic=True)
                obs, reward, done, info = test_env.step(action)
                episode_reward += reward[0]
                episode_length += 1
                
                if episode_length > 100:  # é˜²æ­¢æ— é™å¾ªç¯
                    break
            
            # è·å–æœ€ç»ˆå›æŠ¥
            final_return = info[0].get('total_return', 0.0)
            
            episode_returns.append(final_return)
            episode_lengths.append(episode_length)
            episode_rewards.append(episode_reward)
            
            self.logger.info(f"Episode {episode+1}: é•¿åº¦={episode_length}, å¥–åŠ±={episode_reward:.4f}, å›æŠ¥={final_return:.2f}%")
        
        return {
            "mean_return": np.mean(episode_returns),
            "std_return": np.std(episode_returns),
            "mean_reward": np.mean(episode_rewards),
            "mean_length": np.mean(episode_lengths),
            "episodes_evaluated": n_episodes
        }
    
    def _validate_reward_correlation(self, reward_fn):
        """éªŒè¯å¥–åŠ±å‡½æ•°çš„ç›¸å…³æ€§"""
        
        # æ¨¡æ‹Ÿä¸€äº›äº¤æ˜“æ•°æ®æ¥éªŒè¯ç›¸å…³æ€§
        portfolio_values = [10000 + i*50 + np.random.normal(0, 20) for i in range(20)]
        actions = [np.random.uniform(-0.5, 0.5) for _ in range(20)]
        prices = [1.09 + i*0.001 + np.random.normal(0, 0.002) for i in range(20)]
        
        rewards = []
        returns = []
        
        for i in range(1, len(portfolio_values)):
            portfolio_info = {
                'total_value': portfolio_values[i],
                'cash': portfolio_values[i] * 0.5,
                'shares': portfolio_values[i] * 0.5 / prices[i],
                'return_pct': (portfolio_values[i] - 10000) / 10000 * 100
            }
            
            trade_info = {'executed': True, 'amount': 0, 'cost': 0}
            
            reward = reward_fn.calculate_reward(
                portfolio_value=portfolio_values[i],
                action=actions[i],
                price=prices[i],
                portfolio_info=portfolio_info,
                trade_info=trade_info,
                step=i
            )
            
            actual_return = (portfolio_values[i] - portfolio_values[i-1]) / portfolio_values[i-1] * 100
            
            rewards.append(reward)
            returns.append(actual_return)
        
        # è®¡ç®—ç›¸å…³æ€§
        correlation = np.corrcoef(rewards, returns)[0, 1] if len(rewards) > 1 else 0.0
        
        return {
            "correlation": correlation,
            "correlation_target": 0.8,
            "correlation_status": "GOOD" if correlation >= 0.8 else "NEEDS_IMPROVEMENT",
            "sample_size": len(rewards)
        }
    
    def _print_results(self):
        """æ‰“å°éªŒè¯ç»“æœæ‘˜è¦"""
        
        print("\n" + "="*80)
        print("å®éªŒ006å¿«é€Ÿè®­ç»ƒéªŒè¯ç»“æœ")
        print("="*80)
        
        if self.results.get("training_success"):
            print("âœ… è®­ç»ƒçŠ¶æ€: æˆåŠŸ")
            print(f"ğŸ“Š è®­ç»ƒæ—¶é—´: {self.results['training_time']}")
            print(f"ğŸ¯ è®­ç»ƒæ­¥æ•°: {self.results['training_steps']:,}")
            print(f"ğŸ“ˆ æ•°æ®è§„æ¨¡: {self.results['data_size']} æ¡è®°å½•")
            
            eval_results = self.results.get("evaluation_results", {})
            print(f"\nğŸ“Š è¯„ä¼°ç»“æœ:")
            print(f"   å¹³å‡å›æŠ¥: {eval_results.get('mean_return', 0):.2f}%")
            print(f"   å¹³å‡å¥–åŠ±: {eval_results.get('mean_reward', 0):.4f}")
            print(f"   å¹³å‡Episodeé•¿åº¦: {eval_results.get('mean_length', 0):.1f}")
            
            corr_results = self.results.get("reward_correlation", {})
            print(f"\nğŸ¯ å¥–åŠ±å‡½æ•°éªŒè¯:")
            print(f"   å¥–åŠ±-å›æŠ¥ç›¸å…³æ€§: {corr_results.get('correlation', 0):.4f}")
            print(f"   ç›¸å…³æ€§çŠ¶æ€: {corr_results.get('correlation_status', 'UNKNOWN')}")
            
            if corr_results.get('correlation', 0) >= 0.8:
                print("âœ… DirectPnLRewardå¥–åŠ±å‡½æ•°éªŒè¯é€šè¿‡")
            else:
                print("âš ï¸ DirectPnLRewardå¥–åŠ±å‡½æ•°éœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")
                
        else:
            print("âŒ è®­ç»ƒçŠ¶æ€: å¤±è´¥")
            print(f"é”™è¯¯: {self.results.get('error', 'Unknown error')}")
        
        print("="*80)


def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    
    validator = QuickTrainingValidator()
    results = validator.run_quick_validation()
    
    # ä¿å­˜ç»“æœ
    results_file = f"quick_training_validation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    
    import json
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, default=str, ensure_ascii=False)
    
    print(f"\nç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    
    return results


if __name__ == "__main__":
    main()