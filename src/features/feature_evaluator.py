"""
Experiment #005: ç§‘å­¦ç‰¹å¾è¯„ä¼°æ¡†æ¶ - é¡¹ç›®é›†æˆç‰ˆæœ¬
å®ç°ä¸¥æ ¼çš„ç‰¹å¾é€‰æ‹©å’ŒéªŒè¯æœºåˆ¶ï¼Œé›†æˆåˆ°ç°æœ‰ç‰¹å¾å·¥ç¨‹ç³»ç»Ÿ
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Any, Tuple, Optional
from dataclasses import dataclass
from scipy import stats
import warnings
import logging
from ..utils.logger import setup_logger, get_default_log_file
from ..utils.config import Config

warnings.filterwarnings('ignore')

@dataclass
class FeatureEvaluationResult:
    """ç‰¹å¾è¯„ä¼°ç»“æœ"""
    feature_name: str
    overall_score: float
    individual_scores: Dict[str, float]
    recommendation: str
    p_value: float
    effect_size: float
    confidence_interval: Tuple[float, float]
    evaluation_details: Dict[str, Any]

@dataclass  
class FeatureEvaluatorConfig:
    """ç‰¹å¾è¯„ä¼°å™¨é…ç½®"""
    # è¯„ä¼°æƒé‡é…ç½®
    evaluation_weights: Dict[str, float] = None
    # é˜ˆå€¼é…ç½®
    thresholds: Dict[str, float] = None
    # é‡‘èç†è®ºè¯„åˆ†
    theory_scores: Dict[str, float] = None
    
    def __post_init__(self):
        if self.evaluation_weights is None:
            self.evaluation_weights = {
                'performance': 0.30,      # æ€§èƒ½æ”¹è¿›æƒé‡æœ€é«˜
                'significance': 0.20,     # ç»Ÿè®¡æ˜¾è‘—æ€§å¾ˆé‡è¦
                'importance': 0.20,       # ç‰¹å¾é‡è¦æ€§
                'redundancy': 0.15,       # å†—ä½™åº¦æ£€æŸ¥
                'cost': 0.10,            # è®¡ç®—æˆæœ¬
                'theory': 0.05           # ç†è®ºæ”¯æŒ
            }
        
        if self.thresholds is None:
            self.thresholds = {
                'significance_alpha': 0.05,
                'min_effect_size': 0.2,
                'max_redundancy': 0.8,
                'min_importance': 0.01,
                'acceptance_score': 0.6
            }
        
        if self.theory_scores is None:
            self.theory_scores = {
                # ç»å…¸æŠ€æœ¯åˆ†ææŒ‡æ ‡
                'RSI_14': 0.9, 'MACD': 0.9, 'EMA_21': 0.9, 'SMA_20': 0.8,
                'ATR_14': 0.85, 'Williams_R_14': 0.8, 'CCI_20': 0.8,
                'Stochastic_K_14': 0.8, 'BB_Width_20': 0.8,
                
                # é«˜çº§æŠ€æœ¯æŒ‡æ ‡
                'ADX_14': 0.85, 'Parabolic_SAR': 0.7, 'Ichimoku': 0.7,
                'OBV': 0.75, 'MFI': 0.75,
                
                # æ³¢åŠ¨ç‡æŒ‡æ ‡
                'HV_20': 0.8, 'ATR_Ratio_14_50': 0.75, 'GARCH': 0.7,
                
                # ç»Ÿè®¡æŒ‡æ ‡
                'Rolling_Mean_20': 0.6, 'Rolling_Std_20': 0.6,
                'Z_Score': 0.6, 'Skewness': 0.5, 'Kurtosis': 0.4,
                
                # é»˜è®¤åˆ†æ•°
                'default': 0.5
            }

class FeatureEvaluator:
    """
    ç§‘å­¦çš„ç‰¹å¾è¯„ä¼°å’Œé€‰æ‹©æ¡†æ¶ - Experiment #005
    
    é›†æˆåˆ°é¡¹ç›®ç‰¹å¾å·¥ç¨‹ç³»ç»Ÿä¸­ï¼Œæä¾›ï¼š
    1. æ€§èƒ½æ”¹è¿›è¯„ä¼° - å¯¹æ¨¡å‹æ€§èƒ½çš„å®é™…è´¡çŒ®
    2. ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ - æ”¹è¿›æ˜¯å¦ç»Ÿè®¡æ˜¾è‘—
    3. ç‰¹å¾é‡è¦æ€§åˆ†æ - åœ¨æ¨¡å‹ä¸­çš„é‡è¦ç¨‹åº¦
    4. å†—ä½™åº¦åˆ†æ - ä¸ç°æœ‰ç‰¹å¾çš„ç›¸å…³æ€§
    5. è®¡ç®—æˆæœ¬è¯„ä¼° - è®¡ç®—å¤æ‚åº¦åˆ†æ
    6. é‡‘èç†è®ºæ”¯æŒ - ç†è®ºåŸºç¡€è¯„åˆ†
    """
    
    def __init__(self, config: Optional[Config] = None, 
                 evaluator_config: Optional[FeatureEvaluatorConfig] = None):
        """
        åˆå§‹åŒ–ç‰¹å¾è¯„ä¼°å™¨
        
        Args:
            config: é¡¹ç›®é…ç½®
            evaluator_config: è¯„ä¼°å™¨ä¸“ç”¨é…ç½®
        """
        self.config = config or Config()
        self.evaluator_config = evaluator_config or FeatureEvaluatorConfig()
        
        # åˆå§‹åŒ–æ—¥å¿—
        self.logger = setup_logger(
            name="FeatureEvaluator",
            level="INFO",
            log_file=get_default_log_file("feature_evaluator")
        )
        
        # è¯„ä¼°å†å²è®°å½•
        self.evaluation_history = []
        self.feature_rankings = {}
        
        self.logger.info("FeatureEvaluator (Experiment #005) initialized successfully")
    
    def evaluate_single_feature(self, 
                               feature_name: str,
                               baseline_results: List[Dict],
                               feature_results: List[Dict],
                               feature_data: Optional[pd.Series] = None,
                               existing_features: Optional[pd.DataFrame] = None) -> FeatureEvaluationResult:
        """
        è¯„ä¼°å•ä¸ªç‰¹å¾çš„ä»·å€¼
        
        Args:
            feature_name: ç‰¹å¾åç§°
            baseline_results: åŸºå‡†å®éªŒç»“æœåˆ—è¡¨
            feature_results: åŠ å…¥æ–°ç‰¹å¾åçš„å®éªŒç»“æœåˆ—è¡¨
            feature_data: ç‰¹å¾æ•°æ®ï¼ˆç”¨äºé‡è¦æ€§å’Œå†—ä½™åº¦åˆ†æï¼‰
            existing_features: ç°æœ‰ç‰¹å¾æ•°æ®
        
        Returns:
            FeatureEvaluationResult: è¯„ä¼°ç»“æœ
        """
        
        self.logger.info(f"å¼€å§‹è¯„ä¼°ç‰¹å¾: {feature_name}")
        
        # æå–å…³é”®æŒ‡æ ‡
        baseline_returns = [r.get('mean_return', 0) for r in baseline_results]
        feature_returns = [r.get('mean_return', 0) for r in feature_results]
        
        # è®¡ç®—å„ä¸ªè¯„ä¼°ç»´åº¦çš„åˆ†æ•°
        scores = {}
        evaluation_details = {}
        
        # 1. æ€§èƒ½æ”¹è¿›è¯„åˆ†
        performance_result = self._evaluate_performance_improvement(
            baseline_returns, feature_returns
        )
        scores['performance'] = performance_result['score']
        evaluation_details['performance'] = performance_result
        
        # 2. ç»Ÿè®¡æ˜¾è‘—æ€§è¯„åˆ†
        significance_result = self._evaluate_statistical_significance(
            baseline_returns, feature_returns
        )
        scores['significance'] = significance_result['score']
        p_value = significance_result['p_value']
        effect_size = significance_result['effect_size']
        confidence_interval = significance_result['confidence_interval']
        evaluation_details['significance'] = significance_result
        
        # 3. ç‰¹å¾é‡è¦æ€§è¯„åˆ†
        importance_result = self._evaluate_feature_importance(
            feature_name, feature_data
        )
        scores['importance'] = importance_result['score']
        evaluation_details['importance'] = importance_result
        
        # 4. å†—ä½™åº¦è¯„åˆ†
        redundancy_result = self._evaluate_redundancy(
            feature_data, existing_features
        )
        scores['redundancy'] = redundancy_result['score']
        evaluation_details['redundancy'] = redundancy_result
        
        # 5. è®¡ç®—æˆæœ¬è¯„åˆ†
        cost_result = self._evaluate_computational_cost(feature_name)
        scores['cost'] = cost_result['score']
        evaluation_details['cost'] = cost_result
        
        # 6. é‡‘èç†è®ºæ”¯æŒè¯„åˆ†
        theory_result = self._evaluate_theory_support(feature_name)
        scores['theory'] = theory_result['score']
        evaluation_details['theory'] = theory_result
        
        # è®¡ç®—ç»¼åˆè¯„åˆ†
        overall_score = sum(
            scores[dimension] * self.evaluator_config.evaluation_weights[dimension]
            for dimension in self.evaluator_config.evaluation_weights
        )
        
        # ç”Ÿæˆæ¨è
        recommendation = self._make_recommendation(
            overall_score, p_value, effect_size
        )
        
        # åˆ›å»ºç»“æœ
        result = FeatureEvaluationResult(
            feature_name=feature_name,
            overall_score=overall_score,
            individual_scores=scores,
            recommendation=recommendation,
            p_value=p_value,
            effect_size=effect_size,
            confidence_interval=confidence_interval,
            evaluation_details=evaluation_details
        )
        
        # è®°å½•è¯„ä¼°å†å²
        self.evaluation_history.append(result)
        self.feature_rankings[feature_name] = overall_score
        
        self.logger.info(f"ç‰¹å¾ {feature_name} è¯„ä¼°å®Œæˆ: {overall_score:.3f} ({recommendation})")
        
        return result
    
    def _evaluate_performance_improvement(self, baseline: List[float], 
                                        feature: List[float]) -> Dict[str, Any]:
        """è¯„ä¼°æ€§èƒ½æ”¹è¿›"""
        if not baseline or not feature:
            return {'score': 0.0, 'improvement': 0.0, 'baseline_mean': 0.0, 'feature_mean': 0.0}
        
        baseline_mean = np.mean(baseline)
        feature_mean = np.mean(feature)
        
        # ç›¸å¯¹æ”¹è¿›ç‡
        if baseline_mean != 0:
            improvement = (feature_mean - baseline_mean) / abs(baseline_mean)
        else:
            improvement = feature_mean - baseline_mean
        
        # è½¬æ¢ä¸º0-1åˆ†æ•°
        # æ”¹è¿›>20%å¾—æ»¡åˆ†ï¼Œæ”¹è¿›<-10%å¾—0åˆ†
        if improvement >= 0.2:
            score = 1.0
        elif improvement <= -0.1:
            score = 0.0
        else:
            score = (improvement + 0.1) / 0.3
        
        return {
            'score': max(0.0, min(1.0, score)),
            'improvement': improvement,
            'baseline_mean': baseline_mean,
            'feature_mean': feature_mean
        }
    
    def _evaluate_statistical_significance(self, baseline: List[float], 
                                         feature: List[float]) -> Dict:
        """è¯„ä¼°ç»Ÿè®¡æ˜¾è‘—æ€§"""
        if len(baseline) < 3 or len(feature) < 3:
            return {
                'score': 0.0,
                'p_value': 1.0,
                'effect_size': 0.0,
                'confidence_interval': (0.0, 0.0),
                't_statistic': 0.0,
                'test_type': 'insufficient_data'
            }
        
        try:
            # è¿›è¡Œtæ£€éªŒ
            t_stat, p_value = stats.ttest_ind(feature, baseline, equal_var=False)
            
            # è®¡ç®—æ•ˆåº”å¤§å° (Cohen's d)
            pooled_std = np.sqrt((np.var(baseline) + np.var(feature)) / 2)
            if pooled_std > 0:
                effect_size = (np.mean(feature) - np.mean(baseline)) / pooled_std
            else:
                effect_size = 0.0
            
            # è®¡ç®—ç½®ä¿¡åŒºé—´
            diff_mean = np.mean(feature) - np.mean(baseline)
            diff_se = np.sqrt(np.var(feature)/len(feature) + np.var(baseline)/len(baseline))
            confidence_interval = (
                diff_mean - 1.96 * diff_se,
                diff_mean + 1.96 * diff_se
            )
            
            # è½¬æ¢ä¸ºè¯„åˆ†
            if p_value < 0.01:
                score = 1.0
            elif p_value < 0.05:
                score = 0.8
            elif p_value < 0.1:
                score = 0.5
            else:
                score = 0.0
            
            return {
                'score': score,
                'p_value': p_value,
                'effect_size': effect_size,
                'confidence_interval': confidence_interval,
                't_statistic': t_stat,
                'test_type': 'welch_t_test'
            }
        
        except Exception as e:
            self.logger.warning(f"ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒé”™è¯¯: {e}")
            return {
                'score': 0.0,
                'p_value': 1.0,
                'effect_size': 0.0,
                'confidence_interval': (0.0, 0.0),
                't_statistic': 0.0,
                'test_type': 'error'
            }
    
    def _evaluate_feature_importance(self, feature_name: str, 
                                   feature_data: Optional[pd.Series]) -> Dict[str, Any]:
        """è¯„ä¼°ç‰¹å¾é‡è¦æ€§"""
        if feature_data is None:
            return {'score': 0.5, 'method': 'default', 'cv': None, 'information_gain': None}
        
        try:
            # è®¡ç®—ç‰¹å¾çš„å˜å¼‚ç³»æ•°ä½œä¸ºé‡è¦æ€§æŒ‡æ ‡
            if len(feature_data) > 1:
                mean_val = np.mean(feature_data)
                std_val = np.std(feature_data)
                cv = std_val / (abs(mean_val) + 1e-8)
                
                # è®¡ç®—ä¿¡æ¯å¢ç›Šä»£ç†æŒ‡æ ‡
                # ä½¿ç”¨åˆ†ä½æ•°å·®å¼‚ä½œä¸ºä¿¡æ¯å¢ç›Šçš„ä»£ç†
                q75 = np.percentile(feature_data, 75)
                q25 = np.percentile(feature_data, 25)
                iqr = q75 - q25
                information_gain = iqr / (std_val + 1e-8) if std_val > 0 else 0
                
                # å˜å¼‚ç³»æ•°åœ¨0.1-2ä¹‹é—´æ—¶é‡è¦æ€§è¾ƒé«˜
                if 0.1 <= cv <= 2.0:
                    cv_score = 0.8
                elif cv > 2.0:
                    cv_score = 0.6  # è¿‡äºæ³¢åŠ¨å¯èƒ½æ˜¯å™ªå£°
                else:
                    cv_score = 0.4  # å˜åŒ–å¤ªå°å¯èƒ½ä¿¡æ¯é‡ä¸è¶³
                
                # ä¿¡æ¯å¢ç›ŠåŠ æˆ
                ig_score = min(1.0, information_gain * 0.5)
                
                final_score = (cv_score * 0.7 + ig_score * 0.3)
                
                return {
                    'score': final_score,
                    'method': 'coefficient_of_variation_with_information_gain',
                    'cv': cv,
                    'information_gain': information_gain,
                    'cv_score': cv_score,
                    'ig_score': ig_score
                }
            else:
                return {'score': 0.5, 'method': 'insufficient_data', 'cv': None, 'information_gain': None}
                
        except Exception as e:
            self.logger.warning(f"ç‰¹å¾é‡è¦æ€§è¯„ä¼°é”™è¯¯: {e}")
            return {'score': 0.5, 'method': 'error', 'cv': None, 'information_gain': None}
    
    def _evaluate_redundancy(self, feature_data: Optional[pd.Series], 
                           existing_features: Optional[pd.DataFrame]) -> Dict[str, Any]:
        """è¯„ä¼°ç‰¹å¾å†—ä½™åº¦"""
        if feature_data is None or existing_features is None:
            return {'score': 0.8, 'max_correlation': 0.0, 'correlated_features': []}
        
        try:
            max_correlation = 0.0
            correlated_features = []
            
            for col in existing_features.columns:
                if len(feature_data) == len(existing_features[col]):
                    # å¤„ç†NaNå€¼
                    valid_indices = pd.notna(feature_data) & pd.notna(existing_features[col])
                    if valid_indices.sum() > 10:  # è‡³å°‘éœ€è¦10ä¸ªæœ‰æ•ˆæ•°æ®ç‚¹
                        corr = np.corrcoef(
                            feature_data[valid_indices], 
                            existing_features[col][valid_indices]
                        )[0, 1]
                        
                        if np.isfinite(corr):
                            abs_corr = abs(corr)
                            if abs_corr > max_correlation:
                                max_correlation = abs_corr
                            if abs_corr > 0.7:  # é«˜ç›¸å…³æ€§é˜ˆå€¼
                                correlated_features.append((col, corr))
            
            # ç›¸å…³æ€§è¶Šä½ï¼Œå†—ä½™åº¦åˆ†æ•°è¶Šé«˜
            if max_correlation < 0.3:
                score = 1.0
            elif max_correlation < 0.5:
                score = 0.8
            elif max_correlation < 0.7:
                score = 0.5
            else:
                score = 0.2
            
            return {
                'score': score,
                'max_correlation': max_correlation,
                'correlated_features': correlated_features,
                'method': 'pearson_correlation'
            }
            
        except Exception as e:
            self.logger.warning(f"å†—ä½™åº¦è¯„ä¼°é”™è¯¯: {e}")
            return {'score': 0.8, 'max_correlation': 0.0, 'correlated_features': []}
    
    def _evaluate_computational_cost(self, feature_name: str) -> Dict[str, Any]:
        """è¯„ä¼°è®¡ç®—æˆæœ¬"""
        # ç®€åŒ–çš„è®¡ç®—æˆæœ¬è¯„ä¼°
        cost_mapping = {
            # ä½æˆæœ¬
            'simple': ['SMA', 'EMA', 'RSI', 'Price', 'Volume', 'Close', 'Open', 'High', 'Low'],
            # ä¸­ç­‰æˆæœ¬
            'medium': ['MACD', 'ATR', 'BB', 'Williams_R', 'CCI', 'Stochastic'],
            # é«˜æˆæœ¬
            'high': ['ADX', 'Parabolic_SAR', 'Ichimoku', 'GARCH', 'Wavelet'],
            # å¾ˆé«˜æˆæœ¬
            'very_high': ['FFT', 'Neural', 'Complex', 'Multi_timeframe', 'Cross_correlation']
        }
        
        feature_lower = feature_name.lower()
        cost_level = 'unknown'
        
        for level, keywords in cost_mapping.items():
            if any(keyword.lower() in feature_lower for keyword in keywords):
                cost_level = level
                break
        
        score_mapping = {
            'simple': 1.0,
            'medium': 0.8,
            'high': 0.6,
            'very_high': 0.4,
            'unknown': 0.7
        }
        
        score = score_mapping[cost_level]
        
        return {
            'score': score,
            'cost_level': cost_level,
            'method': 'keyword_based_estimation'
        }
    
    def _evaluate_theory_support(self, feature_name: str) -> Dict[str, Any]:
        """è¯„ä¼°é‡‘èç†è®ºæ”¯æŒ"""
        feature_base = feature_name.split('_')[0] if '_' in feature_name else feature_name
        
        score = self.evaluator_config.theory_scores.get(
            feature_name, 
            self.evaluator_config.theory_scores.get(
                feature_base, 
                self.evaluator_config.theory_scores['default']
            )
        )
        
        return {
            'score': score,
            'matched_feature': feature_name if feature_name in self.evaluator_config.theory_scores else feature_base,
            'method': 'theory_score_mapping'
        }
    
    def _make_recommendation(self, score: float, p_value: float, 
                           effect_size: float) -> str:
        """åŸºäºè¯„åˆ†ç”Ÿæˆæ¨è"""
        if score >= 0.8 and p_value < 0.05 and abs(effect_size) > 0.2:
            return "ACCEPT - å¼ºçƒˆæ¨èä½¿ç”¨"
        elif score >= 0.6 and p_value < 0.1:
            return "CONDITIONAL - è°¨æ…ä½¿ç”¨ï¼Œéœ€è¦è¿›ä¸€æ­¥éªŒè¯"
        elif score >= 0.4:
            return "UNCERTAIN - ä»·å€¼ä¸æ˜ç¡®ï¼Œå»ºè®®æ›´å¤šæµ‹è¯•"
        else:
            return "REJECT - ä¸æ¨èä½¿ç”¨"
    
    def batch_evaluate_features(self, 
                              candidate_features: Dict[str, str],
                              baseline_results: List[Dict],
                              feature_test_results: Dict[str, List[Dict]],
                              feature_data: Optional[pd.DataFrame] = None) -> List[FeatureEvaluationResult]:
        """
        æ‰¹é‡è¯„ä¼°å€™é€‰ç‰¹å¾
        
        Args:
            candidate_features: å€™é€‰ç‰¹å¾æ˜ å°„ {name: description}
            baseline_results: åŸºå‡†æµ‹è¯•ç»“æœ
            feature_test_results: æ¯ä¸ªç‰¹å¾çš„æµ‹è¯•ç»“æœ {feature_name: results}
            feature_data: ç‰¹å¾æ•°æ®
        
        Returns:
            List[FeatureEvaluationResult]: æ’åºåçš„è¯„ä¼°ç»“æœåˆ—è¡¨
        """
        results = []
        
        self.logger.info(f"å¼€å§‹æ‰¹é‡è¯„ä¼° {len(candidate_features)} ä¸ªç‰¹å¾")
        
        for feature_name in candidate_features:
            if feature_name in feature_test_results:
                # è·å–ç‰¹å¾æ•°æ®
                single_feature_data = None
                if feature_data is not None and feature_name in feature_data.columns:
                    single_feature_data = feature_data[feature_name]
                
                # è·å–ç°æœ‰ç‰¹å¾æ•°æ®ï¼ˆæ’é™¤å½“å‰ç‰¹å¾ï¼‰
                existing_features = None
                if feature_data is not None:
                    existing_features = feature_data.drop(columns=[feature_name], errors='ignore')
                
                # è¯„ä¼°ç‰¹å¾
                result = self.evaluate_single_feature(
                    feature_name=feature_name,
                    baseline_results=baseline_results,
                    feature_results=feature_test_results[feature_name],
                    feature_data=single_feature_data,
                    existing_features=existing_features
                )
                
                results.append(result)
        
        # æŒ‰ç»¼åˆè¯„åˆ†æ’åº
        results.sort(key=lambda x: x.overall_score, reverse=True)
        
        self.logger.info(f"æ‰¹é‡è¯„ä¼°å®Œæˆï¼Œå…±è¯„ä¼° {len(results)} ä¸ªç‰¹å¾")
        
        return results
    
    def select_best_features(self, 
                           evaluation_results: List[FeatureEvaluationResult],
                           max_features: int = 2) -> List[str]:
        """
        ä»è¯„ä¼°ç»“æœä¸­é€‰æ‹©æœ€ä½³ç‰¹å¾
        
        Args:
            evaluation_results: ç‰¹å¾è¯„ä¼°ç»“æœåˆ—è¡¨
            max_features: æœ€å¤§é€‰æ‹©ç‰¹å¾æ•°
        
        Returns:
            List[str]: é€‰æ‹©çš„ç‰¹å¾åç§°åˆ—è¡¨
        """
        # è¿‡æ»¤å‡ºæ¨èä½¿ç”¨çš„ç‰¹å¾
        recommended = [
            result for result in evaluation_results
            if result.recommendation.startswith("ACCEPT") or 
               (result.recommendation.startswith("CONDITIONAL") and 
                result.overall_score >= self.evaluator_config.thresholds['acceptance_score'])
        ]
        
        # é€‰æ‹©å‰Nä¸ªæœ€ä½³ç‰¹å¾
        selected = recommended[:max_features]
        
        selected_names = [result.feature_name for result in selected]
        
        self.logger.info(f"é€‰æ‹©äº† {len(selected_names)} ä¸ªæœ€ä½³ç‰¹å¾: {selected_names}")
        
        return selected_names
    
    def generate_evaluation_report(self, 
                                 results: List[FeatureEvaluationResult]) -> str:
        """ç”Ÿæˆç‰¹å¾è¯„ä¼°æŠ¥å‘Š"""
        report = []
        report.append("=" * 80)
        report.append("Experiment #005 ç‰¹å¾è¯„ä¼°æŠ¥å‘Š")
        report.append("=" * 80)
        report.append(f"è¯„ä¼°ç‰¹å¾æ•°é‡: {len(results)}")
        report.append(f"è¯„ä¼°æ—¶é—´: {pd.Timestamp.now()}")
        report.append("")
        
        for i, result in enumerate(results, 1):
            report.append(f"{i}. {result.feature_name}")
            report.append(f"   ç»¼åˆè¯„åˆ†: {result.overall_score:.3f}")
            report.append(f"   æ¨è: {result.recommendation}")
            report.append(f"   ç»Ÿè®¡æ˜¾è‘—æ€§: p={result.p_value:.4f}, effect_size={result.effect_size:.3f}")
            report.append(f"   ç½®ä¿¡åŒºé—´: [{result.confidence_interval[0]:.4f}, {result.confidence_interval[1]:.4f}]")
            report.append(f"   å„ç»´åº¦è¯„åˆ†:")
            
            for dimension, score in result.individual_scores.items():
                report.append(f"     {dimension}: {score:.3f}")
            
            report.append("")
        
        # æ±‡æ€»ç»Ÿè®¡
        accepted = sum(1 for r in results if r.recommendation.startswith("ACCEPT"))
        conditional = sum(1 for r in results if r.recommendation.startswith("CONDITIONAL"))
        rejected = sum(1 for r in results if r.recommendation.startswith("REJECT"))
        uncertain = sum(1 for r in results if r.recommendation.startswith("UNCERTAIN"))
        
        report.append("æ±‡æ€»ç»Ÿè®¡:")
        report.append(f"  å¼ºçƒˆæ¨è: {accepted} ä¸ªç‰¹å¾")
        report.append(f"  æ¡ä»¶æ¥å—: {conditional} ä¸ªç‰¹å¾")
        report.append(f"  ä»·å€¼ä¸æ˜: {uncertain} ä¸ªç‰¹å¾")
        report.append(f"  ä¸æ¨è: {rejected} ä¸ªç‰¹å¾")
        
        report.append("")
        report.append("è¯„ä¼°æ–¹æ³•:")
        report.append("  1. æ€§èƒ½æ”¹è¿›è¯„ä¼° - åŸºäºå®é™…äº¤æ˜“å›æŠ¥æå‡")
        report.append("  2. ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ - Welch tæ£€éªŒ")
        report.append("  3. ç‰¹å¾é‡è¦æ€§åˆ†æ - å˜å¼‚ç³»æ•°ä¸ä¿¡æ¯å¢ç›Š")
        report.append("  4. å†—ä½™åº¦åˆ†æ - Pearsonç›¸å…³æ€§æ£€éªŒ")
        report.append("  5. è®¡ç®—æˆæœ¬è¯„ä¼° - åŸºäºç‰¹å¾å¤æ‚åº¦")
        report.append("  6. é‡‘èç†è®ºæ”¯æŒ - åŸºäºç†è®ºåŸºç¡€è¯„åˆ†")
        
        return "\n".join(report)
    
    def get_feature_rankings(self) -> Dict[str, float]:
        """è·å–ç‰¹å¾æ’å"""
        return dict(sorted(self.feature_rankings.items(), key=lambda x: x[1], reverse=True))
    
    def get_evaluation_summary(self) -> Dict[str, Any]:
        """è·å–è¯„ä¼°æ‘˜è¦"""
        if not self.evaluation_history:
            return {"status": "no_evaluations_performed"}
        
        scores = [r.overall_score for r in self.evaluation_history]
        recommendations = [r.recommendation for r in self.evaluation_history]
        
        return {
            "total_evaluations": len(self.evaluation_history),
            "mean_score": np.mean(scores),
            "median_score": np.median(scores),
            "std_score": np.std(scores),
            "score_range": [min(scores), max(scores)],
            "recommendation_counts": {
                "ACCEPT": sum(1 for r in recommendations if r.startswith("ACCEPT")),
                "CONDITIONAL": sum(1 for r in recommendations if r.startswith("CONDITIONAL")),
                "UNCERTAIN": sum(1 for r in recommendations if r.startswith("UNCERTAIN")),
                "REJECT": sum(1 for r in recommendations if r.startswith("REJECT"))
            },
            "top_features": list(self.get_feature_rankings().keys())[:5]
        }

# å·¥å‚å‡½æ•°
def create_feature_evaluator(config: Optional[Config] = None,
                           evaluator_config: Optional[FeatureEvaluatorConfig] = None) -> FeatureEvaluator:
    """åˆ›å»ºç‰¹å¾è¯„ä¼°å™¨çš„å·¥å‚æ–¹æ³•"""
    return FeatureEvaluator(config=config, evaluator_config=evaluator_config)

if __name__ == "__main__":
    # æµ‹è¯•ç‰¹å¾è¯„ä¼°å™¨
    print("ğŸ§ª æµ‹è¯•FeatureEvaluator (Experiment #005)...")
    
    evaluator = create_feature_evaluator()
    
    # æ¨¡æ‹Ÿæµ‹è¯•æ•°æ®
    baseline_results = [
        {'mean_return': -0.25, 'sharpe_ratio': -0.5},
        {'mean_return': -0.23, 'sharpe_ratio': -0.4},
        {'mean_return': -0.27, 'sharpe_ratio': -0.6}
    ]
    
    feature_results = [
        {'mean_return': -0.18, 'sharpe_ratio': -0.2},
        {'mean_return': -0.15, 'sharpe_ratio': -0.1},
        {'mean_return': -0.20, 'sharpe_ratio': -0.3}
    ]
    
    # è¯„ä¼°å•ä¸ªç‰¹å¾
    result = evaluator.evaluate_single_feature(
        feature_name="Williams_R_14",
        baseline_results=baseline_results,
        feature_results=feature_results
    )
    
    print(f"SUCCESS: ç‰¹å¾è¯„ä¼°å®Œæˆ")
    print(f"   ç‰¹å¾åç§°: {result.feature_name}")
    print(f"   ç»¼åˆè¯„åˆ†: {result.overall_score:.3f}")
    print(f"   æ¨è: {result.recommendation}")
    print(f"   på€¼: {result.p_value:.4f}")
    print(f"   æ•ˆåº”å¤§å°: {result.effect_size:.3f}")
    
    # è·å–è¯„ä¼°æ‘˜è¦
    summary = evaluator.get_evaluation_summary()
    print(f"   è¯„ä¼°æ‘˜è¦: {summary['total_evaluations']} ä¸ªç‰¹å¾å·²è¯„ä¼°")
    
    print("\nğŸ¯ FeatureEvaluator (Experiment #005) å‡†å¤‡å°±ç»ª!")