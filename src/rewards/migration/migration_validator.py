"""
è¿ç§»éªŒè¯å™¨ - éªŒè¯è¿ç§»çš„å¥–åŠ±å‡½æ•°æ˜¯å¦æ­£ç¡®å·¥ä½œ
"""

import unittest
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from pathlib import Path

from ..core.reward_context import RewardContext
from .compatibility_mapper import CompatibilityMapper


@dataclass
class ValidationResult:
    """éªŒè¯ç»“æœ"""
    function_name: str
    passed: bool
    test_count: int
    passed_count: int
    failed_count: int
    errors: List[str] = None
    warnings: List[str] = None
    performance_metrics: Dict[str, float] = None
    
    def __post_init__(self):
        if self.errors is None:
            self.errors = []
        if self.warnings is None:
            self.warnings = []
        if self.performance_metrics is None:
            self.performance_metrics = {}


class MigrationValidator:
    """è¿ç§»éªŒè¯å™¨ - éªŒè¯è¿ç§»åçš„å¥–åŠ±å‡½æ•°"""
    
    def __init__(self):
        self.compatibility_mapper = CompatibilityMapper()
        
        # æ ‡å‡†æµ‹è¯•ç”¨ä¾‹
        self.standard_test_cases = self._create_standard_test_cases()
        
        # æ€§èƒ½åŸºå‡†
        self.performance_benchmarks = {
            'max_computation_time_ms': 10.0,  # æœ€å¤§è®¡ç®—æ—¶é—´
            'max_memory_usage_mb': 100.0,     # æœ€å¤§å†…å­˜ä½¿ç”¨
            'min_reward_range': -1000.0,      # æœ€å°å¥–åŠ±èŒƒå›´
            'max_reward_range': 1000.0,       # æœ€å¤§å¥–åŠ±èŒƒå›´
        }
    
    def validate_all_migrated_functions(self, functions_dir: str) -> List[ValidationResult]:
        """éªŒè¯æ‰€æœ‰è¿ç§»çš„å‡½æ•°"""
        print("ğŸ” å¼€å§‹éªŒè¯è¿ç§»çš„å¥–åŠ±å‡½æ•°...")
        
        functions_path = Path(functions_dir)
        results = []
        
        # æŸ¥æ‰¾æ‰€æœ‰Pythonæ–‡ä»¶
        for py_file in functions_path.rglob("*.py"):
            if py_file.name.startswith("__"):
                continue
                
            try:
                # å°è¯•å¯¼å…¥å’ŒéªŒè¯
                result = self._validate_single_file(py_file)
                if result:
                    results.append(result)
            except Exception as e:
                print(f"âš ï¸ éªŒè¯æ–‡ä»¶å¤±è´¥ {py_file}: {e}")
        
        # æ‰“å°æ€»ç»“
        self._print_validation_summary(results)
        
        return results
    
    def validate_single_function(self, reward_class, function_name: str = None) -> ValidationResult:
        """éªŒè¯å•ä¸ªå¥–åŠ±å‡½æ•°"""
        if function_name is None:
            function_name = reward_class.__name__
            
        print(f"ğŸ§ª éªŒè¯å‡½æ•°: {function_name}")
        
        errors = []
        warnings = []
        test_results = []
        
        try:
            # åˆ›å»ºå®ä¾‹
            reward_instance = reward_class()
            
            # è¿è¡Œæ ‡å‡†æµ‹è¯•
            test_results = self._run_standard_tests(reward_instance)
            
            # æ€§èƒ½æµ‹è¯•
            perf_metrics = self._run_performance_tests(reward_instance)
            
            # å…¼å®¹æ€§æµ‹è¯•
            compat_results = self._run_compatibility_tests(reward_instance)
            test_results.extend(compat_results)
            
            # ç»Ÿè®¡ç»“æœ
            passed_count = sum(1 for result in test_results if result['passed'])
            failed_count = len(test_results) - passed_count
            
            # æ”¶é›†é”™è¯¯å’Œè­¦å‘Š
            for result in test_results:
                if not result['passed']:
                    errors.append(f"{result['test_name']}: {result['error']}")
                if 'warning' in result:
                    warnings.append(f"{result['test_name']}: {result['warning']}")
            
            return ValidationResult(
                function_name=function_name,
                passed=failed_count == 0,
                test_count=len(test_results),
                passed_count=passed_count,
                failed_count=failed_count,
                errors=errors,
                warnings=warnings,
                performance_metrics=perf_metrics
            )
            
        except Exception as e:
            return ValidationResult(
                function_name=function_name,
                passed=False,
                test_count=0,
                passed_count=0,
                failed_count=1,
                errors=[f"åˆå§‹åŒ–å¤±è´¥: {str(e)}"]
            )
    
    def _validate_single_file(self, py_file: Path) -> Optional[ValidationResult]:
        """éªŒè¯å•ä¸ªPythonæ–‡ä»¶"""
        # åŠ¨æ€å¯¼å…¥æ–‡ä»¶ä¸­çš„ç±»
        try:
            import importlib.util
            import sys
            
            spec = importlib.util.spec_from_file_location("temp_module", py_file)
            module = importlib.util.module_from_spec(spec)
            sys.modules["temp_module"] = module
            spec.loader.exec_module(module)
            
            # æŸ¥æ‰¾å¥–åŠ±ç±»
            reward_classes = []
            for attr_name in dir(module):
                attr = getattr(module, attr_name)
                if (isinstance(attr, type) and 
                    hasattr(attr, 'calculate') and 
                    attr.__name__ != 'BaseReward'):
                    reward_classes.append(attr)
            
            # éªŒè¯æ‰¾åˆ°çš„ç±»
            if reward_classes:
                # é€šå¸¸ä¸€ä¸ªæ–‡ä»¶åªæœ‰ä¸€ä¸ªä¸»è¦çš„å¥–åŠ±ç±»
                main_class = reward_classes[0]
                return self.validate_single_function(main_class)
            
        except Exception as e:
            print(f"âš ï¸ å¯¼å…¥æ–‡ä»¶å¤±è´¥ {py_file}: {e}")
            
        return None
    
    def _create_standard_test_cases(self) -> List[RewardContext]:
        """åˆ›å»ºæ ‡å‡†æµ‹è¯•ç”¨ä¾‹"""
        test_cases = []
        
        # åŸºæœ¬æµ‹è¯•ç”¨ä¾‹
        test_cases.append(RewardContext(
            portfolio_value=10000.0,
            action=0.0,
            current_price=1.0,
            step=0
        ))
        
        # ç›ˆåˆ©æƒ…å†µ
        test_cases.append(RewardContext(
            portfolio_value=11000.0,
            action=0.5,
            current_price=1.1,
            step=100,
            portfolio_history=np.array([10000, 10500, 11000])
        ))
        
        # äºæŸæƒ…å†µ
        test_cases.append(RewardContext(
            portfolio_value=9000.0,
            action=-0.5,
            current_price=0.9,
            step=100,
            portfolio_history=np.array([10000, 9500, 9000])
        ))
        
        # è¾¹ç•Œæƒ…å†µ
        test_cases.append(RewardContext(
            portfolio_value=0.0,
            action=1.0,
            current_price=0.001,
            step=1000000
        ))
        
        # å¤–æ±‡ç‰¹å®šæµ‹è¯•
        test_cases.append(RewardContext(
            portfolio_value=10500.0,
            action=0.3,
            current_price=1.2345,
            step=50,
            market_type='forex',
            granularity='1min',
            metadata={'pip_size': 0.0001, 'spread': 2, 'leverage': 100}
        ))
        
        return test_cases
    
    def _run_standard_tests(self, reward_instance) -> List[Dict[str, Any]]:
        """è¿è¡Œæ ‡å‡†æµ‹è¯•"""
        test_results = []
        
        for i, test_case in enumerate(self.standard_test_cases):
            try:
                # æµ‹è¯•åŸºæœ¬è®¡ç®—
                result = reward_instance.calculate(test_case)
                
                test_result = {
                    'test_name': f'standard_test_{i+1}',
                    'passed': True,
                    'result': result
                }
                
                # éªŒè¯ç»“æœç±»å‹
                if not isinstance(result, (int, float)):
                    test_result['passed'] = False
                    test_result['error'] = f"è¿”å›ç±»å‹é”™è¯¯: {type(result)}, æœŸæœ› float"
                
                # éªŒè¯ç»“æœèŒƒå›´
                elif not np.isfinite(result):
                    test_result['passed'] = False
                    test_result['error'] = f"è¿”å›å€¼æ— æ•ˆ: {result}"
                
                # éªŒè¯ç»“æœèŒƒå›´
                elif not (self.performance_benchmarks['min_reward_range'] <= 
                         result <= self.performance_benchmarks['max_reward_range']):
                    test_result['warning'] = f"å¥–åŠ±å€¼è¶…å‡ºå»ºè®®èŒƒå›´: {result}"
                
                test_results.append(test_result)
                
            except Exception as e:
                test_results.append({
                    'test_name': f'standard_test_{i+1}',
                    'passed': False,
                    'error': str(e)
                })
        
        return test_results
    
    def _run_performance_tests(self, reward_instance) -> Dict[str, float]:
        """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
        import time
        import psutil
        import os
        
        metrics = {}
        
        try:
            # å‡†å¤‡æµ‹è¯•æ•°æ®
            test_context = self.standard_test_cases[1]  # ä½¿ç”¨æœ‰å†å²æ•°æ®çš„æµ‹è¯•ç”¨ä¾‹
            
            # æµ‹è¯•è®¡ç®—æ—¶é—´
            start_time = time.perf_counter()
            for _ in range(1000):  # è¿è¡Œ1000æ¬¡
                reward_instance.calculate(test_context)
            end_time = time.perf_counter()
            
            avg_time_ms = (end_time - start_time) * 1000 / 1000
            metrics['avg_computation_time_ms'] = avg_time_ms
            metrics['computation_time_ok'] = avg_time_ms <= self.performance_benchmarks['max_computation_time_ms']
            
            # æµ‹è¯•å†…å­˜ä½¿ç”¨
            process = psutil.Process(os.getpid())
            memory_before = process.memory_info().rss / 1024 / 1024  # MB
            
            # åˆ›å»ºå¤§é‡å®ä¾‹æµ‹è¯•å†…å­˜æ³„æ¼
            instances = [reward_instance.__class__() for _ in range(100)]
            for instance in instances:
                instance.calculate(test_context)
            
            memory_after = process.memory_info().rss / 1024 / 1024  # MB
            memory_usage = memory_after - memory_before
            
            metrics['memory_usage_mb'] = memory_usage
            metrics['memory_usage_ok'] = memory_usage <= self.performance_benchmarks['max_memory_usage_mb']
            
            del instances  # æ¸…ç†
            
        except Exception as e:
            metrics['performance_test_error'] = str(e)
        
        return metrics
    
    def _run_compatibility_tests(self, reward_instance) -> List[Dict[str, Any]]:
        """è¿è¡Œå…¼å®¹æ€§æµ‹è¯•"""
        test_results = []
        
        # æµ‹è¯•å¿…éœ€æ–¹æ³•å­˜åœ¨
        required_methods = ['calculate', 'get_info']
        for method_name in required_methods:
            test_result = {
                'test_name': f'has_method_{method_name}',
                'passed': hasattr(reward_instance, method_name),
            }
            if not test_result['passed']:
                test_result['error'] = f"ç¼ºå°‘å¿…éœ€æ–¹æ³•: {method_name}"
            test_results.append(test_result)
        
        # æµ‹è¯•get_infoæ–¹æ³•
        try:
            info = reward_instance.get_info()
            test_result = {
                'test_name': 'get_info_test',
                'passed': isinstance(info, dict),
            }
            if not test_result['passed']:
                test_result['error'] = f"get_infoè¿”å›ç±»å‹é”™è¯¯: {type(info)}"
            elif 'name' not in info:
                test_result['warning'] = "get_infoç»“æœç¼ºå°‘'name'å­—æ®µ"
            test_results.append(test_result)
        except Exception as e:
            test_results.append({
                'test_name': 'get_info_test',
                'passed': False,
                'error': f"get_infoæ–¹æ³•è°ƒç”¨å¤±è´¥: {str(e)}"
            })
        
        # æµ‹è¯•å‘åå…¼å®¹æ€§ï¼ˆå¦‚æœæ”¯æŒï¼‰
        if hasattr(reward_instance, 'compute_reward'):
            try:
                old_context = {
                    'portfolio_value': 10000.0,
                    'action': 0.5,
                    'current_price': 1.0,
                    'step': 10
                }
                result = reward_instance.compute_reward(old_context)
                test_results.append({
                    'test_name': 'backward_compatibility',
                    'passed': isinstance(result, (int, float)),
                })
            except Exception as e:
                test_results.append({
                    'test_name': 'backward_compatibility',
                    'passed': False,
                    'error': f"å‘åå…¼å®¹æ€§æµ‹è¯•å¤±è´¥: {str(e)}"
                })
        
        return test_results
    
    def _print_validation_summary(self, results: List[ValidationResult]):
        """æ‰“å°éªŒè¯æ€»ç»“"""
        total_functions = len(results)
        passed_functions = sum(1 for r in results if r.passed)
        failed_functions = total_functions - passed_functions
        
        print("\n" + "="*60)
        print("ğŸ“Š å¥–åŠ±å‡½æ•°éªŒè¯æŠ¥å‘Š")
        print("="*60)
        
        print(f"\nğŸ“ˆ æ€»ä½“æƒ…å†µ:")
        print(f"  â€¢ éªŒè¯å‡½æ•°æ•°é‡: {total_functions}")
        print(f"  â€¢ é€šè¿‡éªŒè¯: {passed_functions}")
        print(f"  â€¢ éªŒè¯å¤±è´¥: {failed_functions}")
        print(f"  â€¢ æˆåŠŸç‡: {passed_functions/total_functions*100:.1f}%" if total_functions > 0 else "N/A")
        
        if failed_functions > 0:
            print(f"\nâŒ éªŒè¯å¤±è´¥çš„å‡½æ•°:")
            for result in results:
                if not result.passed:
                    print(f"  â€¢ {result.function_name}: {result.failed_count} ä¸ªæµ‹è¯•å¤±è´¥")
                    for error in result.errors[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªé”™è¯¯
                        print(f"    - {error}")
        
        # æ€§èƒ½ç»Ÿè®¡
        avg_compute_times = [r.performance_metrics.get('avg_computation_time_ms', 0) 
                           for r in results if r.performance_metrics]
        if avg_compute_times:
            print(f"\nâš¡ æ€§èƒ½ç»Ÿè®¡:")
            print(f"  â€¢ å¹³å‡è®¡ç®—æ—¶é—´: {np.mean(avg_compute_times):.2f} ms")
            print(f"  â€¢ æœ€å¤§è®¡ç®—æ—¶é—´: {np.max(avg_compute_times):.2f} ms")
            print(f"  â€¢ æœ€å°è®¡ç®—æ—¶é—´: {np.min(avg_compute_times):.2f} ms")
        
        # è­¦å‘Šæ±‡æ€»
        all_warnings = []
        for result in results:
            all_warnings.extend(result.warnings)
        
        if all_warnings:
            print(f"\nâš ï¸  å¸¸è§è­¦å‘Š (æ˜¾ç¤ºå‰5ä¸ª):")
            warning_counts = {}
            for warning in all_warnings:
                warning_counts[warning] = warning_counts.get(warning, 0) + 1
            
            for warning, count in sorted(warning_counts.items(), 
                                       key=lambda x: x[1], reverse=True)[:5]:
                print(f"  â€¢ {warning} ({count}æ¬¡)")
        
        print("\n" + "="*60)
    
    def create_validation_script(self, output_file: str = "validate_migrated_rewards.py"):
        """åˆ›å»ºéªŒè¯è„šæœ¬"""
        script_content = f'''#!/usr/bin/env python3
"""
è¿ç§»å¥–åŠ±å‡½æ•°éªŒè¯è„šæœ¬
è‡ªåŠ¨ç”Ÿæˆäº: {self._get_timestamp()}
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.rewards.migration.migration_validator import MigrationValidator


def main():
    print("ğŸ” å¼€å§‹éªŒè¯è¿ç§»çš„å¥–åŠ±å‡½æ•°...")
    
    # åˆ›å»ºéªŒè¯å™¨
    validator = MigrationValidator()
    
    # éªŒè¯æ‰€æœ‰è¿ç§»çš„å‡½æ•°
    results = validator.validate_all_migrated_functions("src/rewards/functions")
    
    # åˆ†æç»“æœ
    total = len(results)
    passed = sum(1 for r in results if r.passed)
    failed = total - passed
    
    print(f"\\nğŸ“Š éªŒè¯å®Œæˆ!")
    print(f"  âœ… é€šè¿‡: {{passed}}/{{total}}")
    print(f"  âŒ å¤±è´¥: {{failed}}/{{total}}")
    
    if failed > 0:
        print(f"\\nâŒ éœ€è¦ä¿®å¤çš„å‡½æ•°:")
        for result in results:
            if not result.passed:
                print(f"  â€¢ {{result.function_name}}")
                for error in result.errors[:2]:
                    print(f"    - {{error}}")
        
        return False
    else:
        print("\\nğŸ‰ æ‰€æœ‰å‡½æ•°éªŒè¯é€šè¿‡!")
        return True


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
'''
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(script_content)
        
        import os
        os.chmod(output_file, 0o755)
        print(f"âœ… éªŒè¯è„šæœ¬å·²åˆ›å»º: {output_file}")
    
    def _get_timestamp(self) -> str:
        """è·å–æ—¶é—´æˆ³"""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")