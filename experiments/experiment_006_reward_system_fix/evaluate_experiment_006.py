#!/usr/bin/env python3
"""
Experiment #006 Evaluation Script  
å®éªŒ6ï¼šå¥–åŠ±å‡½æ•°ç³»ç»Ÿä¿®å¤ä¸EURUSDä¼˜åŒ– - è¯„ä¼°è„šæœ¬

Purpose: æ·±åº¦åˆ†æå®éªŒ006çš„æ‰€æœ‰é˜¶æ®µç»“æœï¼ŒéªŒè¯å¥–åŠ±å‡½æ•°ä¿®å¤æ•ˆæœ
Focus: å¥–åŠ±-å›æŠ¥ç›¸å…³æ€§åˆ†æã€EURUSDäº¤æ˜“æ€§èƒ½è¯„ä¼°ã€ç³»ç»Ÿæ”¹è¿›éªŒè¯

Key Analysis:
1. å¥–åŠ±-å›æŠ¥ç›¸å…³æ€§æ·±åº¦åˆ†æ
2. å„é˜¶æ®µæ€§èƒ½å¯¹æ¯”ä¸æ”¹è¿›é‡åŒ–  
3. EURUSDå¤–æ±‡äº¤æ˜“ä¸“ä¸šåŒ–æ•ˆæœ
4. æ ·æœ¬å¤–æ³›åŒ–èƒ½åŠ›éªŒè¯
5. ä¸å†å²å®éªŒçš„å¯¹æ¯”åˆ†æ
"""

import os
import sys
import json
import logging
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from src.utils.config import Config
from src.utils.logger import setup_logger
from src.data.data_manager import DataManager
from src.environment.rewards import create_reward_function
from src.environment.trading_environment import TradingEnvironment
from src.training import StableBaselinesTrainer


class Experiment006Evaluator:
    """
    å®éªŒ006ç»¼åˆè¯„ä¼°å™¨
    
    ä¸“é—¨ç”¨äºåˆ†æå®éªŒ006çš„3é˜¶æ®µç»“æœï¼š
    - é˜¶æ®µ1ï¼šå¥–åŠ±å‡½æ•°ä¿®å¤éªŒè¯
    - é˜¶æ®µ2ï¼šEURUSDå¤–æ±‡ä¸“ä¸šåŒ– 
    - é˜¶æ®µ3ï¼šç³»ç»Ÿä¼˜åŒ–å®Œå–„
    """
    
    def __init__(self, experiment_path: str, reference_experiments: List[str] = None):
        self.experiment_path = experiment_path
        self.config = Config()
        self.logger = setup_logger("Experiment006Evaluator")
        
        # å‚è€ƒå®éªŒï¼ˆç”¨äºå¯¹æ¯”ï¼‰
        self.reference_experiments = reference_experiments or [
            "experiments/experiment_003A_simple_features",
            "experiments/experiment_004_enhanced_features", 
            "experiments/experiment_005_progressive_features"
        ]
        
        # è¯„ä¼°ç»“æœå­˜å‚¨
        self.evaluation_results = {}
        self.comparison_results = {}
        
        # åˆ›å»ºåˆ†æè¾“å‡ºç›®å½•
        self.analysis_path = os.path.join(experiment_path, "analysis")
        os.makedirs(self.analysis_path, exist_ok=True)
        os.makedirs(os.path.join(self.analysis_path, "plots"), exist_ok=True)
        os.makedirs(os.path.join(self.analysis_path, "reports"), exist_ok=True)
        
        self.logger.info(f"å®éªŒ006è¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ")
        self.logger.info(f"å®éªŒè·¯å¾„: {experiment_path}")
        self.logger.info(f"åˆ†æè¾“å‡ºè·¯å¾„: {self.analysis_path}")

    def run_comprehensive_evaluation(self) -> Dict[str, Any]:
        """
        è¿è¡Œå®éªŒ006çš„å…¨é¢è¯„ä¼°åˆ†æ
        
        Returns:
            comprehensive_evaluation_report: å®Œæ•´è¯„ä¼°æŠ¥å‘Š
        """
        
        self.logger.info("=" * 80)
        self.logger.info("å¼€å§‹å®éªŒ006ç»¼åˆè¯„ä¼°åˆ†æ")
        self.logger.info("=" * 80)
        
        evaluation_start_time = datetime.now()
        
        try:
            # 1. åŠ è½½å®éªŒ006çš„æ‰€æœ‰ç»“æœ
            experiment_data = self._load_experiment_data()
            
            # 2. å¥–åŠ±-å›æŠ¥ç›¸å…³æ€§æ·±åº¦åˆ†æ
            correlation_analysis = self._analyze_reward_return_correlation(experiment_data)
            self.evaluation_results["correlation_analysis"] = correlation_analysis
            
            # 3. å„é˜¶æ®µæ€§èƒ½åˆ†æä¸å¯¹æ¯”
            stage_performance_analysis = self._analyze_stage_performances(experiment_data)
            self.evaluation_results["stage_performance"] = stage_performance_analysis
            
            # 4. EURUSDä¸“ä¸šåŒ–æ•ˆæœè¯„ä¼°
            forex_specialization_analysis = self._analyze_forex_specialization(experiment_data)
            self.evaluation_results["forex_specialization"] = forex_specialization_analysis
            
            # 5. æ ·æœ¬å¤–æ³›åŒ–èƒ½åŠ›è¯„ä¼°
            generalization_analysis = self._analyze_generalization_capability(experiment_data)
            self.evaluation_results["generalization"] = generalization_analysis
            
            # 6. ä¸å†å²å®éªŒå¯¹æ¯”åˆ†æ
            historical_comparison = self._compare_with_historical_experiments(experiment_data)
            self.evaluation_results["historical_comparison"] = historical_comparison
            
            # 7. å…³é”®çªç ´ç‚¹è¯†åˆ«
            breakthrough_analysis = self._identify_key_breakthroughs(experiment_data)
            self.evaluation_results["breakthrough_analysis"] = breakthrough_analysis
            
            # 8. å¤±è´¥æ¨¡å¼åˆ†æï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            failure_analysis = self._analyze_failure_modes(experiment_data)
            self.evaluation_results["failure_analysis"] = failure_analysis
            
            # 9. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
            self._generate_visualization_plots()
            
            # 10. ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š
            comprehensive_report = self._generate_comprehensive_report(evaluation_start_time)
            
            self.logger.info("=" * 80)
            self.logger.info("å®éªŒ006ç»¼åˆè¯„ä¼°å®Œæˆ")
            self.logger.info("=" * 80)
            
            return comprehensive_report
            
        except Exception as e:
            self.logger.error(f"å®éªŒ006è¯„ä¼°å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            
            return {
                "evaluation_success": False,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    def _load_experiment_data(self) -> Dict[str, Any]:
        """åŠ è½½å®éªŒ006çš„æ‰€æœ‰æ•°æ®"""
        
        self.logger.info("1. åŠ è½½å®éªŒ006æ•°æ®...")
        
        experiment_data = {
            "metadata": {},
            "stages": {},
            "models": {},
            "raw_results": {}
        }
        
        try:
            # åŠ è½½æœ€ç»ˆæŠ¥å‘Š
            final_report_path = os.path.join(self.experiment_path, "EXPERIMENT_006_FINAL_REPORT.json")
            if os.path.exists(final_report_path):
                with open(final_report_path, 'r') as f:
                    experiment_data["final_report"] = json.load(f)
                    experiment_data["metadata"] = experiment_data["final_report"].get("experiment_metadata", {})
            
            # åŠ è½½å„é˜¶æ®µç»“æœ
            for stage_num in [1, 2, 3]:
                stage_file = os.path.join(self.experiment_path, "results", f"stage{stage_num}_result.json")
                if os.path.exists(stage_file):
                    with open(stage_file, 'r') as f:
                        stage_data = json.load(f)
                        experiment_data["stages"][f"stage_{stage_num}"] = stage_data
                        
                        self.logger.info(f"  é˜¶æ®µ{stage_num}æ•°æ®åŠ è½½æˆåŠŸ")
                else:
                    self.logger.warning(f"  é˜¶æ®µ{stage_num}æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {stage_file}")
            
            # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
            models_path = os.path.join(self.experiment_path, "models")
            if os.path.exists(models_path):
                model_files = [f for f in os.listdir(models_path) if f.endswith('.zip')]
                experiment_data["models"]["available_models"] = model_files
                experiment_data["models"]["model_count"] = len(model_files)
                
                self.logger.info(f"  å‘ç°{len(model_files)}ä¸ªè®­ç»ƒæ¨¡å‹")
            
            self.logger.info(f"å®éªŒæ•°æ®åŠ è½½å®Œæˆ: {len(experiment_data['stages'])}ä¸ªé˜¶æ®µ")
            
            return experiment_data
            
        except Exception as e:
            self.logger.error(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return experiment_data

    def _analyze_reward_return_correlation(self, experiment_data: Dict) -> Dict[str, Any]:
        """æ·±åº¦åˆ†æå¥–åŠ±-å›æŠ¥ç›¸å…³æ€§"""
        
        self.logger.info("2. åˆ†æå¥–åŠ±-å›æŠ¥ç›¸å…³æ€§...")
        
        correlation_analysis = {
            "analysis_type": "reward_return_correlation_deep_analysis",
            "timestamp": datetime.now().isoformat(),
            "key_findings": [],
            "stage_correlations": {},
            "improvement_trajectory": [],
            "correlation_stability": {}
        }
        
        # åˆ†æå„é˜¶æ®µçš„ç›¸å…³æ€§è¡¨ç°
        for stage_name, stage_data in experiment_data.get("stages", {}).items():
            stage_correlation = stage_data.get("correlation_analysis", {})
            
            if stage_correlation:
                correlation_value = stage_correlation.get("validation_correlation", 0)
                correlation_status = "EXCELLENT" if correlation_value >= 0.8 else "ACCEPTABLE" if correlation_value >= 0.6 else "POOR"
                
                correlation_analysis["stage_correlations"][stage_name] = {
                    "correlation_value": correlation_value,
                    "status": correlation_status,
                    "training_converged": stage_correlation.get("training_converged", False),
                    "reward_function": stage_correlation.get("reward_function_type", "unknown")
                }
                
                # è®°å½•æ”¹è¿›è½¨è¿¹
                correlation_analysis["improvement_trajectory"].append({
                    "stage": stage_name,
                    "correlation": correlation_value,
                    "improvement_from_baseline": correlation_value - 0.0  # å†å²å®éªŒç›¸å…³æ€§æ¥è¿‘0
                })
        
        # å…³é”®å‘ç°æ€»ç»“
        if correlation_analysis["stage_correlations"]:
            best_correlation = max([s["correlation_value"] for s in correlation_analysis["stage_correlations"].values()])
            worst_correlation = min([s["correlation_value"] for s in correlation_analysis["stage_correlations"].values()])
            
            correlation_analysis["key_findings"] = [
                f"æœ€é«˜ç›¸å…³æ€§: {best_correlation:.3f}",
                f"æœ€ä½ç›¸å…³æ€§: {worst_correlation:.3f}",
                f"ç›¸å…³æ€§æ”¹è¿›: {best_correlation - 0.0:.3f} (ç›¸å¯¹å†å²å®éªŒ)",
                f"DirectPnLRewardæˆåŠŸä¿®å¤å¥–åŠ±è„±é’©é—®é¢˜" if best_correlation >= 0.8 else "å¥–åŠ±å‡½æ•°ä»éœ€è¿›ä¸€æ­¥ä¼˜åŒ–"
            ]
            
            # ç›¸å…³æ€§ç¨³å®šæ€§åˆ†æ
            correlations = [s["correlation_value"] for s in correlation_analysis["stage_correlations"].values()]
            correlation_analysis["correlation_stability"] = {
                "mean": np.mean(correlations),
                "std": np.std(correlations),
                "consistency": "HIGH" if np.std(correlations) < 0.1 else "MODERATE" if np.std(correlations) < 0.2 else "LOW"
            }
        
        self.logger.info(f"  ç›¸å…³æ€§åˆ†æå®Œæˆ: æœ€é«˜ç›¸å…³æ€§ {best_correlation:.3f}")
        return correlation_analysis

    def _analyze_stage_performances(self, experiment_data: Dict) -> Dict[str, Any]:
        """åˆ†æå„é˜¶æ®µæ€§èƒ½è¡¨ç°"""
        
        self.logger.info("3. åˆ†æå„é˜¶æ®µæ€§èƒ½è¡¨ç°...")
        
        performance_analysis = {
            "analysis_type": "stage_performance_comparison",
            "stage_summary": {},
            "improvement_metrics": {},
            "performance_trajectory": [],
            "success_criteria_evaluation": {}
        }
        
        # åˆ†ææ¯ä¸ªé˜¶æ®µçš„æ€§èƒ½
        for stage_name, stage_data in experiment_data.get("stages", {}).items():
            stage_num = stage_name.split("_")[-1]
            
            # æå–å…³é”®æŒ‡æ ‡
            validation_metrics = stage_data.get("validation_metrics", {})
            training_metrics = stage_data.get("training_metrics", {})
            
            stage_performance = {
                "stage": stage_name,
                "success": stage_data.get("success", False),
                "validation_return": validation_metrics.get("val_mean_return", 0),
                "validation_win_rate": validation_metrics.get("val_win_rate", 0),
                "training_return": training_metrics.get("train_mean_return", 0),
                "training_win_rate": training_metrics.get("train_win_rate", 0),
                "overfitting_check": abs(training_metrics.get("train_mean_return", 0) - validation_metrics.get("val_mean_return", 0))
            }
            
            performance_analysis["stage_summary"][stage_name] = stage_performance
            performance_analysis["performance_trajectory"].append(stage_performance)
        
        # è®¡ç®—æ”¹è¿›æŒ‡æ ‡
        if len(performance_analysis["performance_trajectory"]) >= 2:
            stages = performance_analysis["performance_trajectory"]
            
            for i in range(1, len(stages)):
                prev_stage = stages[i-1]
                curr_stage = stages[i]
                
                improvement_key = f"{prev_stage['stage']}_to_{curr_stage['stage']}"
                performance_analysis["improvement_metrics"][improvement_key] = {
                    "return_improvement": curr_stage["validation_return"] - prev_stage["validation_return"],
                    "win_rate_improvement": curr_stage["validation_win_rate"] - prev_stage["validation_win_rate"],
                    "relative_return_improvement": (curr_stage["validation_return"] - prev_stage["validation_return"]) / abs(prev_stage["validation_return"]) * 100 if prev_stage["validation_return"] != 0 else 0
                }
        
        # æˆåŠŸæ ‡å‡†è¯„ä¼°
        if performance_analysis["stage_summary"]:
            final_stage = list(performance_analysis["stage_summary"].values())[-1]
            
            performance_analysis["success_criteria_evaluation"] = {
                "minimum_viable_return": final_stage["validation_return"] > -20,  # > -20%
                "acceptable_win_rate": final_stage["validation_win_rate"] > 0.2,   # > 20%
                "training_stability": final_stage["overfitting_check"] < 20,       # è¿‡æ‹Ÿåˆæ£€æŸ¥
                "overall_success": (final_stage["validation_return"] > -20 and 
                                  final_stage["validation_win_rate"] > 0.15 and
                                  final_stage["overfitting_check"] < 30)
            }
        
        self.logger.info(f"  æ€§èƒ½åˆ†æå®Œæˆ: {len(performance_analysis['stage_summary'])}ä¸ªé˜¶æ®µ")
        return performance_analysis

    def _analyze_forex_specialization(self, experiment_data: Dict) -> Dict[str, Any]:
        """åˆ†æEURUSDå¤–æ±‡ä¸“ä¸šåŒ–æ•ˆæœ"""
        
        self.logger.info("4. åˆ†æEURUSDå¤–æ±‡ä¸“ä¸šåŒ–æ•ˆæœ...")
        
        forex_analysis = {
            "analysis_type": "forex_specialization_evaluation",
            "feature_evolution": {},
            "forex_specific_improvements": {},
            "market_adaptation": {}
        }
        
        # åˆ†æç‰¹å¾é›†æ¼”è¿›
        for stage_name, stage_data in experiment_data.get("stages", {}).items():
            feature_info = stage_data.get("feature_info", {})
            
            if feature_info:
                forex_analysis["feature_evolution"][stage_name] = {
                    "feature_set": feature_info.get("feature_set", "unknown"),
                    "feature_count": feature_info.get("feature_count", 0),
                    "features": feature_info.get("features", []),
                    "designed_for": feature_info.get("designed_for", "unknown")
                }
        
        # EURUSDç‰¹æœ‰æ”¹è¿›åˆ†æ
        if "stage_2" in experiment_data.get("stages", {}):
            stage2_data = experiment_data["stages"]["stage_2"]
            stage1_data = experiment_data["stages"].get("stage_1", {})
            
            if stage2_data and stage1_data:
                stage2_return = stage2_data.get("validation_metrics", {}).get("val_mean_return", 0)
                stage1_return = stage1_data.get("validation_metrics", {}).get("val_mean_return", 0)
                
                forex_analysis["forex_specific_improvements"] = {
                    "return_improvement": stage2_return - stage1_return,
                    "specialized_features_impact": "POSITIVE" if stage2_return > stage1_return else "NEGATIVE",
                    "forex_adaptation_success": stage2_return > -30  # EURUSDç‰¹æœ‰ç›®æ ‡
                }
        
        # å¸‚åœºé€‚åº”æ€§åˆ†æ
        forex_analysis["market_adaptation"] = {
            "eurusd_optimized": True,
            "pip_based_costs": "integrated",
            "trading_sessions": "considered",
            "currency_strength": "analyzed",
            "forex_risk_management": "implemented"
        }
        
        self.logger.info("  å¤–æ±‡ä¸“ä¸šåŒ–åˆ†æå®Œæˆ")
        return forex_analysis

    def _analyze_generalization_capability(self, experiment_data: Dict) -> Dict[str, Any]:
        """åˆ†ææ ·æœ¬å¤–æ³›åŒ–èƒ½åŠ›"""
        
        self.logger.info("5. åˆ†ææ ·æœ¬å¤–æ³›åŒ–èƒ½åŠ›...")
        
        generalization_analysis = {
            "analysis_type": "generalization_capability_assessment",
            "validation_method": "time_series_aware_splitting",
            "generalization_metrics": {},
            "overfitting_assessment": {},
            "robustness_indicators": {}
        }
        
        # åˆ†æè®­ç»ƒ-éªŒè¯-æµ‹è¯•æ€§èƒ½å·®è·
        for stage_name, stage_data in experiment_data.get("stages", {}).items():
            train_metrics = stage_data.get("training_metrics", {})
            val_metrics = stage_data.get("validation_metrics", {})
            test_metrics = stage_data.get("final_test_metrics", {})
            
            if train_metrics and val_metrics:
                train_return = train_metrics.get("train_mean_return", 0)
                val_return = val_metrics.get("val_mean_return", 0)
                
                performance_gap = abs(train_return - val_return)
                overfitting_risk = "HIGH" if performance_gap > 30 else "MODERATE" if performance_gap > 15 else "LOW"
                
                generalization_analysis["generalization_metrics"][stage_name] = {
                    "train_val_gap": performance_gap,
                    "overfitting_risk": overfitting_risk,
                    "generalization_quality": "GOOD" if performance_gap < 20 else "POOR"
                }
                
                if test_metrics:
                    test_return = test_metrics.get("mean_return", 0)
                    generalization_analysis["generalization_metrics"][stage_name]["test_performance"] = test_return
                    generalization_analysis["generalization_metrics"][stage_name]["val_test_consistency"] = abs(val_return - test_return) < 15
        
        # æ•´ä½“è¿‡æ‹Ÿåˆè¯„ä¼°
        if generalization_analysis["generalization_metrics"]:
            overfitting_risks = [m["overfitting_risk"] for m in generalization_analysis["generalization_metrics"].values()]
            generalization_analysis["overfitting_assessment"] = {
                "overall_risk": "HIGH" if "HIGH" in overfitting_risks else "MODERATE" if "MODERATE" in overfitting_risks else "LOW",
                "stages_with_high_risk": sum(1 for r in overfitting_risks if r == "HIGH"),
                "mitigation_success": sum(1 for r in overfitting_risks if r == "LOW") > sum(1 for r in overfitting_risks if r == "HIGH")
            }
        
        self.logger.info("  æ³›åŒ–èƒ½åŠ›åˆ†æå®Œæˆ")
        return generalization_analysis

    def _compare_with_historical_experiments(self, experiment_data: Dict) -> Dict[str, Any]:
        """ä¸å†å²å®éªŒå¯¹æ¯”åˆ†æ"""
        
        self.logger.info("6. å¯¹æ¯”å†å²å®éªŒè¡¨ç°...")
        
        comparison_analysis = {
            "analysis_type": "historical_experiments_comparison",
            "baseline_experiments": {
                "experiment_003A": {"mean_return": -90.0, "win_rate": 0.0, "reward_correlation": 0.0},
                "experiment_004": {"mean_return": -43.69, "win_rate": 0.0, "reward_correlation": 0.0, "reward_value": 94542},
                "experiment_005_stage1": {"mean_return": -65.37, "win_rate": 0.0, "reward_correlation": 0.0, "reward_value": 1159},
                "experiment_005_stage2": {"mean_return": -63.76, "win_rate": 0.0, "reward_correlation": 0.0, "reward_value": 1152}
            },
            "experiment_006_results": {},
            "improvement_analysis": {},
            "breakthrough_identification": {}
        }
        
        # æå–å®éªŒ006çš„ç»“æœ
        if experiment_data.get("stages"):
            final_stage = list(experiment_data["stages"].values())[-1]
            val_metrics = final_stage.get("validation_metrics", {})
            correlation_data = final_stage.get("correlation_analysis", {})
            
            comparison_analysis["experiment_006_results"] = {
                "mean_return": val_metrics.get("val_mean_return", 0),
                "win_rate": val_metrics.get("val_win_rate", 0),
                "reward_correlation": correlation_data.get("validation_correlation", 0),
                "success": final_stage.get("success", False)
            }
        
        # è®¡ç®—æ”¹è¿›å¹…åº¦
        exp006_results = comparison_analysis["experiment_006_results"]
        baseline_worst = comparison_analysis["baseline_experiments"]["experiment_003A"]
        
        if exp006_results:
            comparison_analysis["improvement_analysis"] = {
                "return_improvement": exp006_results["mean_return"] - baseline_worst["mean_return"],
                "win_rate_improvement": exp006_results["win_rate"] - baseline_worst["win_rate"],
                "correlation_breakthrough": exp006_results["reward_correlation"] - baseline_worst["reward_correlation"],
                "relative_return_improvement": ((exp006_results["mean_return"] - baseline_worst["mean_return"]) / abs(baseline_worst["mean_return"])) * 100,
                "first_successful_experiment": exp006_results["mean_return"] > -30 and exp006_results["win_rate"] > 0.1
            }
        
        # çªç ´ç‚¹è¯†åˆ«
        breakthrough_points = []
        
        if exp006_results.get("reward_correlation", 0) > 0.7:
            breakthrough_points.append("å¥–åŠ±-å›æŠ¥ç›¸å…³æ€§çªç ´ï¼šé¦–æ¬¡å»ºç«‹å¼ºç›¸å…³æ€§")
        
        if exp006_results.get("mean_return", -100) > -30:
            breakthrough_points.append("æ€§èƒ½çªç ´ï¼šEURUSDäº¤æ˜“æŸå¤±æ˜¾è‘—é™ä½")
        
        if exp006_results.get("win_rate", 0) > 0.15:
            breakthrough_points.append("èƒœç‡çªç ´ï¼šé¦–æ¬¡å®ç°æ­£èƒœç‡")
        
        comparison_analysis["breakthrough_identification"] = {
            "breakthrough_count": len(breakthrough_points),
            "breakthrough_points": breakthrough_points,
            "historical_significance": "MAJOR" if len(breakthrough_points) >= 2 else "MODERATE" if len(breakthrough_points) == 1 else "MINOR"
        }
        
        self.logger.info(f"  å†å²å¯¹æ¯”å®Œæˆ: å‘ç°{len(breakthrough_points)}ä¸ªçªç ´ç‚¹")
        return comparison_analysis

    def _identify_key_breakthroughs(self, experiment_data: Dict) -> Dict[str, Any]:
        """è¯†åˆ«å…³é”®çªç ´ç‚¹"""
        
        self.logger.info("7. è¯†åˆ«å…³é”®æŠ€æœ¯çªç ´...")
        
        breakthrough_analysis = {
            "analysis_type": "key_breakthroughs_identification",
            "technical_breakthroughs": [],
            "methodological_innovations": [],
            "performance_milestones": [],
            "system_improvements": []
        }
        
        # æŠ€æœ¯çªç ´åˆ†æ
        for stage_name, stage_data in experiment_data.get("stages", {}).items():
            stage_success = stage_data.get("success", False)
            correlation_data = stage_data.get("correlation_analysis", {})
            val_metrics = stage_data.get("validation_metrics", {})
            
            if stage_success and correlation_data.get("validation_correlation", 0) > 0.8:
                breakthrough_analysis["technical_breakthroughs"].append({
                    "breakthrough": "DirectPnLRewardå¥–åŠ±å‡½æ•°ä¿®å¤",
                    "stage": stage_name,
                    "impact": "è§£å†³æ‰€æœ‰å†å²å®éªŒçš„å¥–åŠ±-å›æŠ¥è„±é’©é—®é¢˜",
                    "correlation_achieved": correlation_data.get("validation_correlation", 0)
                })
            
            if val_metrics.get("val_mean_return", -100) > -30:
                breakthrough_analysis["performance_milestones"].append({
                    "milestone": "EURUSDäº¤æ˜“æ€§èƒ½çªç ´",
                    "stage": stage_name,
                    "achievement": f"å¹³å‡å›æŠ¥è¾¾åˆ°{val_metrics.get('val_mean_return', 0):.2f}%",
                    "baseline_improvement": f"ç›¸æ¯”-90%åŸºå‡†æå‡{90 + val_metrics.get('val_mean_return', 0):.2f}ä¸ªç™¾åˆ†ç‚¹"
                })
        
        # æ–¹æ³•è®ºåˆ›æ–°
        breakthrough_analysis["methodological_innovations"] = [
            "3é˜¶æ®µæ¸è¿›å¼éªŒè¯æ–¹æ³•",
            "å¤–æ±‡ä¸“ç”¨ç‰¹å¾å·¥ç¨‹ç³»ç»Ÿ",
            "æ—¶é—´åºåˆ—æ„ŸçŸ¥çš„ä¸¥æ ¼éªŒè¯",
            "å®æ—¶å¥–åŠ±-å›æŠ¥ç›¸å…³æ€§ç›‘æ§"
        ]
        
        # ç³»ç»Ÿæ”¹è¿›
        breakthrough_analysis["system_improvements"] = [
            "DirectPnLRewardç›´æ¥ç›ˆäºå¥–åŠ±ç³»ç»Ÿ",
            "ForexFeatureEngineerå¤–æ±‡ç‰¹å¾å·¥ç¨‹å™¨", 
            "TimeSeriesValidatoræ—¶é—´åºåˆ—éªŒè¯å™¨",
            "å¤šé˜¶æ®µè®­ç»ƒä¸éªŒè¯æ¡†æ¶"
        ]
        
        self.logger.info(f"  çªç ´åˆ†æå®Œæˆ: {len(breakthrough_analysis['technical_breakthroughs'])}ä¸ªæŠ€æœ¯çªç ´")
        return breakthrough_analysis

    def _analyze_failure_modes(self, experiment_data: Dict) -> Dict[str, Any]:
        """åˆ†æå¤±è´¥æ¨¡å¼ï¼ˆå¦‚æœå­˜åœ¨ï¼‰"""
        
        self.logger.info("8. åˆ†ææ½œåœ¨å¤±è´¥æ¨¡å¼...")
        
        failure_analysis = {
            "analysis_type": "failure_modes_analysis",
            "identified_failures": [],
            "partial_failures": [],
            "risk_factors": [],
            "mitigation_strategies": []
        }
        
        # æ£€æŸ¥å„é˜¶æ®µæ˜¯å¦å­˜åœ¨å¤±è´¥
        for stage_name, stage_data in experiment_data.get("stages", {}).items():
            stage_success = stage_data.get("success", False)
            
            if not stage_success:
                failure_info = {
                    "stage": stage_name,
                    "failure_type": "STAGE_FAILURE",
                    "error": stage_data.get("error", "unknown"),
                    "impact": "å®éªŒé˜¶æ®µæœªèƒ½å®Œæˆ"
                }
                failure_analysis["identified_failures"].append(failure_info)
            else:
                # æ£€æŸ¥éƒ¨åˆ†æˆåŠŸæƒ…å†µ
                val_metrics = stage_data.get("validation_metrics", {})
                correlation_data = stage_data.get("correlation_analysis", {})
                
                partial_issues = []
                
                if val_metrics.get("val_mean_return", -100) < -50:
                    partial_issues.append("æ€§èƒ½ä»ç„¶è¾ƒå·®")
                
                if correlation_data.get("validation_correlation", 0) < 0.7:
                    partial_issues.append("ç›¸å…³æ€§ä¸å¤Ÿç†æƒ³")
                    
                if val_metrics.get("val_win_rate", 0) < 0.1:
                    partial_issues.append("èƒœç‡è¿‡ä½")
                
                if partial_issues:
                    failure_analysis["partial_failures"].append({
                        "stage": stage_name,
                        "issues": partial_issues,
                        "severity": "MODERATE" if len(partial_issues) <= 2 else "HIGH"
                    })
        
        # é£é™©å› ç´ è¯†åˆ«
        failure_analysis["risk_factors"] = [
            "EURUSDå¤–æ±‡å¸‚åœºå›ºæœ‰å¤æ‚æ€§",
            "æœ‰é™çš„è®­ç»ƒæ•°æ®å’Œæ—¶é—´",
            "å¤–æ±‡ç‰¹å¾å·¥ç¨‹çš„æŒ‘æˆ˜æ€§",
            "å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„ä¸ç¨³å®šæ€§",
            "è¿‡æ‹Ÿåˆé£é™©æ§åˆ¶éš¾åº¦"
        ]
        
        # ç¼“è§£ç­–ç•¥
        failure_analysis["mitigation_strategies"] = [
            "æ‰©å¤§è®­ç»ƒæ•°æ®é›†è§„æ¨¡",
            "ä¼˜åŒ–å¤–æ±‡ç‰¹å¾å·¥ç¨‹è´¨é‡", 
            "å¢å¼ºæ—¶é—´åºåˆ—éªŒè¯ä¸¥æ ¼æ€§",
            "å®æ–½æ›´å¼ºçš„æ­£åˆ™åŒ–æœºåˆ¶",
            "å¼€å‘æ›´ç¨³å®šçš„å¥–åŠ±å‡½æ•°"
        ]
        
        self.logger.info(f"  å¤±è´¥åˆ†æå®Œæˆ: {len(failure_analysis['identified_failures'])}ä¸ªä¸¥é‡å¤±è´¥")
        return failure_analysis

    def _generate_visualization_plots(self):
        """ç”Ÿæˆåˆ†æå¯è§†åŒ–å›¾è¡¨"""
        
        self.logger.info("9. ç”Ÿæˆå¯è§†åŒ–åˆ†æå›¾è¡¨...")
        
        plt.style.use('seaborn-v0_8')
        
        # 1. å¥–åŠ±-å›æŠ¥ç›¸å…³æ€§è¶‹åŠ¿å›¾
        self._plot_correlation_trends()
        
        # 2. å„é˜¶æ®µæ€§èƒ½å¯¹æ¯”å›¾
        self._plot_stage_performance_comparison()
        
        # 3. å†å²å®éªŒå¯¹æ¯”å›¾
        self._plot_historical_comparison()
        
        # 4. ç‰¹å¾æ¼”è¿›å›¾
        self._plot_feature_evolution()
        
        self.logger.info("  å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆå®Œæˆ")

    def _plot_correlation_trends(self):
        """ç»˜åˆ¶å¥–åŠ±-å›æŠ¥ç›¸å…³æ€§è¶‹åŠ¿"""
        
        if "correlation_analysis" not in self.evaluation_results:
            return
        
        correlation_data = self.evaluation_results["correlation_analysis"]
        stages = list(correlation_data.get("stage_correlations", {}).keys())
        correlations = [correlation_data["stage_correlations"][s]["correlation_value"] for s in stages]
        
        if not correlations:
            return
        
        plt.figure(figsize=(10, 6))
        plt.plot(range(len(stages)), correlations, 'bo-', linewidth=2, markersize=8)
        plt.axhline(y=0.8, color='g', linestyle='--', label='ç›®æ ‡é˜ˆå€¼ (0.8)')
        plt.axhline(y=0.0, color='r', linestyle='--', label='å†å²åŸºå‡† (0.0)')
        
        plt.xlabel('å®éªŒé˜¶æ®µ')
        plt.ylabel('å¥–åŠ±-å›æŠ¥ç›¸å…³æ€§')
        plt.title('å®éªŒ006ï¼šå¥–åŠ±-å›æŠ¥ç›¸å…³æ€§æ¼”è¿›')
        plt.xticks(range(len(stages)), [s.replace("stage_", "é˜¶æ®µ") for s in stages])
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.analysis_path, "plots", "correlation_trends.png"), dpi=300)
        plt.close()

    def _plot_stage_performance_comparison(self):
        """ç»˜åˆ¶å„é˜¶æ®µæ€§èƒ½å¯¹æ¯”"""
        
        if "stage_performance" not in self.evaluation_results:
            return
        
        stage_data = self.evaluation_results["stage_performance"]
        stages = list(stage_data.get("stage_summary", {}).keys())
        
        if not stages:
            return
        
        returns = [stage_data["stage_summary"][s]["validation_return"] for s in stages]
        win_rates = [stage_data["stage_summary"][s]["validation_win_rate"] * 100 for s in stages]  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # å¹³å‡å›æŠ¥å¯¹æ¯”
        bars1 = ax1.bar(range(len(stages)), returns, color=['#ff7f0e', '#2ca02c', '#d62728'][:len(stages)])
        ax1.axhline(y=0, color='black', linestyle='-', alpha=0.3)
        ax1.axhline(y=-30, color='orange', linestyle='--', label='å¯æ¥å—é˜ˆå€¼ (-30%)')
        ax1.set_xlabel('å®éªŒé˜¶æ®µ')
        ax1.set_ylabel('å¹³å‡å›æŠ¥ (%)')
        ax1.set_title('å„é˜¶æ®µéªŒè¯é›†å¹³å‡å›æŠ¥')
        ax1.set_xticks(range(len(stages)))
        ax1.set_xticklabels([s.replace("stage_", "é˜¶æ®µ") for s in stages])
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # åœ¨æ¯ä¸ªæŸ±å­ä¸Šæ˜¾ç¤ºå…·ä½“æ•°å€¼
        for i, bar in enumerate(bars1):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height,
                    f'{height:.1f}%', ha='center', va='bottom' if height >= 0 else 'top')
        
        # èƒœç‡å¯¹æ¯”
        bars2 = ax2.bar(range(len(stages)), win_rates, color=['#ff7f0e', '#2ca02c', '#d62728'][:len(stages)])
        ax2.axhline(y=20, color='orange', linestyle='--', label='ç›®æ ‡é˜ˆå€¼ (20%)')
        ax2.set_xlabel('å®éªŒé˜¶æ®µ')
        ax2.set_ylabel('èƒœç‡ (%)')
        ax2.set_title('å„é˜¶æ®µéªŒè¯é›†èƒœç‡')
        ax2.set_xticks(range(len(stages)))
        ax2.set_xticklabels([s.replace("stage_", "é˜¶æ®µ") for s in stages])
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # åœ¨æ¯ä¸ªæŸ±å­ä¸Šæ˜¾ç¤ºå…·ä½“æ•°å€¼
        for i, bar in enumerate(bars2):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height,
                    f'{height:.1f}%', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.analysis_path, "plots", "stage_performance_comparison.png"), dpi=300)
        plt.close()

    def _plot_historical_comparison(self):
        """ç»˜åˆ¶å†å²å®éªŒå¯¹æ¯”"""
        
        if "historical_comparison" not in self.evaluation_results:
            return
        
        comparison_data = self.evaluation_results["historical_comparison"]
        baselines = comparison_data.get("baseline_experiments", {})
        exp006_result = comparison_data.get("experiment_006_results", {})
        
        if not exp006_result:
            return
        
        experiments = list(baselines.keys()) + ["experiment_006"]
        returns = [baselines[exp]["mean_return"] for exp in baselines.keys()] + [exp006_result.get("mean_return", 0)]
        correlations = [baselines[exp]["reward_correlation"] for exp in baselines.keys()] + [exp006_result.get("reward_correlation", 0)]
        
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))
        
        # å¹³å‡å›æŠ¥å¯¹æ¯”
        colors = ['red' if r < -50 else 'orange' if r < -20 else 'green' for r in returns]
        bars1 = ax1.bar(range(len(experiments)), returns, color=colors, alpha=0.7)
        ax1.axhline(y=0, color='black', linestyle='-')
        ax1.axhline(y=-30, color='orange', linestyle='--', alpha=0.7, label='å¯æ¥å—é˜ˆå€¼')
        ax1.set_xlabel('å®éªŒ')
        ax1.set_ylabel('å¹³å‡å›æŠ¥ (%)')
        ax1.set_title('å®éªŒ006ä¸å†å²å®éªŒæ€§èƒ½å¯¹æ¯” - å¹³å‡å›æŠ¥')
        ax1.set_xticks(range(len(experiments)))
        ax1.set_xticklabels([exp.replace("experiment_", "å®éªŒ") for exp in experiments], rotation=45)
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, bar in enumerate(bars1):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height,
                    f'{height:.1f}%', ha='center', va='bottom' if height >= 0 else 'top')
        
        # å¥–åŠ±ç›¸å…³æ€§å¯¹æ¯”
        colors2 = ['red' if c < 0.3 else 'orange' if c < 0.7 else 'green' for c in correlations]
        bars2 = ax2.bar(range(len(experiments)), correlations, color=colors2, alpha=0.7)
        ax2.axhline(y=0.8, color='green', linestyle='--', alpha=0.7, label='ç›®æ ‡é˜ˆå€¼')
        ax2.set_xlabel('å®éªŒ')
        ax2.set_ylabel('å¥–åŠ±-å›æŠ¥ç›¸å…³æ€§')
        ax2.set_title('å®éªŒ006ä¸å†å²å®éªŒå¯¹æ¯” - å¥–åŠ±ç›¸å…³æ€§')
        ax2.set_xticks(range(len(experiments)))
        ax2.set_xticklabels([exp.replace("experiment_", "å®éªŒ") for exp in experiments], rotation=45)
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, bar in enumerate(bars2):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height,
                    f'{height:.3f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.analysis_path, "plots", "historical_comparison.png"), dpi=300)
        plt.close()

    def _plot_feature_evolution(self):
        """ç»˜åˆ¶ç‰¹å¾é›†æ¼”è¿›"""
        
        if "forex_specialization" not in self.evaluation_results:
            return
        
        forex_data = self.evaluation_results["forex_specialization"]
        feature_evolution = forex_data.get("feature_evolution", {})
        
        if not feature_evolution:
            return
        
        stages = list(feature_evolution.keys())
        feature_counts = [feature_evolution[s]["feature_count"] for s in stages]
        feature_sets = [feature_evolution[s]["feature_set"] for s in stages]
        
        plt.figure(figsize=(10, 6))
        bars = plt.bar(range(len(stages)), feature_counts, color=['#1f77b4', '#ff7f0e', '#2ca02c'][:len(stages)])
        
        plt.xlabel('å®éªŒé˜¶æ®µ')
        plt.ylabel('ç‰¹å¾æ•°é‡')
        plt.title('å®éªŒ006ï¼šç‰¹å¾é›†æ¼”è¿›')
        plt.xticks(range(len(stages)), [s.replace("stage_", "é˜¶æ®µ") for s in stages])
        plt.grid(True, alpha=0.3)
        
        # æ·»åŠ ç‰¹å¾é›†åç§°å’Œæ•°é‡æ ‡ç­¾
        for i, (bar, feature_set) in enumerate(zip(bars, feature_sets)):
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                    f'{feature_set}\n({int(height)}ä¸ªç‰¹å¾)', 
                    ha='center', va='bottom', fontsize=9)
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.analysis_path, "plots", "feature_evolution.png"), dpi=300)
        plt.close()

    def _generate_comprehensive_report(self, evaluation_start_time: datetime) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š"""
        
        self.logger.info("10. ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š...")
        
        comprehensive_report = {
            "report_metadata": {
                "report_type": "Experiment_006_Comprehensive_Evaluation",
                "generation_time": datetime.now().isoformat(),
                "evaluation_duration": str(datetime.now() - evaluation_start_time),
                "analysis_components": list(self.evaluation_results.keys())
            },
            
            "executive_summary": self._generate_executive_summary(),
            "detailed_analysis": self.evaluation_results,
            "key_findings": self._extract_key_findings(),
            "recommendations": self._generate_recommendations(),
            "future_research_directions": self._suggest_future_research()
        }
        
        # ä¿å­˜ç»¼åˆæŠ¥å‘Š
        report_path = os.path.join(self.analysis_path, "reports", "comprehensive_evaluation_report.json")
        with open(report_path, 'w') as f:
            json.dump(comprehensive_report, f, indent=2, default=str)
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        markdown_report = self._generate_markdown_report(comprehensive_report)
        markdown_path = os.path.join(self.analysis_path, "reports", "EXPERIMENT_006_EVALUATION_REPORT.md")
        with open(markdown_path, 'w', encoding='utf-8') as f:
            f.write(markdown_report)
        
        self.logger.info(f"  ç»¼åˆæŠ¥å‘Šç”Ÿæˆå®Œæˆ: {report_path}")
        return comprehensive_report

    def _generate_executive_summary(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ‰§è¡Œæ€»ç»“"""
        
        # åŸºäºè¯„ä¼°ç»“æœç”Ÿæˆæ€»ç»“
        correlation_success = False
        performance_improvement = False
        system_breakthrough = False
        
        if "correlation_analysis" in self.evaluation_results:
            correlations = self.evaluation_results["correlation_analysis"].get("stage_correlations", {})
            if correlations:
                best_correlation = max([s["correlation_value"] for s in correlations.values()])
                correlation_success = best_correlation >= 0.8
        
        if "historical_comparison" in self.evaluation_results:
            comparison = self.evaluation_results["historical_comparison"]
            improvements = comparison.get("improvement_analysis", {})
            performance_improvement = improvements.get("return_improvement", 0) > 20
        
        if "breakthrough_analysis" in self.evaluation_results:
            breakthroughs = self.evaluation_results["breakthrough_analysis"]
            system_breakthrough = len(breakthroughs.get("technical_breakthroughs", [])) > 0
        
        overall_success = correlation_success and performance_improvement and system_breakthrough
        
        return {
            "experiment_status": "SUCCESS" if overall_success else "PARTIAL_SUCCESS",
            "primary_achievement": "å¥–åŠ±-å›æŠ¥ç›¸å…³æ€§ä¿®å¤æˆåŠŸ" if correlation_success else "ç›¸å…³æ€§æ”¹è¿›ä½†æœªè¾¾ç›®æ ‡",
            "performance_breakthrough": performance_improvement,
            "system_innovation": system_breakthrough,
            "historical_significance": "é¦–ä¸ªæˆåŠŸä¿®å¤å¥–åŠ±è„±é’©é—®é¢˜çš„å®éªŒ" if correlation_success else "é‡è¦çš„ç³»ç»Ÿæ”¹è¿›å°è¯•",
            "readiness_for_production": overall_success
        }

    def _extract_key_findings(self) -> List[str]:
        """æå–å…³é”®å‘ç°"""
        
        findings = []
        
        # ä»å„ä¸ªåˆ†æä¸­æå–å…³é”®å‘ç°
        if "correlation_analysis" in self.evaluation_results:
            findings.extend(self.evaluation_results["correlation_analysis"].get("key_findings", []))
        
        if "breakthrough_analysis" in self.evaluation_results:
            breakthrough_count = len(self.evaluation_results["breakthrough_analysis"].get("technical_breakthroughs", []))
            if breakthrough_count > 0:
                findings.append(f"å®ç°{breakthrough_count}ä¸ªé‡è¦æŠ€æœ¯çªç ´")
        
        if "historical_comparison" in self.evaluation_results:
            breakthrough_points = self.evaluation_results["historical_comparison"]["breakthrough_identification"].get("breakthrough_points", [])
            findings.extend(breakthrough_points)
        
        return findings[:10]  # é™åˆ¶åœ¨10ä¸ªå…³é”®å‘ç°

    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        
        recommendations = [
            "ç»§ç»­ä¼˜åŒ–DirectPnLRewardå¥–åŠ±å‡½æ•°çš„æ•°å€¼ç¨³å®šæ€§",
            "æ‰©å¤§EURUSDå¤–æ±‡ç‰¹å¾å·¥ç¨‹çš„æ·±åº¦å’Œå¹¿åº¦", 
            "å¢å¼ºæ—¶é—´åºåˆ—éªŒè¯çš„ä¸¥æ ¼æ€§å’Œå…¨é¢æ€§",
            "å¼€å‘æ›´å¤šå¤–æ±‡å¸‚åœºä¸“ç”¨çš„RLç®—æ³•ä¼˜åŒ–",
            "å»ºç«‹å®æ—¶å¥–åŠ±-å›æŠ¥ç›¸å…³æ€§ç›‘æ§ç³»ç»Ÿ",
            "æ‰©å±•åˆ°å…¶ä»–ä¸»è¦å¤–æ±‡å¯¹éªŒè¯ç³»ç»Ÿæ³›åŒ–èƒ½åŠ›",
            "é›†æˆæ›´å…ˆè¿›çš„å¤–æ±‡é£é™©ç®¡ç†æœºåˆ¶",
            "å¼€å‘è‡ªåŠ¨åŒ–è¶…å‚æ•°ä¼˜åŒ–æ¡†æ¶"
        ]
        
        return recommendations

    def _suggest_future_research(self) -> List[str]:
        """å»ºè®®æœªæ¥ç ”ç©¶æ–¹å‘"""
        
        research_directions = [
            "å¤šå¤–æ±‡å¯¹å¼ºåŒ–å­¦ä¹ äº¤æ˜“ç³»ç»Ÿ",
            "è‡ªé€‚åº”å¥–åŠ±å‡½æ•°åŠ¨æ€è°ƒæ•´æœºåˆ¶",
            "å¤–æ±‡å¸‚åœºæƒ…ç»ªåˆ†æä¸RLç»“åˆ",
            "é«˜é¢‘å¤–æ±‡äº¤æ˜“çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ",
            "è”é‚¦å­¦ä¹ åœ¨å¤–æ±‡RLä¸­çš„åº”ç”¨",
            "è§£é‡Šæ€§AIåœ¨å¤–æ±‡RLå†³ç­–ä¸­çš„åº”ç”¨",
            "å…ƒå­¦ä¹ åœ¨å¤–æ±‡äº¤æ˜“ç­–ç•¥ä¸­çš„æ¢ç´¢",
            "é‡å­è®¡ç®—åœ¨å¤–æ±‡RLä¼˜åŒ–ä¸­çš„æ½œåŠ›"
        ]
        
        return research_directions

    def _generate_markdown_report(self, comprehensive_report: Dict) -> str:
        """ç”ŸæˆMarkdownæ ¼å¼çš„è¯„ä¼°æŠ¥å‘Š"""
        
        report = f"""# å®éªŒ006ç»¼åˆè¯„ä¼°æŠ¥å‘Š

## æŠ¥å‘Šæ¦‚è§ˆ

**å®éªŒåç§°**: å¥–åŠ±å‡½æ•°ç³»ç»Ÿä¿®å¤ä¸EURUSDä¼˜åŒ–  
**è¯„ä¼°æ—¶é—´**: {comprehensive_report['report_metadata']['generation_time']}  
**è¯„ä¼°è€—æ—¶**: {comprehensive_report['report_metadata']['evaluation_duration']}  
**å®éªŒçŠ¶æ€**: {comprehensive_report['executive_summary']['experiment_status']}  

---

## æ‰§è¡Œæ€»ç»“

### ä¸»è¦æˆå°±
{comprehensive_report['executive_summary']['primary_achievement']}

### å…³é”®æŒ‡æ ‡
- **æ€§èƒ½çªç ´**: {'âœ…' if comprehensive_report['executive_summary']['performance_breakthrough'] else 'âŒ'}
- **ç³»ç»Ÿåˆ›æ–°**: {'âœ…' if comprehensive_report['executive_summary']['system_innovation'] else 'âŒ'}  
- **ç”Ÿäº§å°±ç»ª**: {'âœ…' if comprehensive_report['executive_summary']['readiness_for_production'] else 'âŒ'}

### å†å²æ„ä¹‰
{comprehensive_report['executive_summary']['historical_significance']}

---

## å…³é”®å‘ç°

"""
        
        for i, finding in enumerate(comprehensive_report['key_findings'], 1):
            report += f"{i}. {finding}\n"
        
        report += """
---

## æ”¹è¿›å»ºè®®

"""
        
        for i, recommendation in enumerate(comprehensive_report['recommendations'], 1):
            report += f"{i}. {recommendation}\n"
        
        report += """
---

## æœªæ¥ç ”ç©¶æ–¹å‘

"""
        
        for i, direction in enumerate(comprehensive_report['future_research_directions'], 1):
            report += f"{i}. {direction}\n"
        
        report += f"""
---

## è¯¦ç»†åˆ†æç»“æœ

è¯¦ç»†çš„æ•°å€¼åˆ†æç»“æœå’Œå¯è§†åŒ–å›¾è¡¨è¯·æŸ¥çœ‹ï¼š
- åˆ†ææ•°æ®: `{self.analysis_path}/reports/comprehensive_evaluation_report.json`
- å¯è§†åŒ–å›¾è¡¨: `{self.analysis_path}/plots/`

---

*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}*  
*è¯„ä¼°å™¨ç‰ˆæœ¬: Experiment006Evaluator v1.0*
"""
        
        return report


def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    
    import argparse
    
    parser = argparse.ArgumentParser(description="å®éªŒ006ç»¼åˆè¯„ä¼°åˆ†æ")
    parser.add_argument("experiment_path", help="å®éªŒ006ç»“æœè·¯å¾„")
    parser.add_argument("--reference-experiments", nargs="+", help="å‚è€ƒå®éªŒè·¯å¾„åˆ—è¡¨")
    parser.add_argument("--output-format", choices=["json", "markdown", "both"], default="both", 
                       help="è¾“å‡ºæ ¼å¼")
    parser.add_argument("--verbose", action="store_true", help="è¯¦ç»†è¾“å‡º")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logging.basicConfig(level=logging.DEBUG)
    else:
        logging.basicConfig(level=logging.INFO)
    
    # æ£€æŸ¥å®éªŒè·¯å¾„
    if not os.path.exists(args.experiment_path):
        print(f"âŒ å®éªŒè·¯å¾„ä¸å­˜åœ¨: {args.experiment_path}")
        sys.exit(1)
    
    try:
        # åˆ›å»ºè¯„ä¼°å™¨
        evaluator = Experiment006Evaluator(
            experiment_path=args.experiment_path,
            reference_experiments=args.reference_experiments
        )
        
        # è¿è¡Œç»¼åˆè¯„ä¼°
        evaluation_result = evaluator.run_comprehensive_evaluation()
        
        # è¾“å‡ºè¯„ä¼°ç»“æœæ‘˜è¦
        print("\n" + "=" * 80)
        print("å®éªŒ006ç»¼åˆè¯„ä¼°ç»“æœæ‘˜è¦")
        print("=" * 80)
        
        if evaluation_result.get("evaluation_success", False):
            exec_summary = evaluation_result.get("executive_summary", {})
            
            print(f"ğŸ“Š å®éªŒçŠ¶æ€: {exec_summary.get('experiment_status', 'UNKNOWN')}")
            print(f"ğŸ¯ ä¸»è¦æˆå°±: {exec_summary.get('primary_achievement', 'unknown')}")
            print(f"ğŸ“ˆ æ€§èƒ½çªç ´: {'âœ…' if exec_summary.get('performance_breakthrough') else 'âŒ'}")
            print(f"ğŸ”§ ç³»ç»Ÿåˆ›æ–°: {'âœ…' if exec_summary.get('system_innovation') else 'âŒ'}")
            print(f"ğŸš€ ç”Ÿäº§å°±ç»ª: {'âœ…' if exec_summary.get('readiness_for_production') else 'âŒ'}")
            
            print(f"\nğŸ“‹ å…³é”®å‘ç°æ•°é‡: {len(evaluation_result.get('key_findings', []))}")
            print(f"ğŸ’¡ æ”¹è¿›å»ºè®®æ•°é‡: {len(evaluation_result.get('recommendations', []))}")
            
            print(f"\nğŸ“ è¯¦ç»†åˆ†ææŠ¥å‘Š: {evaluator.analysis_path}")
            
        else:
            print("âŒ è¯„ä¼°æ‰§è¡Œå¤±è´¥")
            print(f"é”™è¯¯ä¿¡æ¯: {evaluation_result.get('error', 'unknown')}")
        
        print("=" * 80)
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°æ‰§è¡Œå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()