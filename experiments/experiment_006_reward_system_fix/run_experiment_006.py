#!/usr/bin/env python3
"""
Experiment #006 Execution Framework
å®éªŒ6å®Œæ•´æ‰§è¡Œæ¡†æ¶

Purpose: æä¾›å®éªŒ006çš„å®Œæ•´æ‰§è¡Œå’Œç®¡ç†æ¡†æ¶
åŒ…å«è®­ç»ƒã€è¯„ä¼°ã€ç›‘æ§ã€æŠ¥å‘Šçš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸç®¡ç†

Usage Examples:
  # è¿è¡Œå®Œæ•´å®éªŒï¼ˆæ¨èï¼‰
  python run_experiment_006.py --full-experiment
  
  # è¿è¡Œç‰¹å®šé˜¶æ®µ
  python run_experiment_006.py --stage 1
  python run_experiment_006.py --stage 2  
  python run_experiment_006.py --stage 3
  
  # è¿è¡Œå¹¶ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
  python run_experiment_006.py --full-experiment --detailed-analysis
  
  # å¿«é€ŸéªŒè¯æ¨¡å¼ï¼ˆå‡å°‘è®­ç»ƒæ­¥æ•°ï¼‰
  python run_experiment_006.py --stage 1 --quick-validation
"""

import os
import sys
import json
import logging
import argparse
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from src.utils.logger import setup_logger
from train_experiment_006 import Experiment006Trainer
from evaluate_experiment_006 import Experiment006Evaluator


class Experiment006Framework:
    """
    å®éªŒ006å®Œæ•´æ‰§è¡Œæ¡†æ¶
    
    æä¾›å®éªŒçš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼š
    - å®éªŒç¯å¢ƒå‡†å¤‡
    - å¤šé˜¶æ®µè®­ç»ƒæ‰§è¡Œ
    - å®æ—¶ç›‘æ§å’ŒçŠ¶æ€ç®¡ç†
    - ç»“æœè¯„ä¼°å’Œåˆ†æ
    - æŠ¥å‘Šç”Ÿæˆå’Œå­˜æ¡£
    """
    
    def __init__(self, config_path: str = None):
        self.base_dir = Path(__file__).parent
        self.config_path = config_path or str(self.base_dir / "experiment_006_config.json")
        
        # åŠ è½½é…ç½®
        with open(self.config_path, 'r', encoding='utf-8') as f:
            self.config = json.load(f)
        
        self.logger = setup_logger("Experiment006Framework")
        
        # åˆ›å»ºå®éªŒè¿è¡Œç›®å½•
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.run_dir = self.base_dir / f"run_{self.timestamp}"
        self.run_dir.mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–å­ç³»ç»Ÿ
        self.trainer = None
        self.evaluator = None
        
        # æ‰§è¡ŒçŠ¶æ€è·Ÿè¸ª
        self.execution_status = {
            "start_time": datetime.now().isoformat(),
            "current_stage": None,
            "completed_stages": [],
            "failed_stages": [],
            "overall_success": None
        }
        
        self.logger.info(f"å®éªŒ006æ¡†æ¶åˆå§‹åŒ–å®Œæˆ")
        self.logger.info(f"è¿è¡Œç›®å½•: {self.run_dir}")
        self.logger.info(f"é…ç½®æ–‡ä»¶: {self.config_path}")

    def run_full_experiment(self, detailed_analysis: bool = True, 
                          quick_validation: bool = False) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„å®éªŒ006
        
        Args:
            detailed_analysis: æ˜¯å¦è¿›è¡Œè¯¦ç»†åˆ†æ
            quick_validation: æ˜¯å¦ä½¿ç”¨å¿«é€ŸéªŒè¯æ¨¡å¼
            
        Returns:
            experiment_results: å®Œæ•´å®éªŒç»“æœ
        """
        
        self.logger.info("ğŸš€ å¼€å§‹å®éªŒ006å®Œæ•´æ‰§è¡Œ")
        self.logger.info("=" * 80)
        
        experiment_results = {
            "experiment_metadata": self.config["experiment_metadata"],
            "execution_config": {
                "detailed_analysis": detailed_analysis,
                "quick_validation": quick_validation,
                "execution_timestamp": self.timestamp,
                "run_directory": str(self.run_dir)
            },
            "stage_results": {},
            "final_analysis": {},
            "execution_summary": {}
        }
        
        try:
            # 1. ç¯å¢ƒå‡†å¤‡å’Œé…ç½®éªŒè¯
            self._prepare_experiment_environment()
            
            # 2. åˆ›å»ºè®­ç»ƒå™¨
            self.trainer = Experiment006Trainer(self.config_path)
            
            # 3. æ‰§è¡Œä¸‰é˜¶æ®µè®­ç»ƒ
            symbol = self.config["data_configuration"]["symbol"]
            period = self._get_data_period(quick_validation)
            
            training_results = self.trainer.run_complete_experiment(symbol, period)
            experiment_results["stage_results"] = training_results.get("stage_results", {})
            experiment_results["training_success"] = training_results.get("experiment_success", False)
            
            # 4. è¯¦ç»†åˆ†æå’Œè¯„ä¼°
            if detailed_analysis:
                self.logger.info("ğŸ“Š å¼€å§‹è¯¦ç»†åˆ†æå’Œè¯„ä¼°...")
                
                self.evaluator = Experiment006Evaluator(
                    experiment_path=str(self.trainer.experiment_path),
                    reference_experiments=self.config["comparison_and_analysis"]["baseline_experiments"]
                )
                
                evaluation_results = self.evaluator.run_comprehensive_evaluation()
                experiment_results["final_analysis"] = evaluation_results
            
            # 5. ç”Ÿæˆæ‰§è¡Œæ‘˜è¦
            execution_summary = self._generate_execution_summary(experiment_results)
            experiment_results["execution_summary"] = execution_summary
            
            # 6. ä¿å­˜æœ€ç»ˆç»“æœ
            self._save_experiment_results(experiment_results)
            
            # 7. ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            self._generate_final_report(experiment_results)
            
            self.logger.info("âœ… å®éªŒ006å®Œæ•´æ‰§è¡ŒæˆåŠŸå®Œæˆ")
            return experiment_results
            
        except Exception as e:
            self.logger.error(f"âŒ å®éªŒ006æ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            
            experiment_results["execution_error"] = str(e)
            experiment_results["execution_success"] = False
            
            return experiment_results



    def run_single_stage(self, stage_number: int, quick_validation: bool = False) -> Dict[str, Any]:
        """
        è¿è¡Œå•ä¸ªå®éªŒé˜¶æ®µ
        
        Args:
            stage_number: é˜¶æ®µç¼–å· (1, 2, 3)
            quick_validation: æ˜¯å¦ä½¿ç”¨å¿«é€ŸéªŒè¯æ¨¡å¼
            
        Returns:
            stage_results: å•é˜¶æ®µç»“æœ
        """
        
        self.logger.info(f"ğŸ¯ å¼€å§‹æ‰§è¡Œå®éªŒ006é˜¶æ®µ{stage_number}")
        
        if stage_number not in [1, 2, 3]:
            raise ValueError(f"æ— æ•ˆçš„é˜¶æ®µç¼–å·: {stage_number}. å¿…é¡»æ˜¯1, 2, æˆ–3")
        
        try:
            # ç¯å¢ƒå‡†å¤‡
            self._prepare_experiment_environment()
            
            # åˆ›å»ºè®­ç»ƒå™¨
            self.trainer = Experiment006Trainer(self.config_path)
            
            # è·å–æ•°æ®é…ç½®
            symbol = self.config["data_configuration"]["symbol"]
            period = self._get_data_period(quick_validation)
            
            # æ‰§è¡ŒæŒ‡å®šé˜¶æ®µ
            if stage_number == 1:
                stage_result = self.trainer.run_stage1_reward_fix(symbol, period)
            elif stage_number == 2:
                stage_result = self.trainer.run_stage2_forex_specialization(symbol, period)
            elif stage_number == 3:
                stage_result = self.trainer.run_stage3_system_optimization(symbol, period)
            
            # ä¿å­˜é˜¶æ®µç»“æœ
            stage_result_path = self.run_dir / f"stage_{stage_number}_result.json"
            with open(stage_result_path, 'w', encoding='utf-8') as f:
                json.dump(stage_result, f, indent=2, default=str)
            
            self.logger.info(f"âœ… é˜¶æ®µ{stage_number}æ‰§è¡Œå®Œæˆ")
            return stage_result
            
        except Exception as e:
            self.logger.error(f"âŒ é˜¶æ®µ{stage_number}æ‰§è¡Œå¤±è´¥: {e}")
            raise

    def run_analysis_only(self, experiment_path: str) -> Dict[str, Any]:
        """
        ä»…è¿è¡Œåˆ†æå’Œè¯„ä¼°ï¼ˆå¯¹å·²å®Œæˆçš„å®éªŒï¼‰
        
        Args:
            experiment_path: å·²å®Œæˆå®éªŒçš„è·¯å¾„
            
        Returns:
            analysis_results: åˆ†æç»“æœ
        """
        
        self.logger.info(f"ğŸ“Š å¼€å§‹åˆ†æå·²å®Œæˆçš„å®éªŒ: {experiment_path}")
        
        try:
            # åˆ›å»ºè¯„ä¼°å™¨
            self.evaluator = Experiment006Evaluator(
                experiment_path=experiment_path,
                reference_experiments=self.config["comparison_and_analysis"]["baseline_experiments"]
            )
            
            # è¿è¡Œç»¼åˆè¯„ä¼°
            analysis_results = self.evaluator.run_comprehensive_evaluation()
            
            # ä¿å­˜åˆ†æç»“æœ
            analysis_result_path = self.run_dir / "analysis_only_result.json"
            with open(analysis_result_path, 'w', encoding='utf-8') as f:
                json.dump(analysis_results, f, indent=2, default=str)
            
            self.logger.info("âœ… åˆ†æå®Œæˆ")
            return analysis_results
            
        except Exception as e:
            self.logger.error(f"âŒ åˆ†ææ‰§è¡Œå¤±è´¥: {e}")
            raise

    def validate_experiment_setup(self) -> Dict[str, Any]:
        """
        éªŒè¯å®éªŒç¯å¢ƒå’Œé…ç½®
        
        Returns:
            validation_results: éªŒè¯ç»“æœ
        """
        
        self.logger.info("ğŸ” éªŒè¯å®éªŒç¯å¢ƒå’Œé…ç½®...")
        
        validation_results = {
            "validation_timestamp": datetime.now().isoformat(),
            "config_validation": {},
            "dependency_validation": {},
            "data_validation": {},
            "environment_validation": {},
            "overall_validation": False
        }
        
        try:
            # 1. é…ç½®æ–‡ä»¶éªŒè¯
            config_validation = self._validate_configuration()
            validation_results["config_validation"] = config_validation
            
            # 2. ä¾èµ–éªŒè¯
            dependency_validation = self._validate_dependencies()
            validation_results["dependency_validation"] = dependency_validation
            
            # 3. æ•°æ®å¯ç”¨æ€§éªŒè¯
            data_validation = self._validate_data_availability()
            validation_results["data_validation"] = data_validation
            
            # 4. ç¯å¢ƒéªŒè¯
            environment_validation = self._validate_environment()
            validation_results["environment_validation"] = environment_validation
            
            # 5. æ•´ä½“éªŒè¯ç»“æœ
            all_validations = [
                config_validation.get("valid", False),
                dependency_validation.get("valid", False),
                data_validation.get("valid", False),
                environment_validation.get("valid", False)
            ]
            
            validation_results["overall_validation"] = all(all_validations)
            
            if validation_results["overall_validation"]:
                self.logger.info("âœ… å®éªŒç¯å¢ƒéªŒè¯é€šè¿‡")
            else:
                self.logger.warning("âš ï¸ å®éªŒç¯å¢ƒéªŒè¯å‘ç°é—®é¢˜")
            
            return validation_results
            
        except Exception as e:
            self.logger.error(f"âŒ ç¯å¢ƒéªŒè¯å¤±è´¥: {e}")
            validation_results["validation_error"] = str(e)
            return validation_results

    def _prepare_experiment_environment(self):
        """å‡†å¤‡å®éªŒç¯å¢ƒ"""
        
        self.logger.info("ğŸ”§ å‡†å¤‡å®éªŒç¯å¢ƒ...")
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        required_dirs = [
            self.run_dir / "logs",
            self.run_dir / "models", 
            self.run_dir / "results",
            self.run_dir / "analysis",
            self.run_dir / "plots"
        ]
        
        for dir_path in required_dirs:
            dir_path.mkdir(exist_ok=True)
        
        # ä¿å­˜é…ç½®å‰¯æœ¬
        config_copy_path = self.run_dir / "experiment_config_copy.json"
        with open(config_copy_path, 'w', encoding='utf-8') as f:
            json.dump(self.config, f, indent=2, ensure_ascii=False)
        
        self.logger.info("âœ… å®éªŒç¯å¢ƒå‡†å¤‡å®Œæˆ")

    def _get_data_period(self, quick_validation: bool = False) -> str:
        """è·å–æ•°æ®å‘¨æœŸé…ç½®"""
        
        if quick_validation:
            return "3mo"  # å¿«é€ŸéªŒè¯ä½¿ç”¨3ä¸ªæœˆæ•°æ®
        else:
            return self.config["data_configuration"]["periods"]["stage2_optimization"]

    def _validate_configuration(self) -> Dict[str, Any]:
        """éªŒè¯é…ç½®æ–‡ä»¶"""
        
        required_sections = [
            "experiment_metadata",
            "data_configuration", 
            "feature_engineering",
            "reward_function_configuration",
            "training_configuration"
        ]
        
        config_validation = {"valid": True, "missing_sections": [], "issues": []}
        
        for section in required_sections:
            if section not in self.config:
                config_validation["missing_sections"].append(section)
                config_validation["valid"] = False
        
        return config_validation

    def _validate_dependencies(self) -> Dict[str, Any]:
        """éªŒè¯ä¾èµ–åŒ…"""
        
        required_packages = self.config["infrastructure_configuration"]["dependencies"]["key_packages"]
        dependency_validation = {"valid": True, "missing_packages": [], "version_issues": []}
        
        for package_spec in required_packages:
            package_name = package_spec.split(">=")[0]
            try:
                __import__(package_name.replace("-", "_"))
            except ImportError:
                dependency_validation["missing_packages"].append(package_name)
                dependency_validation["valid"] = False
        
        return dependency_validation

    def _validate_data_availability(self) -> Dict[str, Any]:
        """éªŒè¯æ•°æ®å¯ç”¨æ€§"""
        
        # ç®€åŒ–çš„æ•°æ®éªŒè¯
        data_validation = {"valid": True, "issues": []}
        
        symbol = self.config["data_configuration"]["symbol"]
        if not symbol:
            data_validation["issues"].append("æœªæŒ‡å®šäº¤æ˜“æ ‡çš„")
            data_validation["valid"] = False
        
        return data_validation

    def _validate_environment(self) -> Dict[str, Any]:
        """éªŒè¯ç¯å¢ƒè®¾ç½®"""
        
        environment_validation = {"valid": True, "issues": []}
        
        # æ£€æŸ¥å†™å…¥æƒé™
        try:
            test_file = self.run_dir / "test_write.tmp"
            test_file.write_text("test")
            test_file.unlink()
        except Exception as e:
            environment_validation["issues"].append(f"å†™å…¥æƒé™æ£€æŸ¥å¤±è´¥: {e}")
            environment_validation["valid"] = False
        
        return environment_validation

    def _generate_execution_summary(self, experiment_results: Dict) -> Dict[str, Any]:
        """ç”Ÿæˆæ‰§è¡Œæ‘˜è¦"""
        
        summary = {
            "execution_time": self.timestamp,
            "total_stages": len(experiment_results.get("stage_results", {})),
            "successful_stages": 0,
            "failed_stages": 0,
            "overall_success": False,
            "key_achievements": [],
            "main_issues": []
        }
        
        # åˆ†æé˜¶æ®µç»“æœ
        for stage_name, stage_result in experiment_results.get("stage_results", {}).items():
            if stage_result.get("success", False):
                summary["successful_stages"] += 1
            else:
                summary["failed_stages"] += 1
        
        # æ£€æŸ¥æ•´ä½“æˆåŠŸ
        training_success = experiment_results.get("training_success", False)
        analysis_success = experiment_results.get("final_analysis", {}).get("evaluation_success", True)
        
        summary["overall_success"] = training_success and analysis_success
        
        # æå–å…³é”®æˆå°±
        if training_success:
            summary["key_achievements"].append("å¤šé˜¶æ®µè®­ç»ƒæˆåŠŸå®Œæˆ")
        
        if experiment_results.get("final_analysis"):
            summary["key_achievements"].append("è¯¦ç»†åˆ†æå’Œè¯„ä¼°å®Œæˆ")
        
        return summary

    def _save_experiment_results(self, experiment_results: Dict):
        """ä¿å­˜å®éªŒç»“æœ"""
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        results_file = self.run_dir / "complete_experiment_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(experiment_results, f, indent=2, default=str, ensure_ascii=False)
        
        # ä¿å­˜æ‘˜è¦
        summary_file = self.run_dir / "experiment_summary.json"
        summary_data = {
            "experiment_id": "006",
            "timestamp": self.timestamp,
            "success": experiment_results.get("execution_summary", {}).get("overall_success", False),
            "key_metrics": experiment_results.get("execution_summary", {}),
            "run_directory": str(self.run_dir)
        }
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"å®éªŒç»“æœå·²ä¿å­˜åˆ°: {results_file}")

    def _generate_final_report(self, experiment_results: Dict):
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        
        report_content = f"""# å®éªŒ006æ‰§è¡ŒæŠ¥å‘Š

## åŸºæœ¬ä¿¡æ¯
- **å®éªŒID**: 006
- **æ‰§è¡Œæ—¶é—´**: {self.timestamp}
- **è¿è¡Œç›®å½•**: {self.run_dir}
- **é…ç½®æ–‡ä»¶**: {self.config_path}

## å®éªŒæ¦‚è§ˆ
- **ä¸»è¦ç›®æ ‡**: {self.config["experiment_metadata"]["primary_objective"]}
- **æ¬¡è¦ç›®æ ‡**: {self.config["experiment_metadata"]["secondary_objective"]}
- **é¢„æœŸç›¸å…³æ€§**: {self.config["experiment_metadata"]["expected_correlation"]}

## æ‰§è¡Œæ‘˜è¦
"""
        
        execution_summary = experiment_results.get("execution_summary", {})
        if execution_summary:
            report_content += f"""
- **æ€»é˜¶æ®µæ•°**: {execution_summary.get("total_stages", 0)}
- **æˆåŠŸé˜¶æ®µ**: {execution_summary.get("successful_stages", 0)}
- **å¤±è´¥é˜¶æ®µ**: {execution_summary.get("failed_stages", 0)}
- **æ•´ä½“æˆåŠŸ**: {'âœ…' if execution_summary.get("overall_success") else 'âŒ'}
"""
        
        # æ·»åŠ å…³é”®æˆå°±
        key_achievements = execution_summary.get("key_achievements", [])
        if key_achievements:
            report_content += "\n## å…³é”®æˆå°±\n"
            for i, achievement in enumerate(key_achievements, 1):
                report_content += f"{i}. {achievement}\n"
        
        # æ·»åŠ ä¸»è¦é—®é¢˜
        main_issues = execution_summary.get("main_issues", [])
        if main_issues:
            report_content += "\n## ä¸»è¦é—®é¢˜\n"
            for i, issue in enumerate(main_issues, 1):
                report_content += f"{i}. {issue}\n"
        
        report_content += f"""
## è¯¦ç»†ç»“æœ
è¯¦ç»†çš„åˆ†æç»“æœå’Œæ•°æ®æ–‡ä»¶è¯·æŸ¥çœ‹è¿è¡Œç›®å½•:
- å®Œæ•´ç»“æœ: `complete_experiment_results.json`
- é˜¶æ®µç»“æœ: `stage_*_result.json`
- åˆ†æå›¾è¡¨: `plots/` ç›®å½•
- è®­ç»ƒæ¨¡å‹: `models/` ç›®å½•

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.run_dir / "EXPERIMENT_006_EXECUTION_REPORT.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        self.logger.info(f"æœ€ç»ˆæŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")


def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    
    parser = argparse.ArgumentParser(
        description="å®éªŒ006å®Œæ•´æ‰§è¡Œæ¡†æ¶",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # è¿è¡Œå®Œæ•´å®éªŒ
  python run_experiment_006.py --full-experiment
  
  # è¿è¡Œç‰¹å®šé˜¶æ®µ
  python run_experiment_006.py --stage 1
  
  # å¿«é€ŸéªŒè¯æ¨¡å¼
  python run_experiment_006.py --stage 1 --quick-validation
  
  # éªŒè¯ç¯å¢ƒè®¾ç½®
  python run_experiment_006.py --validate-setup
        """
    )
    
    # ä¸»è¦é€‰é¡¹
    main_group = parser.add_mutually_exclusive_group(required=True)
    main_group.add_argument("--full-experiment", action="store_true",
                           help="è¿è¡Œå®Œæ•´çš„3é˜¶æ®µå®éªŒ")
    main_group.add_argument("--stage", type=int, choices=[1, 2, 3],
                           help="è¿è¡Œç‰¹å®šé˜¶æ®µ (1=å¥–åŠ±ä¿®å¤, 2=å¤–æ±‡ä¼˜åŒ–, 3=ç³»ç»Ÿå®Œå–„)")
    main_group.add_argument("--analysis-only", 
                           help="ä»…åˆ†æå·²å®Œæˆçš„å®éªŒï¼ˆæä¾›å®éªŒè·¯å¾„ï¼‰")
    main_group.add_argument("--validate-setup", action="store_true",
                           help="éªŒè¯å®éªŒç¯å¢ƒå’Œé…ç½®")
    
    # å¯é€‰å‚æ•°
    parser.add_argument("--config", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--quick-validation", action="store_true",
                       help="ä½¿ç”¨å¿«é€ŸéªŒè¯æ¨¡å¼ï¼ˆå‡å°‘è®­ç»ƒæ—¶é—´ï¼‰")
    parser.add_argument("--no-detailed-analysis", action="store_true",
                       help="è·³è¿‡è¯¦ç»†åˆ†æï¼ˆä»…åœ¨full-experimentä¸­æœ‰æ•ˆï¼‰")
    parser.add_argument("--verbose", action="store_true",
                       help="è¯¦ç»†è¾“å‡º")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    logging.basicConfig(
        level=logging.DEBUG if args.verbose else logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    try:
        # åˆ›å»ºæ¡†æ¶å®ä¾‹
        framework = Experiment006Framework(args.config)
        
        # æ ¹æ®å‚æ•°æ‰§è¡Œç›¸åº”æ“ä½œ
        if args.validate_setup:
            print("éªŒè¯å®éªŒç¯å¢ƒè®¾ç½®...")
            validation_results = framework.validate_experiment_setup()
            
            print("\n" + "=" * 60)
            print("ç¯å¢ƒéªŒè¯ç»“æœ")
            print("=" * 60)
            
            if validation_results["overall_validation"]:
                print("SUCCESS: ç¯å¢ƒéªŒè¯é€šè¿‡ï¼Œå¯ä»¥å¼€å§‹å®éªŒ")
            else:
                print("FAILED: ç¯å¢ƒéªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»¥ä¸‹é—®é¢˜:")
                for section, result in validation_results.items():
                    if isinstance(result, dict) and not result.get("valid", True):
                        print(f"  - {section}: {result}")
        
        elif args.analysis_only:
            print("å¼€å§‹åˆ†æå·²å®Œæˆçš„å®éªŒ...")
            analysis_results = framework.run_analysis_only(args.analysis_only)
            
            print("\n" + "=" * 60) 
            print("åˆ†æå®Œæˆ")
            print("=" * 60)
            print(f"åˆ†æç»“æœå·²ä¿å­˜åˆ°: {framework.run_dir}")
        
        elif args.stage:
            print(f"å¼€å§‹æ‰§è¡Œé˜¶æ®µ{args.stage}...")
            stage_results = framework.run_single_stage(args.stage, args.quick_validation)
            
            print("\n" + "=" * 60)
            print(f"é˜¶æ®µ{args.stage}æ‰§è¡Œç»“æœ")
            print("=" * 60)
            
            success = stage_results.get("success", False)
            print(f"æ‰§è¡ŒçŠ¶æ€: {'SUCCESS' if success else 'FAILED'}")
            
            if success:
                val_metrics = stage_results.get("validation_metrics", {})
                if val_metrics:
                    print(f"éªŒè¯å›æŠ¥: {val_metrics.get('val_mean_return', 0):.2f}%")
                    print(f"éªŒè¯èƒœç‡: {val_metrics.get('val_win_rate', 0):.2f}")
        
        elif args.full_experiment:
            print("å¼€å§‹å®Œæ•´å®éªŒ...")
            detailed_analysis = not args.no_detailed_analysis
            
            experiment_results = framework.run_full_experiment(
                detailed_analysis=detailed_analysis,
                quick_validation=args.quick_validation
            )
            
            print("\n" + "=" * 80)
            print("å®éªŒ006å®Œæ•´æ‰§è¡Œç»“æœ")
            print("=" * 80)
            
            execution_summary = experiment_results.get("execution_summary", {})
            overall_success = execution_summary.get("overall_success", False)
            
            print(f"å®éªŒçŠ¶æ€: {'SUCCESS' if overall_success else 'FAILED'}")
            print(f"æ€»é˜¶æ®µæ•°: {execution_summary.get('total_stages', 0)}")
            print(f"æˆåŠŸé˜¶æ®µ: {execution_summary.get('successful_stages', 0)}")
            print(f"å¤±è´¥é˜¶æ®µ: {execution_summary.get('failed_stages', 0)}")
            
            # æ˜¾ç¤ºå…³é”®æˆå°±
            achievements = execution_summary.get("key_achievements", [])
            if achievements:
                print(f"\nå…³é”®æˆå°±:")
                for achievement in achievements:
                    print(f"   - {achievement}")
            
            # æ˜¾ç¤ºä¸»è¦é—®é¢˜
            issues = execution_summary.get("main_issues", [])
            if issues:
                print(f"\nä¸»è¦é—®é¢˜:")
                for issue in issues:
                    print(f"   - {issue}")
            
            print(f"\nè¯¦ç»†ç»“æœ: {framework.run_dir}")
            print("=" * 80)
    
    except KeyboardInterrupt:
        print("\nWARNING: ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
        sys.exit(1)
    except Exception as e:
        print(f"\nERROR: æ‰§è¡Œå¤±è´¥: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()