#!/usr/bin/env python
"""
å¸‚åœºæ•°æ®ä¸‹è½½å’Œé¢„å¤„ç†è„šæœ¬

åŠŸèƒ½:
1. æ”¯æŒå¤šæ•°æ®æº (YFinance, TrueFX, Oanda, HistData, FXMinute)
2. æ‰¹é‡ä¸‹è½½å¸‚åœºå†å²æ•°æ® (è‚¡ç¥¨/å¤–æ±‡/æœŸè´§ç­‰)
3. æ‰§è¡Œç‰¹å¾å·¥ç¨‹å¤„ç†
4. æ•°æ®é›†åˆ’åˆ†(è®­ç»ƒ/éªŒè¯/æµ‹è¯•)
5. ä¿å­˜å¤„ç†åçš„æ•°æ®åˆ°æœ¬åœ°

ä½¿ç”¨ç¤ºä¾‹:
  # ç›´æ¥è¿è¡Œ (é»˜è®¤ä¸‹è½½EURUSDå¤–æ±‡æ•°æ®ï¼ŒFX-1-Minute-Dataæ•°æ®æº)
  python download_data.py

  # ä¸‹è½½æŒ‡å®šå¤–æ±‡æ•°æ®ï¼Œä½¿ç”¨1å°æ—¶é—´éš”
  python download_data.py --symbol GBPUSD --period 2y --interval 1m

  # æ‰¹é‡ä¸‹è½½å¤šä¸ªå¤–æ±‡å¯¹ï¼Œä½¿ç”¨1åˆ†é’Ÿé—´éš”
  python download_data.py --symbols EURUSD,GBPUSD,USDJPY --period 60d --interval 1m

  # ä½¿ç”¨YFinanceæ•°æ®æºä¸‹è½½è‚¡ç¥¨æ•°æ®
  python download_data.py --data-source yfinance --symbol AAPL --period 1y

  # ä½¿ç”¨TrueFXæ•°æ®æºä¸‹è½½å¤–æ±‡æ•°æ®
  python download_data.py --data-source truefx --symbol EURUSD --period 1mo

  # ä½¿ç”¨Oandaæ•°æ®æºï¼ˆéœ€è¦é…ç½®æ–‡ä»¶ï¼‰
  python download_data.py --data-source oanda --data-source-config oanda_config.json --symbol GBPUSD

  # ä½¿ç”¨HistDataæ•°æ®æºä¸‹è½½å†å²æ•°æ®
  python download_data.py --data-source histdata --symbol GBPUSD --period 1y

  # ä½¿ç”¨FX-1-Minute-Dataæ•°æ®æºä¸‹è½½å†å²å¤–æ±‡æ•°æ®
  python download_data.py --data-source fxminute --symbol EURUSD --period 1mo

  # æŒ‡å®šæ•°æ®é›†åˆ’åˆ†æ¯”ä¾‹
  python download_data.py --symbol AAPL --train-ratio 0.7 --val-ratio 0.2 --test-ratio 0.1

  # ä½¿ç”¨è‡ªå®šä¹‰ä»£ç† (é€‚ç”¨äºyfinanceç­‰éœ€è¦ä»£ç†çš„æ•°æ®æº)
  python download_data.py --proxy-host 127.0.0.1 --proxy-port 7891

  # ç¦ç”¨ä»£ç†
  python download_data.py --no-proxy

æ•°æ®æºè¯´æ˜:
  - fxminute: FX-1-Minute-Dataï¼Œæœ¬åœ°ç¼“å­˜å¤–æ±‡æ•°æ® (é»˜è®¤ï¼Œ2000-2024å¹´é«˜è´¨é‡æ•°æ®)
  - yfinance: Yahoo Financeï¼Œå…è´¹è‚¡ç¥¨æ•°æ® (æ”¯æŒä»£ç†)
  - truefx: TrueFXï¼Œå…è´¹å¤–æ±‡æ•°æ® (éœ€è¦æ³¨å†Œè´¦æˆ·)
  - oanda: Oandaï¼Œä¸“ä¸šå¤–æ±‡/CFDæ•°æ® (éœ€è¦APIè®¿é—®)
  - histdata: HistDataï¼Œå†å²å¤–æ±‡æ–‡ä»¶ (å…è´¹å†å²æ•°æ®)

æ•°æ®é—´éš”è¯´æ˜:
  - å„æ•°æ®æºæ”¯æŒçš„é—´éš”ä¸åŒ
  - fxminute: ä»…æ”¯æŒ1åˆ†é’ŸOHLCæ•°æ®ï¼Œ66+ä¸ªäº¤æ˜“å¯¹
  - yfinance: 1m(7å¤©), 5m(60å¤©), 1h(2å¹´), 1d(å†å²)
  - truefx: tickçº§å®æ—¶æ•°æ®å’Œæ—¥å†…æ•°æ®
  - oanda: ç§’çº§åˆ°æœˆçº§å¤šç§é—´éš”
  - histdata: 1åˆ†é’Ÿå†å²æ•°æ®æ–‡ä»¶

ä»£ç†é…ç½®è¯´æ˜:
  - é€‚ç”¨äºéœ€è¦ä»£ç†è®¿é—®çš„æ•°æ®æº (å¦‚yfinance)
  - é»˜è®¤ä½¿ç”¨ socks5://127.0.0.1:7891 ä»£ç†
  - æ”¯æŒé€šè¿‡ç¯å¢ƒå˜é‡é…ç½®: USE_PROXY, PROXY_HOST, PROXY_PORT
"""

import os
import sys
import argparse
import json
import warnings
import pickle
import time
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
import pandas as pd
import numpy as np
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
PROJECT_ROOT = Path(__file__).parent
sys.path.insert(0, str(PROJECT_ROOT))

# æŠ‘åˆ¶è­¦å‘Š
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)

try:
    from src.data.data_manager import DataManager
    from src.features.feature_engineer import FeatureEngineer
    from src.utils.config import Config
    from src.utils.logger import setup_logger, get_default_log_file
except ImportError as e:
    print(f"å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰ä¾èµ–åŒ…: pip install -r requirements.txt")
    sys.exit(1)


class DataDownloader:
    """
    æ•°æ®ä¸‹è½½å’Œé¢„å¤„ç†ç±»
    
    è´Ÿè´£æ‰¹é‡ä¸‹è½½è‚¡ç¥¨æ•°æ®ã€ç‰¹å¾å·¥ç¨‹å’Œæ•°æ®é›†åˆ’åˆ†
    """
    
    def __init__(self, config_path: Optional[str] = None, output_dir: str = "datasets"):
        """
        åˆå§‹åŒ–æ•°æ®ä¸‹è½½å™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
        """
        # è®¾ç½®ä»£ç†ï¼ˆå¦‚æœéœ€è¦ï¼‰
        self._setup_proxy()
        
        # åŠ è½½é…ç½®
        self.config = Config(config_file=config_path) if config_path else Config()
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True, parents=True)
        
        # åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
        self.logger = setup_logger(
            name="DataDownloader",
            level="INFO",
            log_file=get_default_log_file("data_downloader")
        )
        
        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶ï¼Œä¼ é€’ä»£ç†é…ç½®å’Œæ•°æ®æºç±»å‹
        data_source_type = os.getenv('DATA_SOURCE_TYPE', 'fxminute')
        data_source_config = self._load_data_source_config()
        
        # ä¸ºFXMinuteæ•°æ®æºæä¾›é»˜è®¤é…ç½®
        if data_source_type == 'fxminute' and not data_source_config:
            data_source_config = {
                'data_directory': str(PROJECT_ROOT / 'data_cache' / 'FX-1-Minute-Data'),
                'auto_extract': True,
                'cache_extracted': True,
                'extracted_cache_dir': str(PROJECT_ROOT / 'fx_minute_cache')
            }
        
        self.data_manager = DataManager(
            config=self.config,
            data_source_type=data_source_type,
            data_source_config=data_source_config
        )
        self.feature_engineer = FeatureEngineer(self.config)
        
        # è®¾ç½®ä»£ç†é…ç½®åˆ° DataManager
        self._configure_data_manager_proxy()
        
        self.logger.info("æ•°æ®ä¸‹è½½å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _setup_proxy(self):
        """
        è®¾ç½®ä»£ç†é…ç½®
        """
        use_proxy = os.getenv('USE_PROXY', 'true').lower() == 'true'
        
        if use_proxy:
            proxy_host = os.getenv('PROXY_HOST', '127.0.0.1')
            proxy_port = os.getenv('PROXY_PORT', '7891')
            proxy_url = f"socks5://{proxy_host}:{proxy_port}"
            
            # è®¾ç½®å…¨å±€ç¯å¢ƒå˜é‡ä»£ç†
            os.environ['HTTP_PROXY'] = proxy_url
            os.environ['HTTPS_PROXY'] = proxy_url
            os.environ['http_proxy'] = proxy_url
            os.environ['https_proxy'] = proxy_url
            print(f"SUCCESS: å…¨å±€ä»£ç†å·²è®¾ç½®: {proxy_url}")
        else:
            print("SUCCESS: ä¸ä½¿ç”¨ä»£ç†")
    
    def _configure_data_manager_proxy(self):
        """
        ä¸º DataManager é…ç½®ä»£ç†è®¾ç½®
        """
        use_proxy = os.getenv('USE_PROXY', 'true').lower() == 'true'
        
        if use_proxy:
            proxy_host = os.getenv('PROXY_HOST', '127.0.0.1')
            proxy_port = os.getenv('PROXY_PORT', '7891')
            proxy_url = f"socks5://{proxy_host}:{proxy_port}"
            
            # é…ç½® DataManager ä»£ç†
            if hasattr(self.data_manager, 'set_proxy'):
                self.data_manager.set_proxy(proxy_url)
                self.logger.info(f"DataManager ä»£ç†é…ç½®: {proxy_url}")
            else:
                self.logger.warning("DataManager ä¸æ”¯æŒä»£ç†é…ç½®")
        else:
            self.logger.info("DataManager ä¸ä½¿ç”¨ä»£ç†")
    
    def _load_data_source_config(self) -> Dict:
        """
        åŠ è½½æ•°æ®æºé…ç½®
        
        Returns:
            æ•°æ®æºé…ç½®å­—å…¸
        """
        config_file = os.getenv('DATA_SOURCE_CONFIG')
        if config_file and os.path.exists(config_file):
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    import json
                    config = json.load(f)
                    self.logger.info(f"Loaded data source config from: {config_file}")
                    return config
            except Exception as e:
                self.logger.warning(f"Failed to load data source config: {e}")
        
        return {}
    
    def download_single_stock(
        self,
        symbol: str,
        period: str = "2y",
        interval: str = "1d",
        train_ratio: float = 0.7,
        val_ratio: float = 0.2,
        test_ratio: float = 0.1
    ) -> Dict[str, Any]:
        """
        ä¸‹è½½å•ä¸ªè‚¡ç¥¨æ•°æ®å¹¶è¿›è¡Œé¢„å¤„ç†
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            period: æ•°æ®å‘¨æœŸ
            interval: æ•°æ®é—´éš”
            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
            val_ratio: éªŒè¯é›†æ¯”ä¾‹  
            test_ratio: æµ‹è¯•é›†æ¯”ä¾‹
            
        Returns:
            Dict[str, Any]: å¤„ç†ç»“æœ
        """
        data_source = os.getenv('DATA_SOURCE_TYPE', 'yfinance')
        start_time = time.time()
        
        # æ‰“å°å¼€å§‹ä¿¡æ¯
        print(f"\n{'='*60}")
        print(f"ğŸ“Š å¼€å§‹å¤„ç†: {symbol}")
        print(f"æ•°æ®æº: {data_source} | å‘¨æœŸ: {period} | é—´éš”: {interval}")
        print(f"æ•°æ®é›†åˆ’åˆ†: è®­ç»ƒ({train_ratio:.0%}) éªŒè¯({val_ratio:.0%}) æµ‹è¯•({test_ratio:.0%})")
        print(f"{'='*60}")
        
        self.logger.info(f"å¼€å§‹ä¸‹è½½æ•°æ®: {symbol} (æ•°æ®æº: {data_source}), å‘¨æœŸ: {period}, é—´éš”: {interval}")
        
        try:
            # éªŒè¯æ¯”ä¾‹
            if abs(train_ratio + val_ratio + test_ratio - 1.0) > 1e-6:
                raise ValueError("è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•é›†æ¯”ä¾‹ä¹‹å’Œå¿…é¡»ç­‰äº1.0")
            
            # åˆ›å»ºè¿›åº¦æ¡
            with tqdm(total=5, desc="æ•°æ®å¤„ç†è¿›åº¦", ncols=80, colour='green') as pbar:
                
                # 1. è·å–åŸå§‹æ•°æ®
                pbar.set_description("ğŸ“¥ è·å–åŸå§‹æ•°æ®")
                self.logger.info(f"ä» {data_source} è·å– {symbol} æ•°æ®...")
                step_start = time.time()
                raw_data = self.data_manager.get_stock_data(symbol, period=period, interval=interval)
                step_time = time.time() - step_start
                
                print(f"âœ… æ•°æ®è·å–å®Œæˆ: {len(raw_data):,} æ¡è®°å½• ({step_time:.2f}ç§’)")
                self.logger.info(f"æ•°æ®è·å–å®Œæˆ: {len(raw_data)} æ¡è®°å½• (è€—æ—¶: {step_time:.2f}ç§’)")
                pbar.update(1)
                
                # 2. ç‰¹å¾å·¥ç¨‹
                pbar.set_description("ğŸ”§ æ‰§è¡Œç‰¹å¾å·¥ç¨‹")
                self.logger.info("æ‰§è¡Œç‰¹å¾å·¥ç¨‹...")
                step_start = time.time()
                features_data = self.feature_engineer.prepare_features(raw_data)
                step_time = time.time() - step_start
                
                # åˆ†æç‰¹å¾ç±»åˆ«
                feature_stats = self._analyze_feature_categories(features_data.columns)
                print(f"âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆ: {len(features_data):,} æ¡è®°å½•, {len(features_data.columns)} ä¸ªç‰¹å¾ ({step_time:.2f}ç§’)")
                print(f"   ç‰¹å¾åˆ†å¸ƒ: {feature_stats}")
                self.logger.info(f"ç‰¹å¾å·¥ç¨‹å®Œæˆ: {len(features_data)} æ¡è®°å½•, {len(features_data.columns)} ä¸ªç‰¹å¾ (è€—æ—¶: {step_time:.2f}ç§’)")
                pbar.update(1)
                
                # 3. æ•°æ®è´¨é‡æ£€æŸ¥
                pbar.set_description("ğŸ” æ•°æ®è´¨é‡æ£€æŸ¥")
                step_start = time.time()
                quality_report = self._check_data_quality(features_data)
                step_time = time.time() - step_start
                
                print(f"âœ… æ•°æ®è´¨é‡æ£€æŸ¥: {quality_report} ({step_time:.2f}ç§’)")
                self.logger.info(f"æ•°æ®è´¨é‡æ£€æŸ¥å®Œæˆ (è€—æ—¶: {step_time:.2f}ç§’)")
                pbar.update(1)
                
                # 4. æ•°æ®é›†åˆ’åˆ†
                pbar.set_description("âœ‚ï¸ åˆ’åˆ†æ•°æ®é›†")
                self.logger.info("åˆ’åˆ†æ•°æ®é›†...")
                step_start = time.time()
                datasets = self._split_dataset(features_data, train_ratio, val_ratio, test_ratio)
                step_time = time.time() - step_start
                
                print(f"âœ… æ•°æ®é›†åˆ’åˆ†å®Œæˆ ({step_time:.2f}ç§’)")
                for split_name, data in datasets.items():
                    print(f"   {split_name}: {len(data):,} æ¡è®°å½•")
                pbar.update(1)
                
                # 5. ä¿å­˜æ•°æ®
                pbar.set_description("ğŸ’¾ ä¿å­˜æ•°æ®æ–‡ä»¶")
                step_start = time.time()
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                symbol_dir = self.output_dir / f"{symbol}_{timestamp}"
                symbol_dir.mkdir(exist_ok=True)
                
                saved_files = self._save_datasets(symbol, datasets, symbol_dir)
                
                # ä¿å­˜å…ƒæ•°æ®
                metadata = {
                    'symbol': symbol,
                    'period': period,
                    'interval': interval,
                    'download_time': timestamp,
                    'raw_data_shape': raw_data.shape,
                    'features_data_shape': features_data.shape,
                    'train_ratio': train_ratio,
                    'val_ratio': val_ratio,
                    'test_ratio': test_ratio,
                    'dataset_shapes': {k: v.shape for k, v in datasets.items()},
                    'feature_columns': list(features_data.columns),
                    'feature_categories': feature_stats,
                    'quality_report': quality_report,
                    'processing_time': time.time() - start_time,
                    'saved_files': saved_files
                }
                
                metadata_file = symbol_dir / "metadata.json"
                with open(metadata_file, 'w', encoding='utf-8') as f:
                    json.dump(metadata, f, indent=2, ensure_ascii=False, default=str)
                
                step_time = time.time() - step_start
                print(f"âœ… æ•°æ®ä¿å­˜å®Œæˆ: {symbol_dir.name} ({step_time:.2f}ç§’)")
                self.logger.info(f"æ•°æ®ä¿å­˜å®Œæˆ: {symbol_dir} (è€—æ—¶: {step_time:.2f}ç§’)")
                pbar.update(1)
            
            # æ€»ç»“ä¿¡æ¯
            total_time = time.time() - start_time
            print(f"\nğŸ‰ {symbol} å¤„ç†æˆåŠŸå®Œæˆ!")
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {symbol_dir}")
            print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f}ç§’")
            print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡: {len(raw_data):,} â†’ {len(features_data):,} æ¡è®°å½•, {len(features_data.columns)} ä¸ªç‰¹å¾")
            print(f"{'='*60}")
            
            return {
                'status': 'success',
                'symbol': symbol,
                'output_dir': str(symbol_dir),
                'metadata': metadata
            }
            
        except Exception as e:
            total_time = time.time() - start_time
            print(f"\nâŒ {symbol} å¤„ç†å¤±è´¥!")
            print(f"ğŸ”¥ é”™è¯¯: {e}")
            print(f"â±ï¸  è€—æ—¶: {total_time:.2f}ç§’")
            print(f"{'='*60}")
            
            self.logger.error(f"ä¸‹è½½ {symbol} æ•°æ®å¤±è´¥: {e} (è€—æ—¶: {total_time:.2f}ç§’)")
            return {
                'status': 'error',
                'symbol': symbol,
                'error': str(e)
            }
    
    def download_multiple_stocks(
        self,
        symbols: List[str],
        period: str = "2y",
        interval: str = "1d",
        train_ratio: float = 0.7,
        val_ratio: float = 0.2,
        test_ratio: float = 0.1
    ) -> Dict[str, Any]:
        """
        æ‰¹é‡ä¸‹è½½å¤šä¸ªè‚¡ç¥¨æ•°æ®
        
        Args:
            symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            period: æ•°æ®å‘¨æœŸ
            interval: æ•°æ®é—´éš”
            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
            val_ratio: éªŒè¯é›†æ¯”ä¾‹
            test_ratio: æµ‹è¯•é›†æ¯”ä¾‹
            
        Returns:
            Dict[str, Any]: æ‰¹é‡å¤„ç†ç»“æœ
        """
        data_source = os.getenv('DATA_SOURCE_TYPE', 'yfinance')
        batch_start_time = time.time()
        
        # æ‰“å°æ‰¹é‡å¤„ç†å¼€å§‹ä¿¡æ¯
        print(f"\n{'='*80}")
        print(f"ğŸš€ æ‰¹é‡æ•°æ®å¤„ç†å¼€å§‹")
        print(f"ç¬¦å·åˆ—è¡¨: {', '.join(symbols)}")
        print(f"æ•°æ®æº: {data_source} | å‘¨æœŸ: {period} | é—´éš”: {interval}")
        print(f"æ€»æ•°: {len(symbols)} ä¸ªç¬¦å·")
        print(f"{'='*80}")
        
        self.logger.info(f"å¼€å§‹æ‰¹é‡ä¸‹è½½ {len(symbols)} ä¸ªç¬¦å·æ•°æ® (æ•°æ®æº: {data_source})")
        
        results = {}
        successful_downloads = []
        failed_downloads = []
        
        # ä½¿ç”¨tqdmæ˜¾ç¤ºæ‰¹é‡å¤„ç†è¿›åº¦
        with tqdm(symbols, desc="æ‰¹é‡å¤„ç†", ncols=100, colour='blue') as pbar:
            for i, symbol in enumerate(pbar, 1):
                pbar.set_description(f"å¤„ç† {symbol} ({i}/{len(symbols)})")
                self.logger.info(f"å¤„ç†ç¬¦å·: {symbol} ({i}/{len(symbols)})")
                
                symbol_start_time = time.time()
                result = self.download_single_stock(symbol, period, interval, train_ratio, val_ratio, test_ratio)
                symbol_time = time.time() - symbol_start_time
                
                results[symbol] = result
                result['processing_time'] = symbol_time
                
                if result['status'] == 'success':
                    successful_downloads.append(symbol)
                    self.logger.info(f"SUCCESS: {symbol} ä¸‹è½½æˆåŠŸ (è€—æ—¶: {symbol_time:.2f}ç§’)")
                    pbar.set_postfix(success=len(successful_downloads), failed=len(failed_downloads))
                else:
                    failed_downloads.append(symbol)
                    self.logger.error(f"FAILED: {symbol} ä¸‹è½½å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')} (è€—æ—¶: {symbol_time:.2f}ç§’)")
                    pbar.set_postfix(success=len(successful_downloads), failed=len(failed_downloads))
        
        # æ‰¹é‡å¤„ç†æ€»ç»“
        total_batch_time = time.time() - batch_start_time
        
        # ä¿å­˜æ‰¹é‡å¤„ç†æ‘˜è¦
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        summary = {
            'batch_download_time': timestamp,
            'total_symbols': len(symbols),
            'successful_count': len(successful_downloads),
            'failed_count': len(failed_downloads),
            'successful_symbols': successful_downloads,
            'failed_symbols': failed_downloads,
            'period': period,
            'interval': interval,
            'train_ratio': train_ratio,
            'val_ratio': val_ratio,
            'test_ratio': test_ratio,
            'total_processing_time': total_batch_time,
            'average_time_per_symbol': total_batch_time / len(symbols) if symbols else 0,
            'detailed_results': results
        }
        
        summary_file = self.output_dir / f"batch_download_summary_{timestamp}.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False, default=str)
        
        # æ‰“å°æ‰¹é‡å¤„ç†å®Œæˆä¿¡æ¯
        print(f"\n{'='*80}")
        print(f"ğŸ¯ æ‰¹é‡å¤„ç†å®Œæˆ!")
        print(f"âœ… æˆåŠŸ: {len(successful_downloads)}/{len(symbols)} ä¸ªç¬¦å·")
        print(f"âŒ å¤±è´¥: {len(failed_downloads)}/{len(symbols)} ä¸ªç¬¦å·")
        print(f"â±ï¸  æ€»è€—æ—¶: {total_batch_time:.2f}ç§’")
        print(f"ğŸ“Š å¹³å‡è€—æ—¶: {total_batch_time/len(symbols):.2f}ç§’/ç¬¦å·")
        
        if successful_downloads:
            print(f"âœ… æˆåŠŸç¬¦å·: {', '.join(successful_downloads)}")
        if failed_downloads:
            print(f"âŒ å¤±è´¥ç¬¦å·: {', '.join(failed_downloads)}")
            
        print(f"ğŸ“ æ‘˜è¦æ–‡ä»¶: {summary_file}")
        print(f"{'='*80}")
        
        self.logger.info(f"æ‰¹é‡ä¸‹è½½å®Œæˆ: æˆåŠŸ {len(successful_downloads)}, å¤±è´¥ {len(failed_downloads)}, æ€»è€—æ—¶: {total_batch_time:.2f}ç§’")
        
        return summary
    
    def _split_dataset(
        self,
        data: pd.DataFrame,
        train_ratio: float,
        val_ratio: float,
        test_ratio: float
    ) -> Dict[str, pd.DataFrame]:
        """
        æŒ‰æ—¶é—´é¡ºåºåˆ’åˆ†æ•°æ®é›†
        
        Args:
            data: ç‰¹å¾æ•°æ®
            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
            val_ratio: éªŒè¯é›†æ¯”ä¾‹
            test_ratio: æµ‹è¯•é›†æ¯”ä¾‹
            
        Returns:
            Dict[str, pd.DataFrame]: åˆ’åˆ†åçš„æ•°æ®é›†
        """
        total_len = len(data)
        
        # è®¡ç®—åˆ†å‰²ç‚¹
        train_end = int(total_len * train_ratio)
        val_end = int(total_len * (train_ratio + val_ratio))
        
        # æŒ‰æ—¶é—´é¡ºåºåˆ’åˆ†
        train_data = data.iloc[:train_end].copy()
        val_data = data.iloc[train_end:val_end].copy()
        test_data = data.iloc[val_end:].copy()
        
        self.logger.info(f"æ•°æ®é›†åˆ’åˆ†å®Œæˆ:")
        self.logger.info(f"  è®­ç»ƒé›†: {len(train_data)} æ¡è®°å½• ({len(train_data)/total_len:.1%})")
        self.logger.info(f"  éªŒè¯é›†: {len(val_data)} æ¡è®°å½• ({len(val_data)/total_len:.1%})")
        self.logger.info(f"  æµ‹è¯•é›†: {len(test_data)} æ¡è®°å½• ({len(test_data)/total_len:.1%})")
        
        return {
            'train': train_data,
            'val': val_data,
            'test': test_data
        }
    
    def _save_datasets(
        self,
        symbol: str,
        datasets: Dict[str, pd.DataFrame],
        output_dir: Path
    ) -> Dict[str, str]:
        """
        ä¿å­˜æ•°æ®é›†åˆ°æ–‡ä»¶
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            datasets: æ•°æ®é›†å­—å…¸
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            Dict[str, str]: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        saved_files = {}
        
        for split_name, data in datasets.items():
            # åŒæ—¶ä¿å­˜CSVå’ŒPickleæ ¼å¼
            csv_file = output_dir / f"{symbol}_{split_name}.csv"
            pkl_file = output_dir / f"{symbol}_{split_name}.pkl"
            
            # ä¿å­˜CSV (ä¾¿äºæŸ¥çœ‹)
            data.to_csv(csv_file, index=True)
            
            # ä¿å­˜Pickle (ä¾¿äºå¿«é€ŸåŠ è½½)
            with open(pkl_file, 'wb') as f:
                pickle.dump(data, f)
            
            saved_files[f'{split_name}_csv'] = str(csv_file)
            saved_files[f'{split_name}_pkl'] = str(pkl_file)
            
            self.logger.info(f"ä¿å­˜ {split_name} æ•°æ®: {len(data)} æ¡è®°å½• -> {csv_file.name}")
        
        return saved_files
    
    def load_dataset(
        self,
        symbol: str,
        split: str = "train",
        data_dir: Optional[str] = None,
        format_type: str = "pkl"
    ) -> pd.DataFrame:
        """
        åŠ è½½æ•°æ®é›†
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            split: æ•°æ®é›†åˆ†å‰² (train/val/test)
            data_dir: æ•°æ®ç›®å½•
            format_type: æ–‡ä»¶æ ¼å¼ (pkl/csv)
            
        Returns:
            pd.DataFrame: åŠ è½½çš„æ•°æ®
        """
        if data_dir:
            base_dir = Path(data_dir)
        else:
            # æŸ¥æ‰¾æœ€æ–°çš„æ•°æ®ç›®å½•
            symbol_dirs = list(self.output_dir.glob(f"{symbol}_*"))
            if not symbol_dirs:
                raise FileNotFoundError(f"æœªæ‰¾åˆ° {symbol} çš„æ•°æ®ç›®å½•")
            base_dir = max(symbol_dirs)  # é€‰æ‹©æœ€æ–°çš„
        
        if format_type == "pkl":
            file_path = base_dir / f"{symbol}_{split}.pkl"
            with open(file_path, 'rb') as f:
                data = pickle.load(f)
        else:
            file_path = base_dir / f"{symbol}_{split}.csv"
            data = pd.read_csv(file_path, index_col=0)
        
        self.logger.info(f"åŠ è½½æ•°æ®é›†: {file_path} ({len(data)} æ¡è®°å½•)")
        return data
    
    def _analyze_feature_categories(self, feature_columns: List[str]) -> Dict[str, int]:
        """
        åˆ†æç‰¹å¾ç±»åˆ«åˆ†å¸ƒ
        
        Args:
            feature_columns: ç‰¹å¾åˆ—ååˆ—è¡¨
            
        Returns:
            Dict[str, int]: å„ç±»åˆ«ç‰¹å¾æ•°é‡
        """
        categories = {
            'Price': 0,      # ä»·æ ¼ç›¸å…³
            'Volume': 0,     # æˆäº¤é‡ç›¸å…³
            'MA': 0,         # ç§»åŠ¨å¹³å‡
            'Momentum': 0,   # åŠ¨é‡æŒ‡æ ‡
            'Volatility': 0, # æ³¢åŠ¨æ€§æŒ‡æ ‡
            'Trend': 0,      # è¶‹åŠ¿æŒ‡æ ‡
            'Statistical': 0,# ç»Ÿè®¡ç‰¹å¾
            'Others': 0      # å…¶ä»–
        }
        
        for col in feature_columns:
            col_upper = col.upper()
            if any(x in col_upper for x in ['OPEN', 'HIGH', 'LOW', 'CLOSE', 'PRICE']):
                categories['Price'] += 1
            elif 'VOLUME' in col_upper or 'OBV' in col_upper:
                categories['Volume'] += 1
            elif any(x in col_upper for x in ['SMA', 'EMA', 'WMA']):
                categories['MA'] += 1
            elif any(x in col_upper for x in ['RSI', 'MACD', 'ROC', 'MOM', 'STOCH', 'WILLIAMS', 'CCI']):
                categories['Momentum'] += 1
            elif any(x in col_upper for x in ['ATR', 'BBANDS', 'VOLATILITY', 'STD']):
                categories['Volatility'] += 1
            elif any(x in col_upper for x in ['ADX', 'DI', 'SAR', 'ICHIMOKU', 'TREND']):
                categories['Trend'] += 1
            elif any(x in col_upper for x in ['MEAN', 'MEDIAN', 'SKEW', 'KURT', 'CORR', 'Z_SCORE']):
                categories['Statistical'] += 1
            else:
                categories['Others'] += 1
        
        # åªè¿”å›éé›¶ç±»åˆ«
        return {k: v for k, v in categories.items() if v > 0}
    
    def _check_data_quality(self, data: pd.DataFrame) -> Dict[str, Any]:
        """
        æ£€æŸ¥æ•°æ®è´¨é‡
        
        Args:
            data: ç‰¹å¾æ•°æ®
            
        Returns:
            Dict[str, Any]: æ•°æ®è´¨é‡æŠ¥å‘Š
        """
        total_cells = data.shape[0] * data.shape[1]
        nan_count = data.isnull().sum().sum()
        inf_count = np.isinf(data.select_dtypes(include=[np.number])).sum().sum()
        
        # æ£€æŸ¥å¸¸é‡ç‰¹å¾
        constant_features = []
        for col in data.columns:
            if data[col].nunique() <= 1:
                constant_features.append(col)
        
        # æ£€æŸ¥å¼‚å¸¸å€¼ (è¶…è¿‡3ä¸ªæ ‡å‡†å·®)
        numeric_cols = data.select_dtypes(include=[np.number]).columns
        outliers = {}
        for col in numeric_cols:
            if data[col].std() > 0:  # é¿å…é™¤é›¶
                z_scores = np.abs((data[col] - data[col].mean()) / data[col].std())
                outlier_count = (z_scores > 3).sum()
                if outlier_count > 0:
                    outliers[col] = outlier_count
        
        return {
            'total_cells': total_cells,
            'missing_values': int(nan_count),
            'infinite_values': int(inf_count),
            'missing_percentage': round(nan_count / total_cells * 100, 2),
            'constant_features': len(constant_features),
            'outlier_features': len(outliers),
            'quality_score': round((1 - (nan_count + inf_count) / total_cells) * 100, 2)
        }


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="æ•°æ®ä¸‹è½½å’Œé¢„å¤„ç†è„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )
    
    # è‚¡ç¥¨å‚æ•°
    group = parser.add_mutually_exclusive_group(required=False)
    group.add_argument('--symbol', '-s',
                      help='å•ä¸ªäº¤æ˜“ä»£ç  (é»˜è®¤: EURUSD)')
    
    group.add_argument('--symbols',
                      help='å¤šä¸ªäº¤æ˜“ä»£ç ï¼Œé€—å·åˆ†éš” (ä¾‹å¦‚: EURUSD,GBPUSD,USDJPY)')
    
    parser.add_argument('--period', '-p',
                       help='æ•°æ®å‘¨æœŸ (ä¾‹å¦‚: 1y, 2y, 6m, 3m)')
    
    parser.add_argument('--interval', '-i',
                       help='æ•°æ®é—´éš” (fxminuteä»…æ”¯æŒ1m, yfinance: 1mä»…7å¤©/1hæ”¯æŒ2å¹´/1dæ”¯æŒå†å²)')
    
    # æ•°æ®é›†åˆ’åˆ†å‚æ•°
    parser.add_argument('--train-ratio',
                       type=float,
                       help='è®­ç»ƒé›†æ¯”ä¾‹ (é»˜è®¤: 0.7)')
    
    parser.add_argument('--val-ratio',
                       type=float,
                       help='éªŒè¯é›†æ¯”ä¾‹ (é»˜è®¤: 0.2)')
    
    parser.add_argument('--test-ratio',
                       type=float,
                       help='æµ‹è¯•é›†æ¯”ä¾‹ (é»˜è®¤: 0.1)')
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument('--output-dir',
                       help='è¾“å‡ºç›®å½• (é»˜è®¤: datasets)')
    
    parser.add_argument('--config', '-c',
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    
    # ä»£ç†å‚æ•°
    parser.add_argument('--use-proxy',
                       action='store_true',
                       help='å¯ç”¨ä»£ç†')
    
    parser.add_argument('--no-proxy',
                       action='store_true',
                       help='ç¦ç”¨ä»£ç†')
    
    parser.add_argument('--proxy-host',
                       default='127.0.0.1',
                       help='ä»£ç†ä¸»æœºåœ°å€ (é»˜è®¤: 127.0.0.1)')
    
    parser.add_argument('--proxy-port',
                       default='7891',
                       help='ä»£ç†ç«¯å£ (é»˜è®¤: 7891)')
    
    # æ•°æ®æºå‚æ•°
    parser.add_argument('--data-source',
                       choices=['fxminute', 'yfinance', 'truefx', 'oanda', 'histdata'],
                       default='fxminute',
                       help='æ•°æ®æºç±»å‹ (é»˜è®¤: fxminute)')
    
    parser.add_argument('--data-source-config',
                       help='æ•°æ®æºé…ç½®æ–‡ä»¶è·¯å¾„ (JSONæ ¼å¼)')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--verbose', '-v',
                       action='store_true',
                       help='è¯¦ç»†è¾“å‡º')
    
    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("TensorTrade å¸‚åœºæ•°æ®ä¸‹è½½å’Œé¢„å¤„ç†è„šæœ¬")
    print("æ”¯æŒå¤šæ•°æ®æºçš„å¸‚åœºæ•°æ®è·å–å’Œç‰¹å¾å·¥ç¨‹")
    print("=" * 60)
    
    # =============================================
    # é»˜è®¤å‚æ•°é…ç½® - åœ¨è¿™é‡Œä¿®æ”¹é»˜è®¤å€¼
    # =============================================
    DEFAULT_SYMBOL = "EURUSD"         # é»˜è®¤å¤–æ±‡ä»£ç  (æ¬§å…ƒ/ç¾å…ƒï¼ŒFX-1-Minute-Dataæ”¯æŒ)
    DEFAULT_SYMBOLS = None            # é»˜è®¤å¤šå¤–æ±‡ï¼ˆNoneè¡¨ç¤ºä½¿ç”¨å•å¤–æ±‡ï¼‰
    DEFAULT_PERIOD = "max"            # é»˜è®¤æ•°æ®å‘¨æœŸ (æ‰€æœ‰å¯ç”¨æ•°æ®ï¼Œ2000-2024å¹´)
    DEFAULT_INTERVAL = "1m"           # é»˜è®¤æ•°æ®é—´éš” (1åˆ†é’Ÿç²’åº¦ï¼ŒFXMinuteä»…æ”¯æŒ1åˆ†é’Ÿ)
    DEFAULT_TRAIN_RATIO = 0.7         # é»˜è®¤è®­ç»ƒé›†æ¯”ä¾‹
    DEFAULT_VAL_RATIO = 0.2           # é»˜è®¤éªŒè¯é›†æ¯”ä¾‹  
    DEFAULT_TEST_RATIO = 0.1          # é»˜è®¤æµ‹è¯•é›†æ¯”ä¾‹
    DEFAULT_OUTPUT_DIR = "datasets"   # é»˜è®¤è¾“å‡ºç›®å½•
    DEFAULT_CONFIG = None             # é»˜è®¤é…ç½®æ–‡ä»¶
    DEFAULT_VERBOSE = False           # é»˜è®¤è¯¦ç»†è¾“å‡º
    
    # ä»£ç†é…ç½® - ä¿®æ”¹è¿™é‡Œæ¥é…ç½®ä»£ç†è®¾ç½®ï¼ˆFXMinuteæœ¬åœ°æ•°æ®ä¸éœ€è¦ä»£ç†ï¼‰
    DEFAULT_USE_PROXY = False         # é»˜è®¤ä¸å¯ç”¨ä»£ç†ï¼ˆFXMinuteä½¿ç”¨æœ¬åœ°æ•°æ®ï¼‰
    DEFAULT_PROXY_HOST = "127.0.0.1"  # é»˜è®¤ä»£ç†ä¸»æœº
    DEFAULT_PROXY_PORT = "7891"       # é»˜è®¤ä»£ç†ç«¯å£ï¼ˆæœ¬åœ°socketä»£ç†ï¼‰
    
    # æ•°æ®æºé…ç½® - ä¿®æ”¹è¿™é‡Œæ¥é…ç½®é»˜è®¤æ•°æ®æº
    DEFAULT_DATA_SOURCE = "fxminute"   # é»˜è®¤æ•°æ®æºç±»å‹ (FX-1-Minute-Dataæœ¬åœ°ç¼“å­˜æ•°æ®)
    DEFAULT_DATA_SOURCE_CONFIG = None  # é»˜è®¤æ•°æ®æºé…ç½®æ–‡ä»¶
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_arguments()
    
    # ä½¿ç”¨é»˜è®¤å€¼è¦†ç›–æœªæŒ‡å®šçš„å‚æ•°
    symbol = args.symbol or DEFAULT_SYMBOL
    symbols = args.symbols or DEFAULT_SYMBOLS
    period = args.period or DEFAULT_PERIOD
    interval = args.interval or DEFAULT_INTERVAL
    train_ratio = args.train_ratio or DEFAULT_TRAIN_RATIO
    val_ratio = args.val_ratio or DEFAULT_VAL_RATIO
    test_ratio = args.test_ratio or DEFAULT_TEST_RATIO
    output_dir = args.output_dir or DEFAULT_OUTPUT_DIR
    config_path = args.config or DEFAULT_CONFIG
    verbose = args.verbose or DEFAULT_VERBOSE
    data_source = args.data_source or DEFAULT_DATA_SOURCE
    data_source_config_file = args.data_source_config or DEFAULT_DATA_SOURCE_CONFIG
    
    # å¤„ç†ä»£ç†è®¾ç½®
    if args.no_proxy:
        use_proxy = False
    elif args.use_proxy:
        use_proxy = True
    else:
        use_proxy = DEFAULT_USE_PROXY
    
    # è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆåœ¨åˆ›å»ºä¸‹è½½å™¨ä¹‹å‰ï¼‰
    os.environ['USE_PROXY'] = str(use_proxy).lower()
    os.environ['PROXY_HOST'] = args.proxy_host or DEFAULT_PROXY_HOST
    os.environ['PROXY_PORT'] = args.proxy_port or DEFAULT_PROXY_PORT
    os.environ['DATA_SOURCE_TYPE'] = data_source
    if data_source_config_file:
        os.environ['DATA_SOURCE_CONFIG'] = data_source_config_file
    
    # æ˜¾ç¤ºé…ç½®çŠ¶æ€
    if use_proxy:
        proxy_info = f"{os.environ['PROXY_HOST']}:{os.environ['PROXY_PORT']}"
        print(f"ä»£ç†é…ç½®: http://{proxy_info}")
    else:
        print("ä»£ç†é…ç½®: å·²ç¦ç”¨")
    
    print(f"æ•°æ®æº: {data_source}")
    if data_source_config_file:
        print(f"é…ç½®æ–‡ä»¶: {data_source_config_file}")
    
    try:
        # åˆ›å»ºæ•°æ®ä¸‹è½½å™¨
        downloader = DataDownloader(
            config_path=config_path,
            output_dir=output_dir
        )
        
        # æ‰§è¡Œä¸‹è½½
        if symbols:
            # å¤šä¸ªè‚¡ç¥¨
            symbols_list = [s.strip().upper() for s in symbols.split(',')]
            result = downloader.download_multiple_stocks(
                symbols=symbols_list,
                period=period,
                interval=interval,
                train_ratio=train_ratio,
                val_ratio=val_ratio,
                test_ratio=test_ratio
            )
            
            print(f"\næ‰¹é‡ä¸‹è½½å®Œæˆ:")
            print(f"æ€»è®¡: {result['total_symbols']} ä¸ªè‚¡ç¥¨")
            print(f"æˆåŠŸ: {result['successful_count']} ä¸ª")
            print(f"å¤±è´¥: {result['failed_count']} ä¸ª")
            
            if result['successful_symbols']:
                print(f"æˆåŠŸçš„è‚¡ç¥¨: {', '.join(result['successful_symbols'])}")
            
            if result['failed_symbols']:
                print(f"å¤±è´¥çš„è‚¡ç¥¨: {', '.join(result['failed_symbols'])}")
            
            if verbose:
                print("\nè¯¦ç»†ç»“æœ:")
                print(json.dumps(result, indent=2, ensure_ascii=False, default=str))
        
        else:
            # å•ä¸ªè‚¡ç¥¨ (é»˜è®¤æˆ–æŒ‡å®š)
            result = downloader.download_single_stock(
                symbol=symbol,
                period=period,
                interval=interval,
                train_ratio=train_ratio,
                val_ratio=val_ratio,
                test_ratio=test_ratio
            )
            
            if result['status'] == 'success':
                print(f"\nSUCCESS: {symbol} æ•°æ®ä¸‹è½½æˆåŠŸ!")
                print(f"è¾“å‡ºç›®å½•: {result['output_dir']}")
                if verbose:
                    print("\nè¯¦ç»†ä¿¡æ¯:")
                    print(json.dumps(result['metadata'], indent=2, ensure_ascii=False, default=str))
            else:
                print(f"\nFAILED: {symbol} æ•°æ®ä¸‹è½½å¤±è´¥:")
                print(f"é”™è¯¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                return 1
    
    except Exception as e:
        print(f"\næ‰§è¡Œå¤±è´¥: {e}")
        return 1
    
    print(f"\næ•°æ®ä¸‹è½½å®Œæˆï¼Œä¿å­˜è‡³: {output_dir}")
    return 0


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)