#!/usr/bin/env python
"""
å¸‚åœºæ•°æ®ä¸‹è½½å’Œé¢„å¤„ç†è„šæœ¬

åŠŸèƒ½:
1. æ”¯æŒå¤šæ•°æ®æº (YFinance, TrueFX, Oanda, HistData, FXMinute)
2. æ‰¹é‡ä¸‹è½½å¸‚åœºå†å²æ•°æ® (è‚¡ç¥¨/å¤–æ±‡/æœŸè´§ç­‰)
3. æ‰§è¡Œç‰¹å¾å·¥ç¨‹å¤„ç†
4. æ•°æ®é›†åˆ’åˆ†(è®­ç»ƒ/éªŒè¯/æµ‹è¯•)
5. ä¿å­˜å¤„ç†åçš„æ•°æ®åˆ°æœ¬åœ°

ä½¿ç”¨ç¤ºä¾‹:
  # ç›´æ¥è¿è¡Œ (é»˜è®¤ä¸‹è½½EURUSDå¤–æ±‡æ•°æ®ï¼ŒFX-1-Minute-Dataæ•°æ®æº)
  python download_data.py

  # ä¸‹è½½æŒ‡å®šå¤–æ±‡æ•°æ®ï¼Œä½¿ç”¨1å°æ—¶é—´éš”
  python download_data.py --symbol GBPUSD --period 2y --interval 1m

  # æ‰¹é‡ä¸‹è½½å¤šä¸ªå¤–æ±‡å¯¹ï¼Œä½¿ç”¨1åˆ†é’Ÿé—´éš”
  python download_data.py --symbols EURUSD,GBPUSD,USDJPY --period 60d --interval 1m

  # ä½¿ç”¨YFinanceæ•°æ®æºä¸‹è½½è‚¡ç¥¨æ•°æ®
  python download_data.py --data-source yfinance --symbol AAPL --period 1y

  # ä½¿ç”¨æ—¥æœŸèŒƒå›´ä¸‹è½½æ•°æ® (æ–°å¢åŠŸèƒ½)
  python download_data.py --symbol AAPL --start-date 2023-01-01 --end-date 2023-12-31
  
  # ç»“åˆæ•°æ®æºå’Œæ—¥æœŸèŒƒå›´
  python download_data.py --data-source yfinance --symbol AAPL --start-date 2023-06-01 --end-date 2023-08-31 --interval 1h

  # ä½¿ç”¨TrueFXæ•°æ®æºä¸‹è½½å¤–æ±‡æ•°æ®
  python download_data.py --data-source truefx --symbol EURUSD --period 1mo

  # ä½¿ç”¨Oandaæ•°æ®æºï¼ˆéœ€è¦é…ç½®æ–‡ä»¶ï¼‰
  python download_data.py --data-source oanda --data-source-config oanda_config.json --symbol GBPUSD

  # ä½¿ç”¨HistDataæ•°æ®æºä¸‹è½½å†å²æ•°æ®
  python download_data.py --data-source histdata --symbol GBPUSD --period 1y

  # ä½¿ç”¨FX-1-Minute-Dataæ•°æ®æºä¸‹è½½å†å²å¤–æ±‡æ•°æ®
  python download_data.py --data-source fxminute --symbol EURUSD --period 1mo

  # æŒ‡å®šæ•°æ®é›†åˆ’åˆ†æ¯”ä¾‹
  python download_data.py --symbol AAPL --train-ratio 0.7 --val-ratio 0.2 --test-ratio 0.1

  # ä½¿ç”¨è‡ªå®šä¹‰ä»£ç† (é€‚ç”¨äºyfinanceç­‰éœ€è¦ä»£ç†çš„æ•°æ®æº)
  python download_data.py --proxy-host 127.0.0.1 --proxy-port 7891

  # ç¦ç”¨ä»£ç†
  python download_data.py --no-proxy

  # å¯ç”¨åˆ†æ‰¹æ¬¡ä¸‹è½½ï¼ˆé€‚ç”¨äºå¤§æ•°æ®é‡ï¼‰
  python download_data.py --symbol EURUSD --period max --interval 1m --enable-batch-download

  # è‡ªå®šä¹‰åˆ†æ‰¹æ¬¡é…ç½®
  python download_data.py --symbol AAPL --period 5y --interval 1h --enable-batch-download --batch-threshold-days 180 --batch-size-days 30

  # ç¦ç”¨æ–­ç‚¹ç»­ä¼ 
  python download_data.py --enable-batch-download --no-resume

åˆ†æ‰¹æ¬¡ä¸‹è½½è¯´æ˜:
  - é€‚ç”¨äºå¤§æ•°æ®é‡ä¸‹è½½ï¼Œé¿å…å†…å­˜æº¢å‡ºå’Œç½‘ç»œè¶…æ—¶
  - è‡ªåŠ¨åˆ¤æ–­ä½•æ—¶å¯ç”¨åˆ†æ‰¹æ¬¡æ¨¡å¼ï¼šæ—¶é—´è·¨åº¦>365å¤© æˆ– 1åˆ†é’Ÿæ•°æ®>30å¤© æˆ– ä¼°ç®—è®°å½•æ•°>100ä¸‡æ¡
  - æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼Œä¸‹è½½ä¸­æ–­åå¯ä»¥ç»§ç»­
  - æ™ºèƒ½æ‰¹æ¬¡å¤§å°ï¼šæ ¹æ®æ•°æ®é—´éš”è‡ªåŠ¨è°ƒæ•´æ‰¹æ¬¡å¤§å°
  - å†…å­˜ç®¡ç†ï¼šè‡ªåŠ¨åƒåœ¾å›æ”¶ï¼Œç›‘æ§å†…å­˜ä½¿ç”¨
  - è¿›åº¦ä¿å­˜ï¼šå®šæœŸä¿å­˜ä¸‹è½½è¿›åº¦ï¼Œæ”¯æŒæ¢å¤

æ•°æ®æºè¯´æ˜:
  - fxminute: FX-1-Minute-Dataï¼Œæœ¬åœ°ç¼“å­˜å¤–æ±‡æ•°æ® (é»˜è®¤ï¼Œ2000-2024å¹´é«˜è´¨é‡æ•°æ®)
  - yfinance: Yahoo Financeï¼Œå…è´¹è‚¡ç¥¨æ•°æ® (æ”¯æŒä»£ç†)
  - truefx: TrueFXï¼Œå…è´¹å¤–æ±‡æ•°æ® (éœ€è¦æ³¨å†Œè´¦æˆ·)
  - oanda: Oandaï¼Œä¸“ä¸šå¤–æ±‡/CFDæ•°æ® (éœ€è¦APIè®¿é—®)
  - histdata: HistDataï¼Œå†å²å¤–æ±‡æ–‡ä»¶ (å…è´¹å†å²æ•°æ®)

æ•°æ®é—´éš”è¯´æ˜:
  - å„æ•°æ®æºæ”¯æŒçš„é—´éš”ä¸åŒ
  - fxminute: ä»…æ”¯æŒ1åˆ†é’ŸOHLCæ•°æ®ï¼Œ66+ä¸ªäº¤æ˜“å¯¹
  - yfinance: 1m(7å¤©), 5m(60å¤©), 1h(2å¹´), 1d(å†å²)
  - truefx: tickçº§å®æ—¶æ•°æ®å’Œæ—¥å†…æ•°æ®
  - oanda: ç§’çº§åˆ°æœˆçº§å¤šç§é—´éš”
  - histdata: 1åˆ†é’Ÿå†å²æ•°æ®æ–‡ä»¶

ä»£ç†é…ç½®è¯´æ˜:
  - é€‚ç”¨äºéœ€è¦ä»£ç†è®¿é—®çš„æ•°æ®æº (å¦‚yfinance)
  - é»˜è®¤ä½¿ç”¨ socks5://127.0.0.1:7891 ä»£ç†
  - æ”¯æŒé€šè¿‡ç¯å¢ƒå˜é‡é…ç½®: USE_PROXY, PROXY_HOST, PROXY_PORT
"""

import os
import sys
import argparse
import json
import warnings
import pickle
import time
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any, Union
import pandas as pd
import numpy as np
from tqdm import tqdm

from src.data.core.data_manager import DataManager
from src.data.sources.base import DataSource, DataPeriod, DataInterval
from src.features.feature_engineer import FeatureEngineer
from src.utils.config import Config
from src.utils.logger import setup_logger, get_default_log_file

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
PROJECT_ROOT = Path(__file__).parent
sys.path.insert(0, str(PROJECT_ROOT))

class DataDownloader:
    """
    æ•°æ®ä¸‹è½½å’Œé¢„å¤„ç†ç±»
    
    è´Ÿè´£æ‰¹é‡ä¸‹è½½è‚¡ç¥¨æ•°æ®ã€ç‰¹å¾å·¥ç¨‹å’Œæ•°æ®é›†åˆ’åˆ†
    """
    
    def __init__(self, config_path: Optional[str] = None, output_dir: str = "datasets", data_source_enum: Optional[DataSource] = None):
        """
        åˆå§‹åŒ–æ•°æ®ä¸‹è½½å™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            data_source_enum: æ•°æ®æºæšä¸¾ç±»å‹
        """
        # è®¾ç½®ä»£ç†ï¼ˆå¦‚æœéœ€è¦ï¼‰
        self._setup_proxy()
        
        # åŠ è½½é…ç½®
        self.config = Config(config_file=config_path) if config_path else Config()
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True, parents=True)
        
        # åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
        self.logger = setup_logger(
            name="DataDownloader",
            level="INFO",
            log_file=get_default_log_file("data_downloader")
        )
        
        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶ï¼Œä¼ é€’ä»£ç†é…ç½®å’Œæ•°æ®æºç±»å‹
        if data_source_enum is None:
            # åå¤‡æ–¹æ¡ˆï¼šä»ç¯å¢ƒå˜é‡è·å–å¹¶è½¬æ¢ä¸ºæšä¸¾
            data_source_type = os.getenv('DATA_SOURCE_TYPE', 'fxminute')
            data_source_enum = DataSource.from_string(data_source_type)
        
        data_source_config = self._load_data_source_config()
        
        # ä¸ºFXMinuteæ•°æ®æºæä¾›é»˜è®¤é…ç½®
        if data_source_enum == DataSource.FXMINUTE and not data_source_config:
            data_source_config = {
                'data_directory': str(PROJECT_ROOT / 'local_data' / 'FX-1-Minute-Data'),
                'auto_extract': True,
                'cache_extracted': True,
                'extracted_cache_dir': str(PROJECT_ROOT / 'fx_minute_cache')
            }
        
        self.data_manager = DataManager(
            config=self.config,
            data_source_type=data_source_enum,
            data_source_config=data_source_config
        )
        self.feature_engineer = FeatureEngineer(self.config)
        
        # åˆ†æ‰¹æ¬¡ä¸‹è½½åŠŸèƒ½ç°åœ¨é›†æˆåœ¨ DataManager ä¸­
        
        # è®¾ç½®ä»£ç†é…ç½®åˆ° DataManager
        self._configure_data_manager_proxy()
        
        self.logger.info("æ•°æ®ä¸‹è½½å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _setup_proxy(self):
        """
        è®¾ç½®ä»£ç†é…ç½®
        """
        use_proxy = os.getenv('USE_PROXY', 'true').lower() == 'true'
        
        if use_proxy:
            proxy_host = os.getenv('PROXY_HOST', '127.0.0.1')
            proxy_port = os.getenv('PROXY_PORT', '7891')
            proxy_url = f"socks5://{proxy_host}:{proxy_port}"
            
            # è®¾ç½®å…¨å±€ç¯å¢ƒå˜é‡ä»£ç†
            os.environ['HTTP_PROXY'] = proxy_url
            os.environ['HTTPS_PROXY'] = proxy_url
            os.environ['http_proxy'] = proxy_url
            os.environ['https_proxy'] = proxy_url
            print(f"SUCCESS: å…¨å±€ä»£ç†å·²è®¾ç½®: {proxy_url}")
        else:
            print("SUCCESS: ä¸ä½¿ç”¨ä»£ç†")
    
    def _configure_data_manager_proxy(self):
        """
        ä¸º DataManager é…ç½®ä»£ç†è®¾ç½®
        """
        use_proxy = os.getenv('USE_PROXY', 'true').lower() == 'true'
        
        if use_proxy:
            proxy_host = os.getenv('PROXY_HOST', '127.0.0.1')
            proxy_port = os.getenv('PROXY_PORT', '7891')
            proxy_url = f"socks5://{proxy_host}:{proxy_port}"
            
            # é…ç½® DataManager ä»£ç†
            if hasattr(self.data_manager, 'set_proxy'):
                self.data_manager.set_proxy(proxy_url)
                self.logger.info(f"DataManager ä»£ç†é…ç½®: {proxy_url}")
            else:
                self.logger.warning("DataManager ä¸æ”¯æŒä»£ç†é…ç½®")
        else:
            self.logger.info("DataManager ä¸ä½¿ç”¨ä»£ç†")
    
    def _load_data_source_config(self) -> Dict:
        """
        åŠ è½½æ•°æ®æºé…ç½®
        
        Returns:
            æ•°æ®æºé…ç½®å­—å…¸
        """
        config_file = os.getenv('DATA_SOURCE_CONFIG')
        if config_file and os.path.exists(config_file):
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    import json
                    config = json.load(f)
                    self.logger.info(f"Loaded data source config from: {config_file}")
                    return config
            except Exception as e:
                self.logger.warning(f"Failed to load data source config: {e}")
        
        return {}
    
    def get_supported_periods(self) -> List[DataPeriod]:
        """
        è·å–æ”¯æŒçš„æ•°æ®å‘¨æœŸæšä¸¾åˆ—è¡¨
        
        Returns:
            List[DataPeriod]: æ”¯æŒçš„æ•°æ®å‘¨æœŸåˆ—è¡¨
        """
        return self.data_manager.get_supported_periods()
    
    def get_period_info(self, period: Union[str, DataPeriod]) -> Dict[str, Any]:
        """
        è·å–æ•°æ®å‘¨æœŸçš„è¯¦ç»†ä¿¡æ¯
        
        Args:
            period: æ•°æ®å‘¨æœŸï¼ˆå­—ç¬¦ä¸²æˆ–æšä¸¾ï¼‰
            
        Returns:
            Dict[str, Any]: å‘¨æœŸè¯¦ç»†ä¿¡æ¯
        """
        return self.data_manager.get_period_info(period)
    
    def convert_period_to_date_range(
        self, 
        period: Union[str, DataPeriod], 
        end_date: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        å°†æ•°æ®å‘¨æœŸè½¬æ¢ä¸ºæ—¥æœŸèŒƒå›´
        
        Args:
            period: æ•°æ®å‘¨æœŸï¼ˆå­—ç¬¦ä¸²æˆ–æšä¸¾ï¼‰
            end_date: ç»“æŸæ—¥æœŸï¼Œé»˜è®¤ä¸ºå½“å‰æ—¥æœŸ
            
        Returns:
            Dict[str, Any]: åŒ…å«æ—¥æœŸèŒƒå›´ä¿¡æ¯çš„å­—å…¸
        """
        date_range = self.data_manager.convert_period_to_date_range(period, end_date)
        return {
            'start_date': date_range.start_date.strftime('%Y-%m-%d'),
            'end_date': date_range.end_date.strftime('%Y-%m-%d'),
            'duration_days': date_range.duration_days,
            'period_display': period.display_name if isinstance(period, DataPeriod) else str(period)
        }
    
    def download_single_stock(
        self,
        symbol: str,
        period: Union[str, DataPeriod] = "2y",
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        interval: Union[str, DataInterval] = "1d",
        train_ratio: float = 0.7,
        val_ratio: float = 0.2,
        test_ratio: float = 0.1
    ) -> Dict[str, Any]:
        """
        ä¸‹è½½å•ä¸ªè‚¡ç¥¨æ•°æ®å¹¶è¿›è¡Œé¢„å¤„ç†
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            period: æ•°æ®å‘¨æœŸ (æ”¯æŒå­—ç¬¦ä¸²æˆ–DataPeriodæšä¸¾ï¼Œå¦‚æœæœªæŒ‡å®šstart_dateå’Œend_date)
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DDæ ¼å¼)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DDæ ¼å¼)
            interval: æ•°æ®é—´éš” (æ”¯æŒå­—ç¬¦ä¸²æˆ–DataIntervalæšä¸¾)
            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
            val_ratio: éªŒè¯é›†æ¯”ä¾‹  
            test_ratio: æµ‹è¯•é›†æ¯”ä¾‹
            
        Returns:
            Dict[str, Any]: å¤„ç†ç»“æœ
        """
        data_source = os.getenv('DATA_SOURCE_TYPE', 'yfinance')
        start_time = time.time()
        
        # å¤„ç†DataPeriodæšä¸¾
        period_display = period.display_name if isinstance(period, DataPeriod) else str(period)
        period_value = period if isinstance(period, (str, DataPeriod)) else str(period)
        
        # å¤„ç†DataIntervalæšä¸¾  
        interval_display = interval.value if isinstance(interval, DataInterval) else str(interval)
        interval_value = interval.value if isinstance(interval, DataInterval) else str(interval)
        
        # æ‰“å°å¼€å§‹ä¿¡æ¯
        print(f"\n{'='*60}")
        print(f"å¼€å§‹å¤„ç†: {symbol}")
        
        # æ˜¾ç¤ºæ—¶é—´èŒƒå›´ä¿¡æ¯
        if start_date and end_date:
            print(f"æ•°æ®æº: {data_source} | æ—¶é—´èŒƒå›´: {start_date} ~ {end_date} | é—´éš”: {interval_display}")
            self.logger.info(f"å¼€å§‹ä¸‹è½½æ•°æ®: {symbol} (æ•°æ®æº: {data_source}), æ—¶é—´èŒƒå›´: {start_date} ~ {end_date}, é—´éš”: {interval_display}")
        else:
            print(f"æ•°æ®æº: {data_source} | å‘¨æœŸ: {period_display} | é—´éš”: {interval_display}")
            if isinstance(period, DataPeriod):
                self.logger.info(f"å¼€å§‹ä¸‹è½½æ•°æ®: {symbol} (æ•°æ®æº: {data_source}), å‘¨æœŸ: {period_display} ({period.to_days()}å¤©), é—´éš”: {interval_display}")
            else:
                self.logger.info(f"å¼€å§‹ä¸‹è½½æ•°æ®: {symbol} (æ•°æ®æº: {data_source}), å‘¨æœŸ: {period_display}, é—´éš”: {interval_display}")
        
        print(f"æ•°æ®é›†åˆ’åˆ†: è®­ç»ƒ({train_ratio:.0%}) éªŒè¯({val_ratio:.0%}) æµ‹è¯•({test_ratio:.0%})")
        print(f"{'='*60}")
        
        try:
            # éªŒè¯æ¯”ä¾‹
            if abs(train_ratio + val_ratio + test_ratio - 1.0) > 1e-6:
                raise ValueError("è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•é›†æ¯”ä¾‹ä¹‹å’Œå¿…é¡»ç­‰äº1.0")
            
            # åˆ›å»ºè¿›åº¦æ¡
            with tqdm(total=5, desc="æ•°æ®å¤„ç†è¿›åº¦", ncols=80, colour='green') as pbar:
                
                # 1. è·å–åŸå§‹æ•°æ®
                pbar.set_description("è·å–åŸå§‹æ•°æ®")
                self.logger.info(f"ä» {data_source} è·å– {symbol} æ•°æ®...")
                step_start = time.time()
                
                # ä½¿ç”¨DataManagerçš„æ™ºèƒ½æ•°æ®è·å–ï¼ˆå†…éƒ¨ä¼šè‡ªåŠ¨åˆ¤æ–­æ˜¯å¦ä½¿ç”¨åˆ†æ‰¹æ¬¡ä¸‹è½½ï¼‰
                if start_date and end_date:
                    # ä½¿ç”¨æ—¥æœŸèŒƒå›´ä¸‹è½½
                    raw_data = self.data_manager.get_stock_data_by_date_range(
                        symbol=symbol,
                        start_date=start_date,
                        end_date=end_date,
                        interval=interval_value
                    )
                else:
                    # ä½¿ç”¨ä¼ ç»Ÿçš„å‘¨æœŸä¸‹è½½ï¼ˆæ”¯æŒDataPeriodæšä¸¾ï¼‰
                    raw_data = self.data_manager.get_stock_data(
                        symbol=symbol,
                        period=period_value,  # ä½¿ç”¨period_valueæ”¯æŒDataPeriodæšä¸¾
                        interval=interval_value
                    )
                
                step_time = time.time() - step_start
                
                print(f"æ•°æ®è·å–å®Œæˆ: {len(raw_data):,} æ¡è®°å½• ({step_time:.2f}ç§’)")
                self.logger.info(f"æ•°æ®è·å–å®Œæˆ: {len(raw_data)} æ¡è®°å½• (è€—æ—¶: {step_time:.2f}ç§’)")
                pbar.update(1)
                
                # 2. ç‰¹å¾å·¥ç¨‹
                pbar.set_description("æ‰§è¡Œç‰¹å¾å·¥ç¨‹")
                self.logger.info("æ‰§è¡Œç‰¹å¾å·¥ç¨‹...")
                step_start = time.time()
                features_data = self.feature_engineer.prepare_features(raw_data)
                step_time = time.time() - step_start
                
                # åˆ†æç‰¹å¾ç±»åˆ«
                feature_stats = self._analyze_feature_categories(features_data.columns)
                print(f"âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆ: {len(features_data):,} æ¡è®°å½•, {len(features_data.columns)} ä¸ªç‰¹å¾ ({step_time:.2f}ç§’)")
                print(f"   ç‰¹å¾åˆ†å¸ƒ: {feature_stats}")
                self.logger.info(f"ç‰¹å¾å·¥ç¨‹å®Œæˆ: {len(features_data)} æ¡è®°å½•, {len(features_data.columns)} ä¸ªç‰¹å¾ (è€—æ—¶: {step_time:.2f}ç§’)")
                pbar.update(1)
                
                # 3. æ•°æ®è´¨é‡æ£€æŸ¥
                pbar.set_description("ğŸ” æ•°æ®è´¨é‡æ£€æŸ¥")
                step_start = time.time()
                quality_report = self._check_data_quality(features_data)
                step_time = time.time() - step_start
                
                print(f"âœ… æ•°æ®è´¨é‡æ£€æŸ¥: {quality_report} ({step_time:.2f}ç§’)")
                self.logger.info(f"æ•°æ®è´¨é‡æ£€æŸ¥å®Œæˆ (è€—æ—¶: {step_time:.2f}ç§’)")
                pbar.update(1)
                
                # 4. æ•°æ®é›†åˆ’åˆ†
                pbar.set_description("âœ‚ï¸ åˆ’åˆ†æ•°æ®é›†")
                self.logger.info("åˆ’åˆ†æ•°æ®é›†...")
                step_start = time.time()
                datasets = self._split_dataset(features_data, train_ratio, val_ratio, test_ratio)
                step_time = time.time() - step_start
                
                print(f"âœ… æ•°æ®é›†åˆ’åˆ†å®Œæˆ ({step_time:.2f}ç§’)")
                for split_name, data in datasets.items():
                    print(f"   {split_name}: {len(data):,} æ¡è®°å½•")
                pbar.update(1)
                
                # 5. ä¿å­˜æ•°æ®
                pbar.set_description("ğŸ’¾ ä¿å­˜æ•°æ®æ–‡ä»¶")
                step_start = time.time()
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                symbol_dir = self.output_dir / f"{symbol}_{timestamp}"
                symbol_dir.mkdir(exist_ok=True)
                
                saved_files = self._save_datasets(symbol, datasets, symbol_dir)
                
                # ä¿å­˜å…ƒæ•°æ®
                metadata = {
                    'symbol': symbol,
                    'period': period,
                    'start_date': start_date,  # æ–°å¢
                    'end_date': end_date,      # æ–°å¢
                    'interval': interval_display,
                    'download_time': timestamp,
                    'raw_data_shape': raw_data.shape,
                    'features_data_shape': features_data.shape,
                    'train_ratio': train_ratio,
                    'val_ratio': val_ratio,
                    'test_ratio': test_ratio,
                    'dataset_shapes': {k: v.shape for k, v in datasets.items()},
                    'feature_columns': list(features_data.columns),
                    'feature_categories': feature_stats,
                    'quality_report': quality_report,
                    'processing_time': time.time() - start_time,
                    'saved_files': saved_files
                }
                
                metadata_file = symbol_dir / "metadata.json"
                with open(metadata_file, 'w', encoding='utf-8') as f:
                    json.dump(metadata, f, indent=2, ensure_ascii=False, default=str)
                
                step_time = time.time() - step_start
                print(f"âœ… æ•°æ®ä¿å­˜å®Œæˆ: {symbol_dir.name} ({step_time:.2f}ç§’)")
                self.logger.info(f"æ•°æ®ä¿å­˜å®Œæˆ: {symbol_dir} (è€—æ—¶: {step_time:.2f}ç§’)")
                pbar.update(1)
            
            # æ€»ç»“ä¿¡æ¯
            total_time = time.time() - start_time
            print(f"\nğŸ‰ {symbol} å¤„ç†æˆåŠŸå®Œæˆ!")
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {symbol_dir}")
            print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f}ç§’")
            print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡: {len(raw_data):,} â†’ {len(features_data):,} æ¡è®°å½•, {len(features_data.columns)} ä¸ªç‰¹å¾")
            print(f"{'='*60}")
            
            return {
                'status': 'success',
                'symbol': symbol,
                'output_dir': str(symbol_dir),
                'metadata': metadata
            }
            
        except Exception as e:
            total_time = time.time() - start_time
            print(f"\nâŒ {symbol} å¤„ç†å¤±è´¥!")
            print(f"ğŸ”¥ é”™è¯¯: {e}")
            print(f"â±ï¸  è€—æ—¶: {total_time:.2f}ç§’")
            print(f"{'='*60}")
            
            self.logger.error(f"ä¸‹è½½ {symbol} æ•°æ®å¤±è´¥: {e} (è€—æ—¶: {total_time:.2f}ç§’)")
            return {
                'status': 'error',
                'symbol': symbol,
                'error': str(e)
            }
    
    def download_multiple_stocks(
        self,
        symbols: List[str],
        period: Union[str, DataPeriod] = "2y",
        interval: Union[str, DataInterval] = "1d",
        train_ratio: float = 0.7,
        val_ratio: float = 0.2,
        test_ratio: float = 0.1
    ) -> Dict[str, Any]:
        """
        æ‰¹é‡ä¸‹è½½å¤šä¸ªè‚¡ç¥¨æ•°æ®
        
        Args:
            symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            period: æ•°æ®å‘¨æœŸ (æ”¯æŒå­—ç¬¦ä¸²æˆ–DataPeriodæšä¸¾)
            interval: æ•°æ®é—´éš” (æ”¯æŒå­—ç¬¦ä¸²æˆ–DataIntervalæšä¸¾)
            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
            val_ratio: éªŒè¯é›†æ¯”ä¾‹
            test_ratio: æµ‹è¯•é›†æ¯”ä¾‹
            
        Returns:
            Dict[str, Any]: æ‰¹é‡å¤„ç†ç»“æœ
        """
        data_source = os.getenv('DATA_SOURCE_TYPE', 'yfinance')
        batch_start_time = time.time()
        
        # æ‰“å°æ‰¹é‡å¤„ç†å¼€å§‹ä¿¡æ¯
        print(f"\n{'='*80}")
        print(f"ğŸš€ æ‰¹é‡æ•°æ®å¤„ç†å¼€å§‹")
        print(f"ç¬¦å·åˆ—è¡¨: {', '.join(symbols)}")
        print(f"æ•°æ®æº: {data_source} | å‘¨æœŸ: {period} | é—´éš”: {interval}")
        print(f"æ€»æ•°: {len(symbols)} ä¸ªç¬¦å·")
        print(f"{'='*80}")
        
        self.logger.info(f"å¼€å§‹æ‰¹é‡ä¸‹è½½ {len(symbols)} ä¸ªç¬¦å·æ•°æ® (æ•°æ®æº: {data_source})")
        
        results = {}
        successful_downloads = []
        failed_downloads = []
        
        # ä½¿ç”¨tqdmæ˜¾ç¤ºæ‰¹é‡å¤„ç†è¿›åº¦
        with tqdm(symbols, desc="æ‰¹é‡å¤„ç†", ncols=100, colour='blue') as pbar:
            for i, symbol in enumerate(pbar, 1):
                pbar.set_description(f"å¤„ç† {symbol} ({i}/{len(symbols)})")
                self.logger.info(f"å¤„ç†ç¬¦å·: {symbol} ({i}/{len(symbols)})")
                
                symbol_start_time = time.time()
                result = self.download_single_stock(symbol, period, interval, train_ratio, val_ratio, test_ratio)
                symbol_time = time.time() - symbol_start_time
                
                results[symbol] = result
                result['processing_time'] = symbol_time
                
                if result['status'] == 'success':
                    successful_downloads.append(symbol)
                    self.logger.info(f"SUCCESS: {symbol} ä¸‹è½½æˆåŠŸ (è€—æ—¶: {symbol_time:.2f}ç§’)")
                    pbar.set_postfix(success=len(successful_downloads), failed=len(failed_downloads))
                else:
                    failed_downloads.append(symbol)
                    self.logger.error(f"FAILED: {symbol} ä¸‹è½½å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')} (è€—æ—¶: {symbol_time:.2f}ç§’)")
                    pbar.set_postfix(success=len(successful_downloads), failed=len(failed_downloads))
        
        # æ‰¹é‡å¤„ç†æ€»ç»“
        total_batch_time = time.time() - batch_start_time
        
        # ä¿å­˜æ‰¹é‡å¤„ç†æ‘˜è¦
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        summary = {
            'batch_download_time': timestamp,
            'total_symbols': len(symbols),
            'successful_count': len(successful_downloads),
            'failed_count': len(failed_downloads),
            'successful_symbols': successful_downloads,
            'failed_symbols': failed_downloads,
            'period': period,
            'interval': interval,
            'train_ratio': train_ratio,
            'val_ratio': val_ratio,
            'test_ratio': test_ratio,
            'total_processing_time': total_batch_time,
            'average_time_per_symbol': total_batch_time / len(symbols) if symbols else 0,
            'detailed_results': results
        }
        
        summary_file = self.output_dir / f"batch_download_summary_{timestamp}.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False, default=str)
        
        # æ‰“å°æ‰¹é‡å¤„ç†å®Œæˆä¿¡æ¯
        print(f"\n{'='*80}")
        print(f"ğŸ¯ æ‰¹é‡å¤„ç†å®Œæˆ!")
        print(f"âœ… æˆåŠŸ: {len(successful_downloads)}/{len(symbols)} ä¸ªç¬¦å·")
        print(f"âŒ å¤±è´¥: {len(failed_downloads)}/{len(symbols)} ä¸ªç¬¦å·")
        print(f"â±ï¸  æ€»è€—æ—¶: {total_batch_time:.2f}ç§’")
        print(f"ğŸ“Š å¹³å‡è€—æ—¶: {total_batch_time/len(symbols):.2f}ç§’/ç¬¦å·")
        
        if successful_downloads:
            print(f"âœ… æˆåŠŸç¬¦å·: {', '.join(successful_downloads)}")
        if failed_downloads:
            print(f"âŒ å¤±è´¥ç¬¦å·: {', '.join(failed_downloads)}")
            
        print(f"ğŸ“ æ‘˜è¦æ–‡ä»¶: {summary_file}")
        print(f"{'='*80}")
        
        self.logger.info(f"æ‰¹é‡ä¸‹è½½å®Œæˆ: æˆåŠŸ {len(successful_downloads)}, å¤±è´¥ {len(failed_downloads)}, æ€»è€—æ—¶: {total_batch_time:.2f}ç§’")
        
        return summary
    
    def _split_dataset(
        self,
        data: pd.DataFrame,
        train_ratio: float,
        val_ratio: float,
        test_ratio: float
    ) -> Dict[str, pd.DataFrame]:
        """
        æŒ‰æ—¶é—´é¡ºåºåˆ’åˆ†æ•°æ®é›†
        
        Args:
            data: ç‰¹å¾æ•°æ®
            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
            val_ratio: éªŒè¯é›†æ¯”ä¾‹
            test_ratio: æµ‹è¯•é›†æ¯”ä¾‹
            
        Returns:
            Dict[str, pd.DataFrame]: åˆ’åˆ†åçš„æ•°æ®é›†
        """
        total_len = len(data)
        
        # è®¡ç®—åˆ†å‰²ç‚¹
        train_end = int(total_len * train_ratio)
        val_end = int(total_len * (train_ratio + val_ratio))
        
        # æŒ‰æ—¶é—´é¡ºåºåˆ’åˆ†
        train_data = data.iloc[:train_end].copy()
        val_data = data.iloc[train_end:val_end].copy()
        test_data = data.iloc[val_end:].copy()
        
        self.logger.info(f"æ•°æ®é›†åˆ’åˆ†å®Œæˆ:")
        self.logger.info(f"  è®­ç»ƒé›†: {len(train_data)} æ¡è®°å½• ({len(train_data)/total_len:.1%})")
        self.logger.info(f"  éªŒè¯é›†: {len(val_data)} æ¡è®°å½• ({len(val_data)/total_len:.1%})")
        self.logger.info(f"  æµ‹è¯•é›†: {len(test_data)} æ¡è®°å½• ({len(test_data)/total_len:.1%})")
        
        return {
            'train': train_data,
            'val': val_data,
            'test': test_data
        }
    
    def _save_datasets(
        self,
        symbol: str,
        datasets: Dict[str, pd.DataFrame],
        output_dir: Path
    ) -> Dict[str, str]:
        """
        ä¿å­˜æ•°æ®é›†åˆ°æ–‡ä»¶
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            datasets: æ•°æ®é›†å­—å…¸
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            Dict[str, str]: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        saved_files = {}
        
        for split_name, data in datasets.items():
            # åŒæ—¶ä¿å­˜CSVå’ŒPickleæ ¼å¼
            csv_file = output_dir / f"{symbol}_{split_name}.csv"
            pkl_file = output_dir / f"{symbol}_{split_name}.pkl"
            
            # ä¿å­˜CSV (ä¾¿äºæŸ¥çœ‹)
            data.to_csv(csv_file, index=True)
            
            # ä¿å­˜Pickle (ä¾¿äºå¿«é€ŸåŠ è½½)
            with open(pkl_file, 'wb') as f:
                pickle.dump(data, f)
            
            saved_files[f'{split_name}_csv'] = str(csv_file)
            saved_files[f'{split_name}_pkl'] = str(pkl_file)
            
            self.logger.info(f"ä¿å­˜ {split_name} æ•°æ®: {len(data)} æ¡è®°å½• -> {csv_file.name}")
        
        return saved_files
    
    def load_dataset(
        self,
        symbol: str,
        split: str = "train",
        data_dir: Optional[str] = None,
        format_type: str = "pkl"
    ) -> pd.DataFrame:
        """
        åŠ è½½æ•°æ®é›†
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            split: æ•°æ®é›†åˆ†å‰² (train/val/test)
            data_dir: æ•°æ®ç›®å½•
            format_type: æ–‡ä»¶æ ¼å¼ (pkl/csv)
            
        Returns:
            pd.DataFrame: åŠ è½½çš„æ•°æ®
        """
        if data_dir:
            base_dir = Path(data_dir)
        else:
            # æŸ¥æ‰¾æœ€æ–°çš„æ•°æ®ç›®å½•
            symbol_dirs = list(self.output_dir.glob(f"{symbol}_*"))
            if not symbol_dirs:
                raise FileNotFoundError(f"æœªæ‰¾åˆ° {symbol} çš„æ•°æ®ç›®å½•")
            base_dir = max(symbol_dirs)  # é€‰æ‹©æœ€æ–°çš„
        
        if format_type == "pkl":
            file_path = base_dir / f"{symbol}_{split}.pkl"
            with open(file_path, 'rb') as f:
                data = pickle.load(f)
        else:
            file_path = base_dir / f"{symbol}_{split}.csv"
            data = pd.read_csv(file_path, index_col=0)
        
        self.logger.info(f"åŠ è½½æ•°æ®é›†: {file_path} ({len(data)} æ¡è®°å½•)")
        return data
    
    def _analyze_feature_categories(self, feature_columns: List[str]) -> Dict[str, int]:
        """
        åˆ†æç‰¹å¾ç±»åˆ«åˆ†å¸ƒ
        
        Args:
            feature_columns: ç‰¹å¾åˆ—ååˆ—è¡¨
            
        Returns:
            Dict[str, int]: å„ç±»åˆ«ç‰¹å¾æ•°é‡
        """
        categories = {
            'Price': 0,      # ä»·æ ¼ç›¸å…³
            'Volume': 0,     # æˆäº¤é‡ç›¸å…³
            'MA': 0,         # ç§»åŠ¨å¹³å‡
            'Momentum': 0,   # åŠ¨é‡æŒ‡æ ‡
            'Volatility': 0, # æ³¢åŠ¨æ€§æŒ‡æ ‡
            'Trend': 0,      # è¶‹åŠ¿æŒ‡æ ‡
            'Statistical': 0,# ç»Ÿè®¡ç‰¹å¾
            'Others': 0      # å…¶ä»–
        }
        
        for col in feature_columns:
            col_upper = col.upper()
            if any(x in col_upper for x in ['OPEN', 'HIGH', 'LOW', 'CLOSE', 'PRICE']):
                categories['Price'] += 1
            elif 'VOLUME' in col_upper or 'OBV' in col_upper:
                categories['Volume'] += 1
            elif any(x in col_upper for x in ['SMA', 'EMA', 'WMA']):
                categories['MA'] += 1
            elif any(x in col_upper for x in ['RSI', 'MACD', 'ROC', 'MOM', 'STOCH', 'WILLIAMS', 'CCI']):
                categories['Momentum'] += 1
            elif any(x in col_upper for x in ['ATR', 'BBANDS', 'VOLATILITY', 'STD']):
                categories['Volatility'] += 1
            elif any(x in col_upper for x in ['ADX', 'DI', 'SAR', 'ICHIMOKU', 'TREND']):
                categories['Trend'] += 1
            elif any(x in col_upper for x in ['MEAN', 'MEDIAN', 'SKEW', 'KURT', 'CORR', 'Z_SCORE']):
                categories['Statistical'] += 1
            else:
                categories['Others'] += 1
        
        # åªè¿”å›éé›¶ç±»åˆ«
        return {k: v for k, v in categories.items() if v > 0}
    
    def _check_data_quality(self, data: pd.DataFrame) -> Dict[str, Any]:
        """
        æ£€æŸ¥æ•°æ®è´¨é‡
        
        Args:
            data: ç‰¹å¾æ•°æ®
            
        Returns:
            Dict[str, Any]: æ•°æ®è´¨é‡æŠ¥å‘Š
        """
        total_cells = data.shape[0] * data.shape[1]
        nan_count = data.isnull().sum().sum()
        inf_count = np.isinf(data.select_dtypes(include=[np.number])).sum().sum()
        
        # æ£€æŸ¥å¸¸é‡ç‰¹å¾
        constant_features = []
        for col in data.columns:
            if data[col].nunique() <= 1:
                constant_features.append(col)
        
        # æ£€æŸ¥å¼‚å¸¸å€¼ (è¶…è¿‡3ä¸ªæ ‡å‡†å·®)
        numeric_cols = data.select_dtypes(include=[np.number]).columns
        outliers = {}
        for col in numeric_cols:
            if data[col].std() > 0:  # é¿å…é™¤é›¶
                z_scores = np.abs((data[col] - data[col].mean()) / data[col].std())
                outlier_count = (z_scores > 3).sum()
                if outlier_count > 0:
                    outliers[col] = outlier_count
        
        return {
            'total_cells': total_cells,
            'missing_values': int(nan_count),
            'infinite_values': int(inf_count),
            'missing_percentage': round(nan_count / total_cells * 100, 2),
            'constant_features': len(constant_features),
            'outlier_features': len(outliers),
            'quality_score': round((1 - (nan_count + inf_count) / total_cells) * 100, 2)
        }
    
    def _should_use_batch_download(self, period: str, interval: str, threshold_days: int) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨åˆ†æ‰¹æ¬¡ä¸‹è½½
        
        Args:
            period: æ•°æ®å‘¨æœŸ
            interval: æ•°æ®é—´éš”
            threshold_days: é˜ˆå€¼å¤©æ•°
            
        Returns:
            bool: æ˜¯å¦éœ€è¦åˆ†æ‰¹æ¬¡ä¸‹è½½
        """
        # è§£æperiodåˆ°å¤©æ•°
        period_days = self._parse_period_to_days(period)
        
        # ä¼°ç®—æ•°æ®é‡
        interval_multipliers = {
            '1m': 1440,    # æ¯å¤©1440æ¡è®°å½•
            '5m': 288,     # æ¯å¤©288æ¡è®°å½•
            '15m': 96,     # æ¯å¤©96æ¡è®°å½•
            '30m': 48,     # æ¯å¤©48æ¡è®°å½•
            '1h': 24,      # æ¯å¤©24æ¡è®°å½•
            '1d': 1,       # æ¯å¤©1æ¡è®°å½•
            '1wk': 0.14,   # æ¯å‘¨1æ¡è®°å½•
            '1mo': 0.03,   # æ¯æœˆ1æ¡è®°å½•
        }
        
        multiplier = interval_multipliers.get(interval, 1)
        estimated_records = period_days * multiplier
        
        # åˆ¤æ–­æ¡ä»¶ï¼š
        # 1. æ—¶é—´è·¨åº¦è¶…è¿‡é˜ˆå€¼
        # 2. é«˜é¢‘æ•°æ®ï¼ˆ1m, 5mï¼‰ä¸”æ—¶é—´è·¨åº¦è¾ƒé•¿
        # 3. ä¼°ç®—è®°å½•æ•°è¶…è¿‡100ä¸‡æ¡
        
        if period_days > threshold_days:
            return True
        
        if interval in ['1m', '5m'] and period_days > 30:
            return True
            
        if estimated_records > 1000000:  # 100ä¸‡æ¡è®°å½•
            return True
            
        return False
    
    def _parse_period_to_days(self, period: str) -> int:
        """
        è§£æperiodå­—ç¬¦ä¸²åˆ°å¤©æ•°
        
        Args:
            period: å‘¨æœŸå­—ç¬¦ä¸² (å¦‚ '1y', '6mo', '30d', 'max')
            
        Returns:
            int: å¤©æ•°
        """
        if period == 'max':
            return 365 * 20  # å‡è®¾æœ€å¤§20å¹´
        
        period = period.lower()
        
        if period.endswith('d'):
            return int(period[:-1])
        elif period.endswith('w'):
            return int(period[:-1]) * 7
        elif period.endswith('mo'):
            return int(period[:-2]) * 30
        elif period.endswith('y'):
            return int(period[:-1]) * 365
        else:
            # é»˜è®¤æŒ‰å¤©å¤„ç†
            try:
                return int(period)
            except:
                return 365  # é»˜è®¤1å¹´
    
    def _download_with_batches(self, symbol: str, period: str, interval: str) -> pd.DataFrame:
        """
        ä½¿ç”¨åˆ†æ‰¹æ¬¡ä¸‹è½½æ¨¡å¼
        
        Args:
            symbol: äº¤æ˜“ç¬¦å·
            period: æ•°æ®å‘¨æœŸ
            interval: æ•°æ®é—´éš”
            
        Returns:
            pd.DataFrame: ä¸‹è½½çš„æ•°æ®
        """
        # è®¡ç®—æ—¶é—´èŒƒå›´
        end_date = datetime.now()
        period_days = self._parse_period_to_days(period)
        start_date = end_date - timedelta(days=period_days)
        
        start_str = start_date.strftime('%Y-%m-%d')
        end_str = end_date.strftime('%Y-%m-%d')
        
        self.logger.info(f"ä½¿ç”¨åˆ†æ‰¹æ¬¡ä¸‹è½½: {symbol}, {start_str} åˆ° {end_str}")
        
        # ä¼°ç®—ä¸‹è½½æ—¶é—´ï¼ˆä½¿ç”¨ DataManager çš„å†…ç½®æ–¹æ³•ï¼‰
        estimation = self.data_manager.get_batch_download_estimation(
            symbol, period, interval
        )
        
        print(f"ğŸ“Š ä¸‹è½½ä¼°ç®—:")
        print(f"   æ—¶é—´èŒƒå›´: {start_str} â†’ {end_str} ({estimation['total_days']} å¤©)")
        print(f"   æ‰¹æ¬¡é…ç½®: {estimation['batch_days']} å¤©/æ‰¹æ¬¡, å…± {estimation['total_batches']} æ‰¹æ¬¡")
        print(f"   é¢„è®¡è€—æ—¶: {estimation['total_estimated_time_minutes']:.1f} åˆ†é’Ÿ")
        
        # è¯¢é—®ç”¨æˆ·ç¡®è®¤
        if estimation['total_estimated_time_minutes'] > 10:
            try:
                confirm = input(f"\nâš ï¸  é¢„è®¡ä¸‹è½½æ—¶é—´è¾ƒé•¿ ({estimation['total_estimated_time_minutes']:.1f} åˆ†é’Ÿ)ï¼Œæ˜¯å¦ç»§ç»­ï¼Ÿ [y/N]: ")
                if confirm.lower() not in ['y', 'yes']:
                    print("âŒ ç”¨æˆ·å–æ¶ˆä¸‹è½½")
                    return pd.DataFrame()
            except KeyboardInterrupt:
                print("\nâŒ ç”¨æˆ·å–æ¶ˆä¸‹è½½")
                return pd.DataFrame()
        
        # æ‰§è¡Œåˆ†æ‰¹æ¬¡ä¸‹è½½ï¼ˆä½¿ç”¨ DataManager çš„å†…ç½®æ–¹æ³•ï¼‰
        return self.data_manager._download_in_batches(
            symbol=symbol,
            start_date=start_str,
            end_date=end_str,
            interval=interval,
            resume=True
        )


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="æ•°æ®ä¸‹è½½å’Œé¢„å¤„ç†è„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )
    
    # è‚¡ç¥¨å‚æ•°
    group = parser.add_mutually_exclusive_group(required=False)
    group.add_argument('--symbol', '-s',
                      help='å•ä¸ªäº¤æ˜“ä»£ç  (é»˜è®¤: EURUSD)')
    
    group.add_argument('--symbols',
                      help='å¤šä¸ªäº¤æ˜“ä»£ç ï¼Œé€—å·åˆ†éš” (ä¾‹å¦‚: EURUSD,GBPUSD,USDJPY)')
    
    parser.add_argument('--period', '-p',
                       help='æ•°æ®å‘¨æœŸ (ä¾‹å¦‚: 1y, 2y, 6m, 3m)')
    
    # æ—¥æœŸèŒƒå›´å‚æ•° (æ–°å¢)
    parser.add_argument('--start-date',
                       help='å¼€å§‹æ—¥æœŸ (æ ¼å¼: YYYY-MM-DD, ä¾‹å¦‚: 2023-01-01)')
    
    parser.add_argument('--end-date',
                       help='ç»“æŸæ—¥æœŸ (æ ¼å¼: YYYY-MM-DD, ä¾‹å¦‚: 2023-12-31)')
    
    parser.add_argument('--interval', '-i',
                       help='æ•°æ®é—´éš” (fxminuteä»…æ”¯æŒ1m, yfinance: 1mä»…7å¤©/1hæ”¯æŒ2å¹´/1dæ”¯æŒå†å²)')
    
    # æ•°æ®é›†åˆ’åˆ†å‚æ•°
    parser.add_argument('--train-ratio',
                       type=float,
                       help='è®­ç»ƒé›†æ¯”ä¾‹ (é»˜è®¤: 0.7)')
    
    parser.add_argument('--val-ratio',
                       type=float,
                       help='éªŒè¯é›†æ¯”ä¾‹ (é»˜è®¤: 0.2)')
    
    parser.add_argument('--test-ratio',
                       type=float,
                       help='æµ‹è¯•é›†æ¯”ä¾‹ (é»˜è®¤: 0.1)')
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument('--output-dir',
                       help='è¾“å‡ºç›®å½• (é»˜è®¤: datasets)')
    
    parser.add_argument('--config', '-c',
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    
    # ä»£ç†å‚æ•°
    parser.add_argument('--use-proxy',
                       action='store_true',
                       help='å¯ç”¨ä»£ç†')
    
    parser.add_argument('--no-proxy',
                       action='store_true',
                       help='ç¦ç”¨ä»£ç†')
    
    parser.add_argument('--proxy-host',
                       default='127.0.0.1',
                       help='ä»£ç†ä¸»æœºåœ°å€ (é»˜è®¤: 127.0.0.1)')
    
    parser.add_argument('--proxy-port',
                       default='7891',
                       help='ä»£ç†ç«¯å£ (é»˜è®¤: 7891)')
    
    # æ•°æ®æºå‚æ•°
    parser.add_argument('--data-source',
                       choices=[ds.value for ds in DataSource if ds != DataSource.AUTO],
                       default=DataSource.FXMINUTE.value,
                       help='æ•°æ®æºç±»å‹ (é»˜è®¤: fxminute)')
    
    parser.add_argument('--data-source-config',
                       help='æ•°æ®æºé…ç½®æ–‡ä»¶è·¯å¾„ (JSONæ ¼å¼)')
    
    # åˆ†æ‰¹æ¬¡ä¸‹è½½å‚æ•°
    parser.add_argument('--enable-batch-download',
                       action='store_true',
                       help='å¯ç”¨åˆ†æ‰¹æ¬¡ä¸‹è½½ï¼ˆé€‚ç”¨äºå¤§æ•°æ®é‡ï¼‰')
    
    parser.add_argument('--batch-threshold-days',
                       type=int,
                       default=365,
                       help='åˆ†æ‰¹æ¬¡ä¸‹è½½é˜ˆå€¼ï¼ˆå¤©æ•°ï¼Œé»˜è®¤365å¤©ï¼‰')
    
    parser.add_argument('--batch-size-days',
                       type=int,
                       help='åˆ†æ‰¹æ¬¡å¤§å°ï¼ˆå¤©æ•°ï¼Œé»˜è®¤è‡ªåŠ¨è®¡ç®—ï¼‰')
    
    parser.add_argument('--resume-download',
                       action='store_true',
                       default=True,
                       help='å¯ç”¨æ–­ç‚¹ç»­ä¼ ï¼ˆé»˜è®¤å¯ç”¨ï¼‰')
    
    parser.add_argument('--no-resume',
                       action='store_true',
                       help='ç¦ç”¨æ–­ç‚¹ç»­ä¼ ')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--verbose', '-v',
                       action='store_true',
                       help='è¯¦ç»†è¾“å‡º')
    
    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("TensorTrade å¸‚åœºæ•°æ®ä¸‹è½½å’Œé¢„å¤„ç†è„šæœ¬")
    print("æ”¯æŒå¤šæ•°æ®æºçš„å¸‚åœºæ•°æ®è·å–å’Œç‰¹å¾å·¥ç¨‹")
    print("=" * 60)
    
    # =============================================
    # é»˜è®¤å‚æ•°é…ç½® - åœ¨è¿™é‡Œä¿®æ”¹é»˜è®¤å€¼
    # =============================================
    DEFAULT_SYMBOL = "EURUSD"         # é»˜è®¤å¤–æ±‡ä»£ç  (æ¬§å…ƒ/ç¾å…ƒï¼ŒFX-1-Minute-Dataæ”¯æŒ)
    DEFAULT_SYMBOLS = None            # é»˜è®¤å¤šå¤–æ±‡ï¼ˆNoneè¡¨ç¤ºä½¿ç”¨å•å¤–æ±‡ï¼‰
    DEFAULT_PERIOD = DataPeriod.MAX    # é»˜è®¤æ•°æ®å‘¨æœŸ (æ‰€æœ‰å¯ç”¨æ•°æ®ï¼Œ2000-2024å¹´)
    DEFAULT_INTERVAL = DataInterval.MINUTE_1  # é»˜è®¤æ•°æ®é—´éš” (1åˆ†é’Ÿç²’åº¦ï¼ŒFXMinuteä»…æ”¯æŒ1åˆ†é’Ÿ)
    DEFAULT_TRAIN_RATIO = 0.7         # é»˜è®¤è®­ç»ƒé›†æ¯”ä¾‹
    DEFAULT_VAL_RATIO = 0.2           # é»˜è®¤éªŒè¯é›†æ¯”ä¾‹  
    DEFAULT_TEST_RATIO = 0.1          # é»˜è®¤æµ‹è¯•é›†æ¯”ä¾‹
    DEFAULT_OUTPUT_DIR = "datasets"   # é»˜è®¤è¾“å‡ºç›®å½•
    DEFAULT_CONFIG = None             # é»˜è®¤é…ç½®æ–‡ä»¶
    DEFAULT_VERBOSE = False           # é»˜è®¤è¯¦ç»†è¾“å‡º
    
    # ä»£ç†é…ç½® - ä¿®æ”¹è¿™é‡Œæ¥é…ç½®ä»£ç†è®¾ç½®ï¼ˆFXMinuteæœ¬åœ°æ•°æ®ä¸éœ€è¦ä»£ç†ï¼‰
    DEFAULT_USE_PROXY = False         # é»˜è®¤ä¸å¯ç”¨ä»£ç†ï¼ˆFXMinuteä½¿ç”¨æœ¬åœ°æ•°æ®ï¼‰
    DEFAULT_PROXY_HOST = "127.0.0.1"  # é»˜è®¤ä»£ç†ä¸»æœº
    DEFAULT_PROXY_PORT = "7891"       # é»˜è®¤ä»£ç†ç«¯å£ï¼ˆæœ¬åœ°socketä»£ç†ï¼‰
    
    # æ•°æ®æºé…ç½® - ä¿®æ”¹è¿™é‡Œæ¥é…ç½®é»˜è®¤æ•°æ®æº
    DEFAULT_DATA_SOURCE = DataSource.FXMINUTE   # é»˜è®¤æ•°æ®æºæšä¸¾ (FX-1-Minute-Dataæœ¬åœ°ç¼“å­˜æ•°æ®)
    DEFAULT_DATA_SOURCE_CONFIG = None  # é»˜è®¤æ•°æ®æºé…ç½®æ–‡ä»¶
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_arguments()
    
    # ä½¿ç”¨é»˜è®¤å€¼è¦†ç›–æœªæŒ‡å®šçš„å‚æ•°
    symbol = args.symbol or DEFAULT_SYMBOL
    symbols = args.symbols or DEFAULT_SYMBOLS
    start_date = args.start_date  # æ–°å¢ï¼šå¼€å§‹æ—¥æœŸ
    end_date = args.end_date      # æ–°å¢ï¼šç»“æŸæ—¥æœŸ
    
    # å¤„ç†periodå‚æ•°ï¼ˆæ”¯æŒå‘½ä»¤è¡Œå­—ç¬¦ä¸²å’Œé»˜è®¤æšä¸¾ï¼‰
    if args.period:
        # ç”¨æˆ·æä¾›äº†å‘½ä»¤è¡Œå‚æ•°ï¼Œè½¬æ¢å­—ç¬¦ä¸²ä¸ºDataPeriodæšä¸¾
        try:
            period = DataPeriod.from_string(args.period)
            print(f"ä½¿ç”¨DataPeriodæšä¸¾ (ä»å‘½ä»¤è¡Œ): {period.display_name} ({period.to_days()}å¤©)")
        except ValueError:
            period = args.period  # ä¿æŒå­—ç¬¦ä¸²æ ¼å¼ä½œä¸ºåå¤‡
            print(f"âš ï¸  ä½¿ç”¨å­—ç¬¦ä¸²å‘¨æœŸ (ä»å‘½ä»¤è¡Œ): {args.period} (æœªæ‰¾åˆ°å¯¹åº”çš„DataPeriodæšä¸¾)")
    else:
        # ä½¿ç”¨é»˜è®¤æšä¸¾å€¼
        period = DEFAULT_PERIOD
        print(f"ä½¿ç”¨é»˜è®¤DataPeriodæšä¸¾: {period.display_name} ({period.to_days()}å¤©)")
    
    # å¤„ç†intervalå‚æ•°ï¼ˆæ”¯æŒå‘½ä»¤è¡Œå­—ç¬¦ä¸²å’Œé»˜è®¤æšä¸¾ï¼‰
    if args.interval:
        # ç”¨æˆ·æä¾›äº†å‘½ä»¤è¡Œå‚æ•°ï¼Œè½¬æ¢å­—ç¬¦ä¸²ä¸ºDataIntervalæšä¸¾
        interval_str = args.interval
        interval_mapping = {
            '1m': DataInterval.MINUTE_1,
            '5m': DataInterval.MINUTE_5, 
            '15m': DataInterval.MINUTE_15,
            '30m': DataInterval.MINUTE_30,
            '1h': DataInterval.HOUR_1,
            '4h': DataInterval.HOUR_4,
            '1d': DataInterval.DAY_1,
            '1w': DataInterval.WEEK_1,
            '1M': DataInterval.MONTH_1,
            '1Y': DataInterval.YEAR_1
        }
        
        if interval_str in interval_mapping:
            interval = interval_mapping[interval_str]
            print(f"ä½¿ç”¨DataIntervalæšä¸¾ (ä»å‘½ä»¤è¡Œ): {interval.value}")
        else:
            # å°è¯•é€šè¿‡valueç›´æ¥åŒ¹é…
            interval = None
            for enum_item in DataInterval:
                if enum_item.value == interval_str:
                    interval = enum_item
                    print(f"ä½¿ç”¨DataIntervalæšä¸¾ (ä»å‘½ä»¤è¡Œ,é€šè¿‡å€¼åŒ¹é…): {interval.value}")
                    break
            
            if interval is None:
                interval = interval_str  # ä¿æŒå­—ç¬¦ä¸²æ ¼å¼ä½œä¸ºåå¤‡
                print(f"âš ï¸  ä½¿ç”¨å­—ç¬¦ä¸²é—´éš” (ä»å‘½ä»¤è¡Œ): {interval_str} (æœªæ‰¾åˆ°å¯¹åº”çš„DataIntervalæšä¸¾)")
    else:
        # ä½¿ç”¨é»˜è®¤æšä¸¾å€¼
        interval = DEFAULT_INTERVAL
        print(f"ä½¿ç”¨é»˜è®¤DataIntervalæšä¸¾: {interval.value}")
    
    train_ratio = args.train_ratio or DEFAULT_TRAIN_RATIO
    val_ratio = args.val_ratio or DEFAULT_VAL_RATIO
    test_ratio = args.test_ratio or DEFAULT_TEST_RATIO
    output_dir = args.output_dir or DEFAULT_OUTPUT_DIR
    config_path = args.config or DEFAULT_CONFIG
    verbose = args.verbose or DEFAULT_VERBOSE
    
    # å¤„ç†æ•°æ®æºå‚æ•°ï¼šè½¬æ¢å­—ç¬¦ä¸²å‚æ•°ä¸ºæšä¸¾
    if args.data_source:
        data_source_enum = DataSource.from_string(args.data_source)
    else:
        data_source_enum = DEFAULT_DATA_SOURCE
    
    data_source_config_file = args.data_source_config or DEFAULT_DATA_SOURCE_CONFIG
    
    # å¤„ç†ä»£ç†è®¾ç½®
    if args.no_proxy:
        use_proxy = False
    elif args.use_proxy:
        use_proxy = True
    else:
        use_proxy = DEFAULT_USE_PROXY
    
    # è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆåœ¨åˆ›å»ºä¸‹è½½å™¨ä¹‹å‰ï¼‰
    os.environ['USE_PROXY'] = str(use_proxy).lower()
    os.environ['PROXY_HOST'] = args.proxy_host or DEFAULT_PROXY_HOST
    os.environ['PROXY_PORT'] = args.proxy_port or DEFAULT_PROXY_PORT
    os.environ['DATA_SOURCE_TYPE'] = data_source_enum.value  # ç¯å¢ƒå˜é‡ä»éœ€è¦å­—ç¬¦ä¸²å€¼
    if data_source_config_file:
        os.environ['DATA_SOURCE_CONFIG'] = data_source_config_file
    
    # åˆ†æ‰¹æ¬¡ä¸‹è½½é…ç½®
    os.environ['USE_BATCH_DOWNLOAD'] = str(args.enable_batch_download).lower()
    os.environ['BATCH_THRESHOLD_DAYS'] = str(args.batch_threshold_days)
    if args.batch_size_days:
        os.environ['BATCH_SIZE_DAYS'] = str(args.batch_size_days)
    os.environ['RESUME_DOWNLOAD'] = str(not args.no_resume).lower()
    
    # æ˜¾ç¤ºé…ç½®çŠ¶æ€
    if use_proxy:
        proxy_info = f"{os.environ['PROXY_HOST']}:{os.environ['PROXY_PORT']}"
        print(f"ä»£ç†é…ç½®: http://{proxy_info}")
    else:
        print("ä»£ç†é…ç½®: å·²ç¦ç”¨")
    
    print(f"æ•°æ®æº: {data_source_enum.display_name} ({data_source_enum.value})")
    if data_source_config_file:
        print(f"é…ç½®æ–‡ä»¶: {data_source_config_file}")
    
    # æ˜¾ç¤ºåˆ†æ‰¹æ¬¡ä¸‹è½½é…ç½®
    if args.enable_batch_download:
        print(f"åˆ†æ‰¹æ¬¡ä¸‹è½½: å¯ç”¨ (é˜ˆå€¼: {args.batch_threshold_days}å¤©)")
        if args.batch_size_days:
            print(f"æ‰¹æ¬¡å¤§å°: {args.batch_size_days}å¤©")
        else:
            print(f"æ‰¹æ¬¡å¤§å°: è‡ªåŠ¨è®¡ç®—")
        print(f"æ–­ç‚¹ç»­ä¼ : {'å¯ç”¨' if not args.no_resume else 'ç¦ç”¨'}")
    else:
        print(f"åˆ†æ‰¹æ¬¡ä¸‹è½½: ç¦ç”¨")
    
    try:
        # åˆ›å»ºæ•°æ®ä¸‹è½½å™¨
        downloader = DataDownloader(
            config_path=config_path,
            output_dir=output_dir,
            data_source_enum=data_source_enum
        )
        
        # æ‰§è¡Œä¸‹è½½
        if symbols:
            # å¤šä¸ªè‚¡ç¥¨
            symbols_list = [s.strip().upper() for s in symbols.split(',')]
            result = downloader.download_multiple_stocks(
                symbols=symbols_list,
                period=period,
                interval=interval,
                train_ratio=train_ratio,
                val_ratio=val_ratio,
                test_ratio=test_ratio
            )
            
            print(f"\næ‰¹é‡ä¸‹è½½å®Œæˆ:")
            print(f"æ€»è®¡: {result['total_symbols']} ä¸ªè‚¡ç¥¨")
            print(f"æˆåŠŸ: {result['successful_count']} ä¸ª")
            print(f"å¤±è´¥: {result['failed_count']} ä¸ª")
            
            if result['successful_symbols']:
                print(f"æˆåŠŸçš„è‚¡ç¥¨: {', '.join(result['successful_symbols'])}")
            
            if result['failed_symbols']:
                print(f"å¤±è´¥çš„è‚¡ç¥¨: {', '.join(result['failed_symbols'])}")
            
            if verbose:
                print("\nè¯¦ç»†ç»“æœ:")
                print(json.dumps(result, indent=2, ensure_ascii=False, default=str))
        
        else:
            # å•ä¸ªè‚¡ç¥¨ (é»˜è®¤æˆ–æŒ‡å®š)
            result = downloader.download_single_stock(
                symbol=symbol,
                period=period,
                start_date=start_date,
                end_date=end_date,
                interval=interval,
                train_ratio=train_ratio,
                val_ratio=val_ratio,
                test_ratio=test_ratio
            )
            
            if result['status'] == 'success':
                print(f"\nSUCCESS: {symbol} æ•°æ®ä¸‹è½½æˆåŠŸ!")
                print(f"è¾“å‡ºç›®å½•: {result['output_dir']}")
                if verbose:
                    print("\nè¯¦ç»†ä¿¡æ¯:")
                    print(json.dumps(result['metadata'], indent=2, ensure_ascii=False, default=str))
            else:
                print(f"\nFAILED: {symbol} æ•°æ®ä¸‹è½½å¤±è´¥:")
                print(f"é”™è¯¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                return 1
    
    except Exception as e:
        print(f"\næ‰§è¡Œå¤±è´¥: {e}")
        return 1
    
    print(f"\næ•°æ®ä¸‹è½½å®Œæˆï¼Œä¿å­˜è‡³: {output_dir}")
    return 0


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)